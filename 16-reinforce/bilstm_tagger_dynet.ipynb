{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bilstm-tagger-dynet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "PdbyLByMBn4N",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "843e655b-72e6-4fdd-a262-b4a14e66d58b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532608776098,
          "user_tz": -540,
          "elapsed": 9715,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dynet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/8c/767cc83241b2abe567d705f87589d8ad44cca321f7c78720269c45e0469f/dyNET-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (27.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 27.8MB 1.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dynet) (1.14.5)\n",
            "Collecting cython (from dynet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/10/ffebdb9faa901c83b69ab7040a1f5f3b2c71899be141752a6d466718c491/Cython-0.28.4-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.4MB 5.5MB/s \n",
            "\u001b[?25hInstalling collected packages: cython, dynet\n",
            "Successfully installed cython-0.28.4 dynet-2.0.3\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "paS1nLuWByXn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import dynet as dy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D5vaLVxLBzcb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#the parameters from mixer\n",
        "NXENT = 40\n",
        "NXER = 20\n",
        "delta = 2\n",
        "\n",
        "# format of files: each line is \"word1|tag1 word2|tag2 ...\"\n",
        "train_file = \"nn4nlp-code/data/tags/train.txt\"\n",
        "dev_file = \"nn4nlp-code/data/tags/dev.txt\"\n",
        "\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "t2i = defaultdict(lambda: len(t2i))\n",
        "\n",
        "\n",
        "def read(fname):\n",
        "    \"\"\"\n",
        "    Read tagged file\n",
        "    \"\"\"\n",
        "    with open(fname, \"r\") as f:\n",
        "        for line in f:\n",
        "            words, tags = [], []\n",
        "            for wt in line.strip().split():\n",
        "                w, t = wt.split('|')\n",
        "                words.append(w2i[w])\n",
        "                tags.append(t2i[t])\n",
        "            yield (words, tags)\n",
        "\n",
        "\n",
        "# Read the data\n",
        "train = list(read(train_file))\n",
        "unk_word = w2i[\"<unk>\"]\n",
        "w2i = defaultdict(lambda: unk_word, w2i)\n",
        "unk_tag = t2i[\"<unk>\"]\n",
        "t2i = defaultdict(lambda: unk_tag, t2i)\n",
        "nwords = len(w2i)\n",
        "ntags = len(t2i)\n",
        "dev = list(read(dev_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T16mj0I4Bog-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DyNet Starts\n",
        "model = dy.Model()\n",
        "trainer = dy.AdamTrainer(model)\n",
        "\n",
        "# Model parameters\n",
        "EMBED_SIZE = 64\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "# Lookup parameters for word embeddings\n",
        "LOOKUP = model.add_lookup_parameters((nwords, EMBED_SIZE))\n",
        "\n",
        "# Word-level BiLSTM\n",
        "LSTM = dy.BiRNNBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
        "\n",
        "# Word-level softmax\n",
        "W_sm = model.add_parameters((ntags, HIDDEN_SIZE))\n",
        "b_sm = model.add_parameters(ntags)\n",
        "\n",
        "\n",
        "#Baseline reward parameters\n",
        "W_bl_p = model.add_parameters((1, HIDDEN_SIZE))\n",
        "b_bl_p = model.add_parameters(1)\n",
        "\n",
        "# Calculate the scores for one example\n",
        "def calc_scores(words):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    word_reps = LSTM.transduce([LOOKUP[x] for x in words])\n",
        "\n",
        "    # Softmax scores\n",
        "    W = dy.parameter(W_sm)\n",
        "    b = dy.parameter(b_sm)\n",
        "    scores = [dy.affine_transform([b, W, x]) for x in word_reps]\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Calculate MLE loss for one example\n",
        "def calc_loss(scores, tags):\n",
        "    losses = [dy.pickneglogsoftmax(score, tag) for score, tag in zip(scores, tags)]\n",
        "    return dy.esum(losses)\n",
        "\n",
        "def calc_reinforce_loss(words, tags, delta):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    word_reps = LSTM.transduce([LOOKUP[x] for x in words])\n",
        "\n",
        "    # Softmax scores\n",
        "    W = dy.parameter(W_sm)\n",
        "    b = dy.parameter(b_sm)\n",
        "\n",
        "    #calculate the probability distribution \n",
        "    scores = [dy.affine_transform([b, W, x]) for x in word_reps]\n",
        "    losses = [dy.pickneglogsoftmax(score, tag) for score, tag in zip(scores, tags)]\n",
        "    probs = [-dy.exp(loss).as_array() for loss in losses]\n",
        "\n",
        "    #then take samples from the probability distribution\n",
        "    samples = [np.random.choice(range(len(x)), p=x) for x in probs]\n",
        "\n",
        "    #calculate accuracy=reward\n",
        "    correct = [sample == tag for sample, tag in zip(samples, tags)]\n",
        "    r_i = float(sum(correct))/len(correct)\n",
        "    r = dy.constant((1), r_i)\n",
        "    # Reward baseline for each word\n",
        "    W_bl = dy.parameter(W_bl_p)\n",
        "    b_bl = dy.parameter(b_bl_p)\n",
        "    r_b = [dy.affine_transform([b_bl, W_bl, dy.nobackprop(x)]) for x in word_reps]\n",
        "\n",
        "    #we need to take the value in order to break the computation graph\n",
        "    #as the reward portion is trained seperatley and not backpropogated through during the overall score\n",
        "    rewards_over_baseline = [(r - dy.nobackprop(x)) for x in r_b]\n",
        "    #the scores for training the baseline\n",
        "    baseline_scores = [dy.square(r - x) for x in r_b]\n",
        "\n",
        "    #then calculate the reinforce scores using reinforce\n",
        "    reinforce_scores = [r_s*score for r_s, score in zip(rewards_over_baseline, scores)]\n",
        "\n",
        "    #we want the first len(sent)-delta scores from xent then delta scores from reinforce\n",
        "    #for mixer\n",
        "    if len(scores) > delta:\n",
        "        mixer_scores = scores[:len(scores)-delta] + reinforce_scores[delta-1:]\n",
        "    else:\n",
        "        mixer_scores = reinforce_scores\n",
        "    return dy.esum(mixer_scores), dy.esum(baseline_scores)\n",
        "\n",
        "\n",
        "# Calculate number of tags correct for one example\n",
        "def calc_correct(scores, tags):\n",
        "    correct = [np.argmax(score.npvalue()) == tag for score, tag in zip(scores, tags)]\n",
        "    return sum(correct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NtEm8OMXB3lR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 4204
        },
        "outputId": "6c1b3f0b-f4cd-44d2-aff6-beb71baa2ecc",
        "executionInfo": {
          "status": "error",
          "timestamp": 1532609629539,
          "user_tz": -540,
          "elapsed": 848520,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Perform training\n",
        "for ITER in range(NXENT+NXER):\n",
        "    random.shuffle(train)\n",
        "    start = time.time()\n",
        "    this_sents = this_words = this_loss = this_correct = 0\n",
        "    for sid in range(0, len(train)):\n",
        "        this_sents += 1\n",
        "        if this_sents % int(1000) == 0:\n",
        "            print(\"train loss/word=%.4f, acc=%.2f%%, word/sec=%.4f\" % (\n",
        "                this_loss / this_words, 100 * this_correct / this_words, this_words / (time.time() - start)),\n",
        "                  file=sys.stderr)\n",
        "        # train on the example\n",
        "        if ITER < NXER:\n",
        "            words, tags = train[sid]\n",
        "            scores = calc_scores(words)\n",
        "            loss_exp = calc_loss(scores, tags)\n",
        "            this_correct += calc_correct(scores, tags)\n",
        "            this_loss += loss_exp.scalar_value()\n",
        "            this_words += len(words)\n",
        "            loss_exp.backward()\n",
        "            trainer.update()\n",
        "        else:\n",
        "            delta = 2*(ITER - NXENT)\n",
        "            mixer_loss, baseline_loss = calc_reinforce_loss(words, tags, delta)\n",
        "            this_loss += mixer_loss.scalar_value() + baseline_loss.scalar_value()\n",
        "            this_words += len(words)\n",
        "            mixer_loss.backward()\n",
        "            baseline_loss.backward()\n",
        "            trainer.update()\n",
        "    # Perform evaluation \n",
        "    start = time.time()\n",
        "    this_sents = this_words = this_loss = this_correct = 0\n",
        "    for words, tags in dev:\n",
        "        this_sents += 1\n",
        "        scores = calc_scores(words)\n",
        "        loss_exp = calc_loss(scores, tags)\n",
        "        this_correct += calc_correct(scores, tags)\n",
        "        this_loss += loss_exp.scalar_value()\n",
        "        this_words += len(words)\n",
        "    print(\"dev loss/word=%.4f, acc=%.2f%%, word/sec=%.4f\" % (\n",
        "        this_loss / this_words, 100 * this_correct / this_words, this_words / (time.time() - start)), file=sys.stderr)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.5637, acc=86.76%, word/sec=5603.4796\n",
            "train loss/word=0.4980, acc=86.88%, word/sec=5275.9078\n",
            "train loss/word=0.4553, acc=87.44%, word/sec=5370.1220\n",
            "train loss/word=0.4272, acc=87.92%, word/sec=5429.1631\n",
            "train loss/word=0.4031, acc=88.43%, word/sec=5557.0012\n",
            "train loss/word=0.3827, acc=88.90%, word/sec=5677.5937\n",
            "train loss/word=0.3647, acc=89.34%, word/sec=5762.1203\n",
            "train loss/word=0.3490, acc=89.73%, word/sec=5796.2541\n",
            "train loss/word=0.3355, acc=90.10%, word/sec=5848.1657\n",
            "train loss/word=0.3257, acc=90.36%, word/sec=5896.0950\n",
            "dev loss/word=0.4211, acc=86.94%, word/sec=17244.6169\n",
            "train loss/word=0.1830, acc=94.36%, word/sec=6301.6586\n",
            "train loss/word=0.1803, acc=94.43%, word/sec=6317.0340\n",
            "train loss/word=0.1765, acc=94.60%, word/sec=6270.0666\n",
            "train loss/word=0.1752, acc=94.65%, word/sec=6257.2067\n",
            "train loss/word=0.1756, acc=94.67%, word/sec=6280.3980\n",
            "train loss/word=0.1735, acc=94.77%, word/sec=6279.4957\n",
            "train loss/word=0.1732, acc=94.78%, word/sec=6292.9446\n",
            "train loss/word=0.1732, acc=94.80%, word/sec=6291.5460\n",
            "train loss/word=0.1720, acc=94.84%, word/sec=6299.4147\n",
            "train loss/word=0.1709, acc=94.88%, word/sec=6300.3743\n",
            "dev loss/word=0.4240, acc=88.22%, word/sec=17024.9359\n",
            "train loss/word=0.1177, acc=96.48%, word/sec=6374.2348\n",
            "train loss/word=0.1164, acc=96.51%, word/sec=6362.3639\n",
            "train loss/word=0.1170, acc=96.53%, word/sec=6351.6202\n",
            "train loss/word=0.1154, acc=96.55%, word/sec=6335.4114\n",
            "train loss/word=0.1151, acc=96.56%, word/sec=6334.5132\n",
            "train loss/word=0.1176, acc=96.55%, word/sec=6330.2564\n",
            "train loss/word=0.1191, acc=96.54%, word/sec=6342.0432\n",
            "train loss/word=0.1171, acc=96.60%, word/sec=6354.0384\n",
            "train loss/word=0.1167, acc=96.60%, word/sec=6359.6919\n",
            "train loss/word=0.1158, acc=96.65%, word/sec=6364.1477\n",
            "dev loss/word=0.5201, acc=89.00%, word/sec=17258.5111\n",
            "train loss/word=0.0822, acc=97.46%, word/sec=6438.0781\n",
            "train loss/word=0.0790, acc=97.66%, word/sec=6440.7272\n",
            "train loss/word=0.0820, acc=97.61%, word/sec=6428.9472\n",
            "train loss/word=0.0824, acc=97.64%, word/sec=6420.0890\n",
            "train loss/word=0.0844, acc=97.62%, word/sec=6422.7391\n",
            "train loss/word=0.0823, acc=97.66%, word/sec=6398.5212\n",
            "train loss/word=0.0822, acc=97.67%, word/sec=6383.7487\n",
            "train loss/word=0.0821, acc=97.69%, word/sec=6361.5927\n",
            "train loss/word=0.0817, acc=97.70%, word/sec=6345.0709\n",
            "train loss/word=0.0823, acc=97.69%, word/sec=6330.8464\n",
            "dev loss/word=0.5178, acc=89.32%, word/sec=16687.6221\n",
            "train loss/word=0.0507, acc=98.58%, word/sec=6140.5683\n",
            "train loss/word=0.0537, acc=98.52%, word/sec=6209.2512\n",
            "train loss/word=0.0545, acc=98.50%, word/sec=6223.2346\n",
            "train loss/word=0.0555, acc=98.49%, word/sec=6221.5332\n",
            "train loss/word=0.0561, acc=98.47%, word/sec=6239.0653\n",
            "train loss/word=0.0562, acc=98.46%, word/sec=6249.8658\n",
            "train loss/word=0.0568, acc=98.45%, word/sec=6264.1504\n",
            "train loss/word=0.0576, acc=98.44%, word/sec=6277.3092\n",
            "train loss/word=0.0578, acc=98.45%, word/sec=6282.6745\n",
            "train loss/word=0.0579, acc=98.43%, word/sec=6288.9416\n",
            "dev loss/word=0.6132, acc=89.41%, word/sec=17098.1208\n",
            "train loss/word=0.0318, acc=99.04%, word/sec=6358.0029\n",
            "train loss/word=0.0360, acc=99.02%, word/sec=6354.4647\n",
            "train loss/word=0.0376, acc=98.98%, word/sec=6359.6540\n",
            "train loss/word=0.0383, acc=98.95%, word/sec=6352.1061\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0384, acc=98.95%, word/sec=6337.4868\n",
            "train loss/word=0.0375, acc=98.97%, word/sec=6335.4072\n",
            "train loss/word=0.0383, acc=98.96%, word/sec=6339.8847\n",
            "train loss/word=0.0385, acc=98.96%, word/sec=6329.2470\n",
            "train loss/word=0.0383, acc=98.96%, word/sec=6336.9445\n",
            "train loss/word=0.0393, acc=98.95%, word/sec=6343.0772\n",
            "dev loss/word=0.7254, acc=89.55%, word/sec=17147.3225\n",
            "train loss/word=0.0226, acc=99.32%, word/sec=6343.3786\n",
            "train loss/word=0.0235, acc=99.33%, word/sec=6359.5700\n",
            "train loss/word=0.0241, acc=99.33%, word/sec=6369.9006\n",
            "train loss/word=0.0237, acc=99.36%, word/sec=6361.7786\n",
            "train loss/word=0.0239, acc=99.34%, word/sec=6358.7953\n",
            "train loss/word=0.0244, acc=99.33%, word/sec=6358.7531\n",
            "train loss/word=0.0247, acc=99.33%, word/sec=6361.0555\n",
            "train loss/word=0.0253, acc=99.31%, word/sec=6354.9474\n",
            "train loss/word=0.0254, acc=99.32%, word/sec=6358.5355\n",
            "train loss/word=0.0254, acc=99.32%, word/sec=6367.3133\n",
            "dev loss/word=0.8641, acc=89.46%, word/sec=17378.3417\n",
            "train loss/word=0.0145, acc=99.66%, word/sec=6490.9224\n",
            "train loss/word=0.0133, acc=99.66%, word/sec=6478.8977\n",
            "train loss/word=0.0138, acc=99.64%, word/sec=6456.9328\n",
            "train loss/word=0.0147, acc=99.63%, word/sec=6457.9311\n",
            "train loss/word=0.0144, acc=99.63%, word/sec=6455.6442\n",
            "train loss/word=0.0135, acc=99.65%, word/sec=6447.2661\n",
            "train loss/word=0.0139, acc=99.63%, word/sec=6441.6106\n",
            "train loss/word=0.0143, acc=99.63%, word/sec=6435.2253\n",
            "train loss/word=0.0150, acc=99.62%, word/sec=6422.3408\n",
            "train loss/word=0.0161, acc=99.60%, word/sec=6411.9646\n",
            "dev loss/word=0.9367, acc=89.30%, word/sec=17110.2158\n",
            "train loss/word=0.0109, acc=99.75%, word/sec=6236.8133\n",
            "train loss/word=0.0110, acc=99.75%, word/sec=6182.3115\n",
            "train loss/word=0.0112, acc=99.74%, word/sec=6186.0899\n",
            "train loss/word=0.0105, acc=99.74%, word/sec=6209.5418\n",
            "train loss/word=0.0101, acc=99.75%, word/sec=6227.7678\n",
            "train loss/word=0.0108, acc=99.72%, word/sec=6241.3207\n",
            "train loss/word=0.0104, acc=99.73%, word/sec=6240.7420\n",
            "train loss/word=0.0101, acc=99.73%, word/sec=6249.5279\n",
            "train loss/word=0.0104, acc=99.72%, word/sec=6250.4199\n",
            "train loss/word=0.0103, acc=99.73%, word/sec=6253.2935\n",
            "dev loss/word=1.1042, acc=89.36%, word/sec=16489.0726\n",
            "train loss/word=0.0037, acc=99.93%, word/sec=6238.9904\n",
            "train loss/word=0.0039, acc=99.90%, word/sec=6269.3034\n",
            "train loss/word=0.0050, acc=99.88%, word/sec=6250.0776\n",
            "train loss/word=0.0049, acc=99.87%, word/sec=6290.8589\n",
            "train loss/word=0.0058, acc=99.86%, word/sec=6299.6544\n",
            "train loss/word=0.0062, acc=99.85%, word/sec=6316.2147\n",
            "train loss/word=0.0063, acc=99.85%, word/sec=6322.0402\n",
            "train loss/word=0.0063, acc=99.85%, word/sec=6326.4663\n",
            "train loss/word=0.0065, acc=99.84%, word/sec=6325.2838\n",
            "train loss/word=0.0065, acc=99.84%, word/sec=6332.1845\n",
            "dev loss/word=1.2362, acc=89.36%, word/sec=17053.9866\n",
            "train loss/word=0.0027, acc=99.90%, word/sec=6192.6941\n",
            "train loss/word=0.0029, acc=99.91%, word/sec=6189.4623\n",
            "train loss/word=0.0031, acc=99.91%, word/sec=6207.3703\n",
            "train loss/word=0.0033, acc=99.91%, word/sec=6193.1912\n",
            "train loss/word=0.0036, acc=99.90%, word/sec=6182.7051\n",
            "train loss/word=0.0037, acc=99.90%, word/sec=6190.1430\n",
            "train loss/word=0.0038, acc=99.89%, word/sec=6191.2236\n",
            "train loss/word=0.0039, acc=99.89%, word/sec=6188.7847\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0039, acc=99.89%, word/sec=6194.4832\n",
            "train loss/word=0.0041, acc=99.89%, word/sec=6199.1903\n",
            "dev loss/word=1.2339, acc=89.33%, word/sec=17071.3899\n",
            "train loss/word=0.0021, acc=99.93%, word/sec=6235.9321\n",
            "train loss/word=0.0019, acc=99.94%, word/sec=6210.4839\n",
            "train loss/word=0.0020, acc=99.93%, word/sec=6204.0298\n",
            "train loss/word=0.0024, acc=99.92%, word/sec=6208.1281\n",
            "train loss/word=0.0025, acc=99.92%, word/sec=6221.4735\n",
            "train loss/word=0.0023, acc=99.92%, word/sec=6229.6998\n",
            "train loss/word=0.0026, acc=99.92%, word/sec=6230.1403\n",
            "train loss/word=0.0028, acc=99.92%, word/sec=6234.9062\n",
            "train loss/word=0.0028, acc=99.91%, word/sec=6233.6331\n",
            "train loss/word=0.0029, acc=99.91%, word/sec=6232.2648\n",
            "dev loss/word=1.4318, acc=89.06%, word/sec=16964.1641\n",
            "train loss/word=0.0015, acc=99.96%, word/sec=6206.7753\n",
            "train loss/word=0.0014, acc=99.96%, word/sec=6198.8095\n",
            "train loss/word=0.0019, acc=99.95%, word/sec=6209.0702\n",
            "train loss/word=0.0019, acc=99.95%, word/sec=6213.1301\n",
            "train loss/word=0.0017, acc=99.95%, word/sec=6219.6728\n",
            "train loss/word=0.0020, acc=99.95%, word/sec=6218.8351\n",
            "train loss/word=0.0019, acc=99.95%, word/sec=6212.3951\n",
            "train loss/word=0.0018, acc=99.95%, word/sec=6214.8790\n",
            "train loss/word=0.0020, acc=99.94%, word/sec=6223.3181\n",
            "train loss/word=0.0020, acc=99.95%, word/sec=6230.1809\n",
            "dev loss/word=1.6022, acc=89.28%, word/sec=17032.4774\n",
            "train loss/word=0.0015, acc=99.95%, word/sec=6230.5577\n",
            "train loss/word=0.0009, acc=99.97%, word/sec=6236.7964\n",
            "train loss/word=0.0008, acc=99.97%, word/sec=6230.2651\n",
            "train loss/word=0.0009, acc=99.97%, word/sec=6233.6252\n",
            "train loss/word=0.0012, acc=99.96%, word/sec=6238.2885\n",
            "train loss/word=0.0012, acc=99.96%, word/sec=6243.3457\n",
            "train loss/word=0.0012, acc=99.96%, word/sec=6241.4799\n",
            "train loss/word=0.0013, acc=99.96%, word/sec=6245.8805\n",
            "train loss/word=0.0013, acc=99.96%, word/sec=6250.8153\n",
            "train loss/word=0.0013, acc=99.96%, word/sec=6258.4629\n",
            "dev loss/word=1.6084, acc=89.22%, word/sec=17177.3861\n",
            "train loss/word=0.0015, acc=99.96%, word/sec=6303.9767\n",
            "train loss/word=0.0012, acc=99.96%, word/sec=6341.8727\n",
            "train loss/word=0.0011, acc=99.97%, word/sec=6338.6161\n",
            "train loss/word=0.0012, acc=99.96%, word/sec=6318.6931\n",
            "train loss/word=0.0012, acc=99.96%, word/sec=6322.3754\n",
            "train loss/word=0.0012, acc=99.96%, word/sec=6323.7172\n",
            "train loss/word=0.0013, acc=99.96%, word/sec=6296.7943\n",
            "train loss/word=0.0013, acc=99.96%, word/sec=6268.8223\n",
            "train loss/word=0.0013, acc=99.96%, word/sec=6244.8507\n",
            "train loss/word=0.0012, acc=99.96%, word/sec=6236.2720\n",
            "dev loss/word=1.6593, acc=89.18%, word/sec=16724.1491\n",
            "train loss/word=0.0004, acc=99.99%, word/sec=6120.5147\n",
            "train loss/word=0.0004, acc=99.99%, word/sec=6156.6783\n",
            "train loss/word=0.0004, acc=99.99%, word/sec=6155.3228\n",
            "train loss/word=0.0005, acc=99.98%, word/sec=6145.4631\n",
            "train loss/word=0.0006, acc=99.98%, word/sec=6152.3844\n",
            "train loss/word=0.0006, acc=99.98%, word/sec=6147.1316\n",
            "train loss/word=0.0006, acc=99.98%, word/sec=6150.8145\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=6160.5247\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6161.4105\n",
            "train loss/word=0.0008, acc=99.97%, word/sec=6158.8680\n",
            "dev loss/word=1.7418, acc=89.27%, word/sec=16835.0924\n",
            "train loss/word=0.0006, acc=99.98%, word/sec=5983.9731\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0005, acc=99.98%, word/sec=5802.7664\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=5799.1016\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=5892.4475\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=5955.8564\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=6002.8348\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6036.9220\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6063.1508\n",
            "train loss/word=0.0009, acc=99.98%, word/sec=6088.0391\n",
            "train loss/word=0.0009, acc=99.98%, word/sec=6077.9788\n",
            "dev loss/word=1.7989, acc=89.09%, word/sec=16930.5725\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6278.4539\n",
            "train loss/word=0.0006, acc=99.98%, word/sec=6257.3298\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6222.6731\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6083.1523\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=6033.9415\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=6060.4028\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6110.9066\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6139.7763\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6161.0838\n",
            "train loss/word=0.0008, acc=99.98%, word/sec=6178.0006\n",
            "dev loss/word=2.0625, acc=89.16%, word/sec=16967.2006\n",
            "train loss/word=0.0006, acc=99.98%, word/sec=5860.2885\n",
            "train loss/word=0.0010, acc=99.97%, word/sec=5964.7973\n",
            "train loss/word=0.0011, acc=99.97%, word/sec=5765.4171\n",
            "train loss/word=0.0012, acc=99.97%, word/sec=5612.0666\n",
            "train loss/word=0.0011, acc=99.97%, word/sec=5604.6459\n",
            "train loss/word=0.0010, acc=99.97%, word/sec=5630.7802\n",
            "train loss/word=0.0010, acc=99.97%, word/sec=5679.9236\n",
            "train loss/word=0.0010, acc=99.97%, word/sec=5672.2350\n",
            "train loss/word=0.0010, acc=99.98%, word/sec=5587.9284\n",
            "train loss/word=0.0009, acc=99.98%, word/sec=5632.1567\n",
            "dev loss/word=1.8794, acc=89.26%, word/sec=16520.8283\n",
            "train loss/word=0.0002, acc=99.99%, word/sec=5289.2603\n",
            "train loss/word=0.0004, acc=99.99%, word/sec=5471.6961\n",
            "train loss/word=0.0006, acc=99.99%, word/sec=5440.8558\n",
            "train loss/word=0.0005, acc=99.99%, word/sec=5472.4116\n",
            "train loss/word=0.0006, acc=99.99%, word/sec=5597.5468\n",
            "train loss/word=0.0007, acc=99.99%, word/sec=5686.4712\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=5744.7116\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=5795.9164\n",
            "train loss/word=0.0007, acc=99.98%, word/sec=5826.5868\n",
            "train loss/word=0.0006, acc=99.98%, word/sec=5851.3599\n",
            "dev loss/word=1.9514, acc=89.36%, word/sec=16419.6007\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a223c03407d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITER\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mXENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mmixer_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_reinforce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mthis_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmixer_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbaseline_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'XENT' is not defined"
          ]
        }
      ]
    }
  ]
}