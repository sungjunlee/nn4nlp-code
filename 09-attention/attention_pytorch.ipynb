{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention-pytorch.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "d3VqIg5jYGNw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "8b5a58a3-f55c-4532-a99f-50860b7b98bd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796825878,
          "user_tz": -540,
          "elapsed": 3812,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (0.4.0)\r\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python2.7/dist-packages (0.2.3)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from torchtext) (2.18.4)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python2.7/dist-packages (from torchtext) (4.23.4)\r\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext) (2.6)\r\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ugNqiuswYMb-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "343a1119-e8c3-4a91-c2bc-a2091b6e42fd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796826701,
          "user_tz": -540,
          "elapsed": 780,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WkF0zfBUYdij",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "9f1d1fa6-7b59-4c1e-9097-346f71fa99c0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796827398,
          "user_tz": -540,
          "elapsed": 540,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_src_file = \"nn4nlp-code/data/parallel/train.ja\"\n",
        "train_trg_file = \"nn4nlp-code/data/parallel/train.en\"\n",
        "dev_src_file = \"nn4nlp-code/data/parallel/dev.ja\"\n",
        "dev_trg_file = \"nn4nlp-code/data/parallel/dev.en\"\n",
        "test_src_file = \"nn4nlp-code/data/parallel/test.ja\"\n",
        "test_trg_file = \"nn4nlp-code/data/parallel/test.en\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bEUlfwsIYxHf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "eb26cf37-20ab-4191-ba09-8b48ebf038d4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796828094,
          "user_tz": -540,
          "elapsed": 504,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "w2i_src = defaultdict(lambda: len(w2i_src))\n",
        "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
        "\n",
        "def read(fname_src, fname_trg):\n",
        "    \"\"\"\n",
        "    Read parallel files where each line lines up\n",
        "    \"\"\"\n",
        "    with open(fname_src, \"r\") as f_src, open(fname_trg, \"r\") as f_trg:\n",
        "        for line_src, line_trg in zip(f_src, f_trg):\n",
        "            #need to append EOS tags to at least the target sentence\n",
        "            sent_src = [w2i_src[x] for x in line_src.strip().split() + ['</s>']] \n",
        "            sent_trg = [w2i_trg[x] for x in ['<s>'] + line_trg.strip().split() + ['</s>']] \n",
        "            yield (sent_src, sent_trg)\n",
        "\n",
        "# Read the data\n",
        "train = list(read(train_src_file, train_trg_file))\n",
        "unk_src = w2i_src[\"<unk>\"]\n",
        "eos_src = w2i_src['</s>']\n",
        "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
        "unk_trg = w2i_trg[\"<unk>\"]\n",
        "eos_trg = w2i_trg['</s>']\n",
        "sos_trg = w2i_trg['<s>']\n",
        "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
        "i2w_trg = {v: k for k, v in w2i_trg.items()}\n",
        "i2w_src = {v: k for k, v in w2i_src.items()}\n",
        "\n",
        "nwords_src = len(w2i_src)\n",
        "nwords_trg = len(w2i_trg)\n",
        "dev = list(read(dev_src_file, dev_trg_file))\n",
        "test = list(read(test_src_file, test_trg_file))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDsk0MahQLZJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "30658d65-67c9-4dc3-cf55-9ec050514500",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796828732,
          "user_tz": -540,
          "elapsed": 486,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ParallelCorpus(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "    \n",
        "  def __getitem__(self, ix):\n",
        "    return torch.LongTensor(self.data[ix][0]), torch.LongTensor(self.data[ix][1])\n",
        "  \n",
        "def my_collate_fn(batch):\n",
        "  src, trg = zip(*batch)\n",
        "  src_len, trg_len = list(map(len, src)), list(map(len, trg))\n",
        "  src_maxlen, trg_maxlen = max(src_len), max(trg_len)\n",
        "  \n",
        "  src = torch.stack([F.pad(e, (0, src_maxlen-len(e))) for e in src])\n",
        "  trg = torch.stack([F.pad(e, (0, trg_maxlen-len(e))) for e in trg])\n",
        "  \n",
        "  return src, trg, torch.LongTensor(src_len), torch.LongTensor(trg_len)\n",
        "\n",
        "# my_collate_fn([train_corpus[i] for i in range(4)])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8KnlnwbYzvA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "1a11c957-8443-44cb-ccba-cbb2cc84cc7a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796829361,
          "user_tz": -540,
          "elapsed": 480,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, embed_size, hidden_size):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(input_size, embed_size)\n",
        "    self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "  \n",
        "  def forward(self, x, x_len):\n",
        "    h0 = self.init_hidden(x_len.shape[0])\n",
        "    encoded = self.embedding(x)\n",
        "    output, _ = self.gru(encoded, h0)\n",
        "    return output\n",
        "    \n",
        "  def init_hidden(self, bs):\n",
        "    return torch.zeros(1, bs, self.hidden_size, device=device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Oy3DyolgTpS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "d583f373-1bf4-4f1f-ccec-3d02f4d66dab",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796830112,
          "user_tz": -540,
          "elapsed": 601,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# class Attention(nn.Module):\n",
        "#   def __init__(self, method, hidden_dim):\n",
        "#     super(Attention, self).__init__()\n",
        "    \n",
        "#     self.method = method\n",
        "#     self.hidden_dim = hidden_dim\n",
        "    \n",
        "#     if self.method == 'general':\n",
        "#       self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "#     elif self.method == 'concat':\n",
        "#       self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "#       self.v = nn.Parameter(torch.FloatTensor(1, hidden_dim))\n",
        "    \n",
        "#   def forward(self, query, value, mask=None):\n",
        "#     # query: bs x maxlen x hidden_dim\n",
        "#     score = self.score(query, value)\n",
        "#     if mask is not None:\n",
        "#       score = score.mask_fill(mask == 0, -1e9)\n",
        "#     p_attn = F.softmax(score, dim=-1)\n",
        "    \n",
        "#     return torch.matmul(p_attn, value), p_attn\n",
        "  \n",
        "#   def score(self, query, value):\n",
        "#     if self.method == 'dot':\n",
        "#       return torch.matmul(query, value.transpose(-2, -1))\n",
        "#     elif self.method == 'general':\n",
        "#       return torch.matmul(query, self.attn(value).transpose(-2, -1))\n",
        "#     elif self.method == 'concat':\n",
        "#       return torch.matmul(self.v, F.tanh(self.attn(torch.cat((query, value), 1)).transpose(-2, -1)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2We-jHXkU5J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "25b55cb8-13c5-417f-8ff5-93caca7b79bf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796830735,
          "user_tz": -540,
          "elapsed": 500,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.linear = nn.Linear(dim*2, dim, bias=False)\n",
        "        \n",
        "    def forward(self, x, context):\n",
        "        attn = F.softmax(\n",
        "            context.bmm(\n",
        "                x.unsqueeze(2) # bsz x dim x 1\n",
        "            )                  # bsz x seq x 1\n",
        "            .squeeze(2)        # bsz x seq\n",
        "            , dim = 1)\n",
        "        weighted_context = attn.unsqueeze(1) # bsz x 1 x seq \n",
        "        weighted_context = weighted_context.bmm(context)             # bsz x 1 x dim\n",
        "        weighted_context = weighted_context.squeeze(1)               # bsz x dim\n",
        "        o = self.linear(torch.cat((x, weighted_context), 1))\n",
        "        return F.tanh(o)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVYY3UE4ZvI1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "f48374b6-9727-456e-88d8-636a850091c6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796893444,
          "user_tz": -540,
          "elapsed": 885,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class BahdanauAttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, output_size, embed_size, hidden_size):\n",
        "    super(BahdanauAttnDecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.embedding = nn.Embedding(output_size, embed_size)\n",
        "    self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "#     self.attn = Attention('concat', hidden_size)\n",
        "    self.attn = Attention(hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "  \n",
        "  def init_hidden(self, bs):\n",
        "    return torch.zeros(1, bs, self.hidden_size, device=device)\n",
        "  \n",
        "  def forward(self, xs, xs_len, context):\n",
        "    xs = xs.transpose(0, 1)\n",
        "    bs = xs.size(1)\n",
        "    o = []\n",
        "    embeds = self.embedding(xs)\n",
        "    hidden = self.init_hidden(bs)\n",
        "    for emb in embeds:\n",
        "      res, hidden = self.gru(emb.unsqueeze(1), hidden)\n",
        "      o.append(self.attn(res.squeeze(1), context))\n",
        "    output = self.out(torch.stack(o))\n",
        "    return output.transpose(0, 1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nao7wgOeiKpB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "06e91c3e-fcdf-4351-cde2-88cd792c76db",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796894176,
          "user_tz": -540,
          "elapsed": 503,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "EMBED_SIZE = 64\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wHt2uDzwi8qz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "8fa61040-d50c-493f-ff80-475980e4e683",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796894853,
          "user_tz": -540,
          "elapsed": 523,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_corpus = ParallelCorpus(train)\n",
        "train_loader = DataLoader(train_corpus, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, collate_fn=my_collate_fn)\n",
        "\n",
        "dev_corpus = ParallelCorpus(dev)\n",
        "dev_loader = DataLoader(dev_corpus, batch_size=BATCH_SIZE, shuffle=False, num_workers=1, collate_fn=my_collate_fn)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2WWmB4bxctBf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "dc07ab08-edd4-4d3c-a14f-082f7c54fb1d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796895488,
          "user_tz": -540,
          "elapsed": 466,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = EncoderRNN(nwords_src, EMBED_SIZE, HIDDEN_SIZE).to(device)\n",
        "decoder = BahdanauAttnDecoderRNN(nwords_trg, EMBED_SIZE, HIDDEN_SIZE).to(device)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cytBvg_qiEm6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "c2a88d95-5faa-4c33-9c71-771ff0ab2645",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530796896099,
          "user_tz": -540,
          "elapsed": 465,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_ZoQl7jiXsJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "b94f0142-7547-43a5-fdd5-82cc339fecf5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1530802487592,
          "user_tz": -540,
          "elapsed": 3888458,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch_i in range(20):\n",
        "  encoder.train()\n",
        "  decoder.train()\n",
        "  total_loss = 0.\n",
        "  for batch_i, (s, t, sl, tl) in enumerate(train_loader):\n",
        "    s, t, si, ti = s.to(device), t.to(device), sl.to(device), tl.to(device)\n",
        "    bs = s.shape[0]\n",
        "    t_in = t[:,:-1]\n",
        "    t_out = t[:, 1:]\n",
        "    encoded = encoder(s, sl)\n",
        "    decoded = decoder(t_in, tl, encoded)\n",
        "    decoded = torch.cat([decoded[ix, :tl[ix]-1].view(-1, nwords_trg) for ix in range(bs)], 0)\n",
        "    t_out = torch.cat([t_out[ix, :tl[ix]-1].view(-1) for ix in range(bs)], 0)\n",
        "    loss = criterion(decoded, t_out)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    trainer.zero_grad()\n",
        "    loss.backward()\n",
        "    trainer.step()\n",
        "  \n",
        "  print(\"epoch {} | train loss {:5.4f}\".format(epoch_i, total_loss / len(train_loader)))\n",
        "  \n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "  dev_loss = 0.\n",
        "  for batch_i, (s, t, sl, tl) in enumerate(dev_loader):\n",
        "    s, t, si, ti = s.to(device), t.to(device), sl.to(device), tl.to(device)\n",
        "    bs = s.shape[0]\n",
        "    t_in = t[:,:-1]\n",
        "    t_out = t[:, 1:]\n",
        "    encoded = encoder(s, sl)\n",
        "    decoded = decoder(t_in, tl, encoded)\n",
        "    decoded = torch.cat([decoded[ix, :tl[ix]-1].view(-1, nwords_trg) for ix in range(bs)], 0)\n",
        "    t_out = torch.cat([t_out[ix, :tl[ix]-1].view(-1) for ix in range(bs)], 0)\n",
        "    loss = criterion(decoded, t_out)\n",
        "    dev_loss += loss.item()\n",
        "  print(\"epoch {} | val loss {:5.4f}\".format(epoch_i, dev_loss / len(dev_loader)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 | train loss 4.1152\n",
            "epoch 0 | val loss 4.4178\n",
            "epoch 1 | train loss 3.8242\n",
            "epoch 1 | val loss 4.3185\n",
            "epoch 2 | train loss 3.5713\n",
            "epoch 2 | val loss 4.2843\n",
            "epoch 3 | train loss 3.3382\n",
            "epoch 3 | val loss 4.3012\n",
            "epoch 4 | train loss 3.1197\n",
            "epoch 4 | val loss 4.3102\n",
            "epoch 5 | train loss 2.9122\n",
            "epoch 5 | val loss 4.3525\n",
            "epoch 6 | train loss 2.7126\n",
            "epoch 6 | val loss 4.3840\n",
            "epoch 7 | train loss 2.5219\n",
            "epoch 7 | val loss 4.4362\n",
            "epoch 8 | train loss 2.3444\n",
            "epoch 8 | val loss 4.4829\n",
            "epoch 9 | train loss 2.1730\n",
            "epoch 9 | val loss 4.5617\n",
            "epoch 10 | train loss 2.0136\n",
            "epoch 10 | val loss 4.6452\n",
            "epoch 11 | train loss 1.8651\n",
            "epoch 11 | val loss 4.7111\n",
            "epoch 12 | train loss 1.7273\n",
            "epoch 12 | val loss 4.7986\n",
            "epoch 13 | train loss 1.5958\n",
            "epoch 13 | val loss 4.8795\n",
            "epoch 14 | train loss 1.4759\n",
            "epoch 14 | val loss 4.9750\n",
            "epoch 15 | train loss 1.3665\n",
            "epoch 15 | val loss 5.0246\n",
            "epoch 16 | train loss 1.2590\n",
            "epoch 16 | val loss 5.1462\n",
            "epoch 17 | train loss 1.1625\n",
            "epoch 17 | val loss 5.2381\n",
            "epoch 18 | train loss 1.0719\n",
            "epoch 18 | val loss 5.3397\n",
            "epoch 19 | train loss 0.9909\n",
            "epoch 19 | val loss 5.4255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pjsjOVX4iY3e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}