{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordemb-cbow.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "phDtr1X15jaE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e8764435-8a36-4b63-d7ba-ea3ec12a7ae2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526393365191,
          "user_tz": -540,
          "elapsed": 5971,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dynet in /usr/local/lib/python3.6/dist-packages (2.0.3)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dynet) (1.14.3)\r\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from dynet) (0.28.2)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NPJBoWVw5sbV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import dynet as dy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vrO9UJgb6CCG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "N=2 #length of window on each side (so N=2 gives a total window size of 5, as in t-2 t-1 t t+1 t+2)\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "\n",
        "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
        "labels_location = \"labels.txt\" #the file to write the labels to\n",
        "\n",
        "# We reuse the data reading from the language modeling class\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "S = w2i[\"<s>\"]\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      yield [w2i[x] for x in line.strip().split(\" \")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pc-V-N_F6ESJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Read in the data\n",
        "train = list(read_dataset(\"nn4nlp-code/data/ptb/train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"nn4nlp-code/data/ptb/valid.txt\"))\n",
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "47r1FUun6GOg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with open(labels_location, 'w') as labels_file:\n",
        "  for i in range(nwords):\n",
        "    labels_file.write(i2w[i] + '\\n')\n",
        "\n",
        "# Start DyNet and define trainer\n",
        "model = dy.Model()\n",
        "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4L-43ZT06I-y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Start DyNet and define trainer\n",
        "model = dy.Model()\n",
        "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.1)\n",
        "\n",
        "# Define the model\n",
        "W_c_p = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word weights at each position\n",
        "W_w_p = model.add_parameters((nwords, EMB_SIZE))         # Weights of the softmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8938NP8l66Ei",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Calculate the loss value for the entire sentence\n",
        "def calc_sent_loss(sent):\n",
        "  # Create a computation graph\n",
        "  dy.renew_cg()\n",
        "  \n",
        "  #add padding to the sentence equal to the size of the window\n",
        "  #as we need to predict the eos as well, the future window at that point is N past it \n",
        "  padded_sent = [S] * N + sent + [S] * N\n",
        "  padded_emb = [W_c_p[x] for x in padded_sent]\n",
        "\n",
        "  W_w = dy.parameter(W_w_p)\n",
        "\n",
        "  # Step through the sentence\n",
        "  all_losses = [] \n",
        "  for i in range(N,len(sent)+N):\n",
        "    c = dy.esum(padded_emb[i-N:i] + padded_emb[i+1:i+N+1])\n",
        "    s = W_w * c\n",
        "    all_losses.append(dy.pickneglogsoftmax(s, padded_sent[i]))\n",
        "  return dy.esum(all_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LaQSAD9Y7_9D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "40299f2a-c495-44ad-c525-565222db80e6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526393370357,
          "user_tz": -540,
          "elapsed": 514,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "calc_sent_loss(sent)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "expression 121/4298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "M-fSvQOp67-0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "a269d09e-6167-494d-d994-5c9b7af983f4"
      },
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "\n",
        "for ITER in range(100):\n",
        "  print(\"started iter %r\" % ITER)\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(train):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    train_loss += my_loss.value()\n",
        "    train_words += len(sent)\n",
        "    my_loss.backward()\n",
        "    trainer.update()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences\" % (sent_id+1))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(dev):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    dev_loss += my_loss.value()\n",
        "    dev_words += len(sent)\n",
        "    trainer.update()\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
        "\n",
        "  print(\"saving embedding files\")\n",
        "  with open(embeddings_location, 'w') as embeddings_file:\n",
        "    W_w_np = W_w_p.as_array()\n",
        "    for i in range(nwords):\n",
        "      ith_embedding = '\\t'.join(map(str, W_w_np[i]))\n",
        "      embeddings_file.write(ith_embedding + '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started iter 0\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 0: train loss/word=5.8570, ppl=349.6586, time=4235.18s\n",
            "iter 0: dev loss/word=5.4783, ppl=239.4457, time=113.97s\n",
            "saving embedding files\n",
            "started iter 1\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 1: train loss/word=5.2307, ppl=186.9267, time=4173.08s\n",
            "iter 1: dev loss/word=5.2924, ppl=198.8171, time=102.28s\n",
            "saving embedding files\n",
            "started iter 2\n",
            "--finished 5000 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6j_hjsIQ69xb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}