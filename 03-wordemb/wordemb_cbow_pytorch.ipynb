{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordemb-cbow_pytorch.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Zr4f5FCx86TY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "7a8cd9fb-c8eb-40e9-9fac-c415b5fa8e72",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526390427666,
          "user_tz": -540,
          "elapsed": 81885,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 24kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5bf5c000 @  0x7fc0e503f1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 12.8MB/s \n",
            "\u001b[?25hCollecting tqdm (from torchtext)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/ca/6524dfba7a0e850d3fda223693779035ddc8bf5c242acd9ee4eb9e52711a/tqdm-4.23.3-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.4.16)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torch, tqdm, torchtext\n",
            "Successfully installed torch-0.4.0 torchtext-0.2.3 tqdm-4.23.3\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cLoJ9QkI9EH8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Vy7r7FT9sxH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "N=2 #length of window on each side (so N=2 gives a total window size of 5, as in t-2 t-1 t t+1 t+2)\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "\n",
        "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
        "labels_location = \"labels.txt\" #the file to write the labels to\n",
        "\n",
        "# We reuse the data reading from the language modeling class\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "S = w2i[\"<s>\"]\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      yield [w2i[x] for x in line.strip().split(\" \")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-LNj4rgh-7s4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Read in the data\n",
        "train = list(read_dataset(\"nn4nlp-code/data/ptb/train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"nn4nlp-code/data/ptb/valid.txt\"))\n",
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)\n",
        "\n",
        "with open(labels_location, 'w') as labels_file:\n",
        "  for i in range(nwords):\n",
        "    labels_file.write(i2w[i] + '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9YwLOzeV-8uL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim):\n",
        "    super(CBOW, self).__init__()\n",
        "    \n",
        "    self.embeddings_bag = nn.EmbeddingBag(vocab_size, embed_dim, mode='sum')\n",
        "    self.fcl = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.embeddings_bag(x.view(1, -1))\n",
        "    return self.fcl(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JY5iI5WmAMvk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = CBOW(nwords, EMB_SIZE)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mHRHAZnjAN2H",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Calculate the loss value for the entire sentence\n",
        "def calc_sent_loss(sent):\n",
        "  #add padding to the sentence equal to the size of the window\n",
        "  #as we need to predict the eos as well, the future window at that point is N past it \n",
        "  padded_sent = [S] * N + sent + [S] * N\n",
        "\n",
        "  # Step through the sentence\n",
        "  all_losses = [] \n",
        "  for i in range(N,len(sent)+N):\n",
        "    model.zero_grad()\n",
        "    \n",
        "    logits = model(torch.LongTensor(padded_sent[i-N:i] + padded_sent[i+1:i+N+1]))\n",
        "    loss = F.cross_entropy(logits, torch.tensor(padded_sent[i]).view(1))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    all_losses.append(loss.cpu().detach().numpy())\n",
        "  return sum(all_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXXBWXQCGrfC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L2o_CDXCCNyj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "5be64e61-5684-480f-b0f1-6d8c36935017"
      },
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "\n",
        "for ITER in range(100):\n",
        "  print(\"started iter %r\" % ITER)\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(train):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    train_loss += my_loss\n",
        "    train_words += len(sent)\n",
        "#     my_loss.backward()\n",
        "#     trainer.update()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences\" % (sent_id+1))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(dev):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    dev_loss += my_loss\n",
        "    dev_words += len(sent)\n",
        "#     trainer.update()\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
        "\n",
        "  print(\"saving embedding files\")\n",
        "  with open(embeddings_location, 'w') as embeddings_file:\n",
        "    W_w_np = W_w_p.as_array()\n",
        "    for i in range(nwords):\n",
        "      ith_embedding = '\\t'.join(map(str, W_w_np[i]))\n",
        "      embeddings_file.write(ith_embedding + '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started iter 0\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yTP83RTnFRg5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}