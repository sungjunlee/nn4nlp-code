{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordemb-skip.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "m_xDtA7f7l6U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "fc8f1ddd-0ca4-4c25-804e-9d24b7b25844",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1526222259264,
          "user_tz": -540,
          "elapsed": 4916,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dynet in /usr/local/lib/python2.7/dist-packages (2.0.3)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from dynet) (1.14.3)\r\n",
            "Requirement already satisfied: cython in /usr/local/lib/python2.7/dist-packages (from dynet) (0.28.2)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ykFsyP2l7s2w",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import dynet as dy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OMrEjPYS7pWr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "N=2 #length of window on each side (so N=2 gives a total window size of 5, as in t-2 t-1 t t+1 t+2)\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "\n",
        "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
        "labels_location = \"labels.txt\" #the file to write the labels to\n",
        "\n",
        "# We reuse the data reading from the language modeling class\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "S = w2i[\"<s>\"]\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      yield [w2i[x] for x in line.strip().split(\" \")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G03AFuUA7xwd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Read in the data\n",
        "train = list(read_dataset(\"nn4nlp-code/data/ptb/train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"nn4nlp-code/data/ptb/valid.txt\"))\n",
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)\n",
        "\n",
        "with open(labels_location, 'w') as labels_file:\n",
        "  for i in range(nwords):\n",
        "    labels_file.write(i2w[i] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e5-FLkPg7zSs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Start DyNet and define trainer\n",
        "model = dy.Model()\n",
        "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.1)\n",
        "\n",
        "# Define the model\n",
        "W_c_p = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word weights at each position\n",
        "W_w_p = model.add_parameters((nwords, EMB_SIZE))         # Weights of the softmax\n",
        "\n",
        "# Calculate the loss value for the entire sentence\n",
        "def calc_sent_loss(sent):\n",
        "  # Create a computation graph\n",
        "  dy.renew_cg()\n",
        "  \n",
        "  #add padding to the sentence equal to the size of the window\n",
        "  #as we need to predict the eos as well, the future window at that point is N past it \n",
        "  emb = [W_c_p[x] for x in sent]\n",
        "\n",
        "  W_w = dy.parameter(W_w_p)\n",
        "\n",
        "  # Step through the sentence\n",
        "  all_losses = [] \n",
        "  for i, my_emb in enumerate(emb):\n",
        "    s = W_w * my_emb\n",
        "    lp = dy.log_softmax(s)\n",
        "    for j in range(1,N+1):\n",
        "      all_losses.append(dy.pick(lp, sent[i-j] if i-j >= 0 else S))\n",
        "      all_losses.append(dy.pick(lp, sent[i+j] if i+j < len(sent) else S))\n",
        "  return dy.esum(all_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-9LM3Rn7p_f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "4798a360-f78c-45c7-a675-792d8a0de4ff"
      },
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "\n",
        "for ITER in range(100):\n",
        "  print(\"started iter %r\" % ITER)\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(train):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    train_loss += my_loss.value()\n",
        "    train_words += len(sent)\n",
        "    my_loss.backward()\n",
        "    trainer.update()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences\" % (sent_id+1))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(dev):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    dev_loss += my_loss.value()\n",
        "    dev_words += len(sent)\n",
        "    trainer.update()\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
        "\n",
        "  print(\"saving embedding files\")\n",
        "  with open(embeddings_location, 'w') as embeddings_file:\n",
        "    W_w_np = W_w_p.as_array()\n",
        "    for i in range(nwords):\n",
        "      ith_embedding = '\\t'.join(map(str, W_w_np[i]))\n",
        "      embeddings_file.write(ith_embedding + '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started iter 0\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 0: train loss/word=-8479525.5423, ppl=0.0000, time=3997.36s\n",
            "iter 0: dev loss/word=-25437023.9444, ppl=0.0000, time=101.85s\n",
            "saving embedding files\n",
            "started iter 1\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}