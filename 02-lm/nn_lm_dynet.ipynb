{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn-lm_dynet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1NklpTikd6rDldr-jSv_OpjPcATonmEEL",
          "timestamp": 1525967273447
        }
      ],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "NbpH46NhWXlE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "f09701ba-59fa-4873-a605-436e69dadd7a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525967387298,
          "user_tz": -540,
          "elapsed": 6848,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dynet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/de/181a8380e9fdb89d9aa5838059336bb535503d5f2053e621438e69081407/dyNET-2.0.3-cp27-cp27mu-manylinux1_x86_64.whl (27.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 27.6MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from dynet) (1.14.3)\n",
            "Collecting cython (from dynet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/d6/a097bd9913cc0fc974b968f5586d3f0609f46ca58b2aae3b8dfd51c1fe18/Cython-0.28.2-cp27-cp27mu-manylinux1_x86_64.whl (3.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.3MB 9.5MB/s \n",
            "\u001b[?25hInstalling collected packages: cython, dynet\n",
            "Successfully installed cython-0.28.2 dynet-2.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Nh8HTurXHyZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "cc0e61b2-c2a6-4765-ce1e-1400e4ea2595",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525967391340,
          "user_tz": -540,
          "elapsed": 3991,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nn4nlp-code'...\n",
            "remote: Counting objects: 372, done.\u001b[K\n",
            "remote: Total 372 (delta 0), reused 0 (delta 0), pack-reused 372\u001b[K\n",
            "Receiving objects: 100% (372/372), 6.33 MiB | 19.81 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4UGDlQCGWxgs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import dynet as dy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RAMj1moWy9f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "N = 2 # The length of the n-gram\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "HID_SIZE = 128 # The size of the hidden layer\n",
        "\n",
        "# Functions to read in the corpus\n",
        "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
        "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
        "#       data we would have to do pre-processing and consider how to choose\n",
        "#       unknown words, etc.\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "S = w2i[\"<s>\"]\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      yield [w2i[x] for x in line.strip().split(\" \")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BS61BVrJW1Nx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Read in the data\n",
        "train = list(read_dataset(\"nn4nlp-code/data/ptb/train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"nn4nlp-code/data/ptb/valid.txt\"))\n",
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZM4zb9bFW4Cy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Start DyNet and define trainer\n",
        "model = dy.Model()\n",
        "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJNy19A0W8iS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word weights at each position\n",
        "W_h_p = model.add_parameters((HID_SIZE, EMB_SIZE * N))    # Weights of the softmax\n",
        "b_h_p = model.add_parameters((HID_SIZE))                  # Weights of the softmax\n",
        "W_sm_p = model.add_parameters((nwords, HID_SIZE))         # Weights of the softmax\n",
        "b_sm_p = model.add_parameters((nwords))                   # Softmax bias\n",
        "\n",
        "# A function to calculate scores for one value\n",
        "def calc_score_of_history(words):\n",
        "  # Lookup the embeddings and concatenate them\n",
        "  emb = dy.concatenate([W_emb[x] for x in words])\n",
        "  # Create the hidden layer\n",
        "  W_h = dy.parameter(W_h_p)\n",
        "  b_h = dy.parameter(b_h_p)\n",
        "  h = dy.tanh(dy.affine_transform([b_h, W_h, emb]))\n",
        "  # Calculate the score and return\n",
        "  W_sm = dy.parameter(W_sm_p)\n",
        "  b_sm = dy.parameter(b_sm_p)\n",
        "  return dy.affine_transform([b_sm, W_sm, h])\n",
        "\n",
        "# Calculate the loss value for the entire sentence\n",
        "def calc_sent_loss(sent):\n",
        "  # Create a computation graph\n",
        "  dy.renew_cg()\n",
        "  # The initial history is equal to end of sentence symbols\n",
        "  hist = [S] * N\n",
        "  # Step through the sentence, including the end of sentence token\n",
        "  all_losses = []\n",
        "  for next_word in sent + [S]:\n",
        "    s = calc_score_of_history(hist)\n",
        "    all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
        "    hist = hist[1:] + [next_word]\n",
        "  return dy.esum(all_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXOnrsHhW_j2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "# Generate a sentence\n",
        "def generate_sent():\n",
        "  dy.renew_cg()\n",
        "  hist = [S] * N\n",
        "  sent = []\n",
        "  while True:\n",
        "    p = dy.softmax(calc_score_of_history(hist)).npvalue()\n",
        "    next_word = np.random.choice(nwords, p=p/p.sum())\n",
        "    if next_word == S or len(sent) == MAX_LEN:\n",
        "      break\n",
        "    sent.append(next_word)\n",
        "    hist = hist[1:] + [next_word]\n",
        "  return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0aljWYXRWa4A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1ac172ac-5ad3-4989-940a-446a27f8b155"
      },
      "cell_type": "code",
      "source": [
        "for ITER in range(100):\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(train):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    train_loss += my_loss.value()\n",
        "    train_words += len(sent)\n",
        "    my_loss.backward()\n",
        "    trainer.update()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences\" % (sent_id+1))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(dev):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    dev_loss += my_loss.value()\n",
        "    dev_words += len(sent)\n",
        "    trainer.update()\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
        "  # Generate a few sentences\n",
        "  for _ in range(5):\n",
        "    sent = generate_sent()\n",
        "    print(\" \".join([i2w[x] for x in sent]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2DcvvmV5WiRX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}