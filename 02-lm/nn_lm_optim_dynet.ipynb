{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn-lm-optim_dynet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1t1CszPT9w82cXXuWGgRsHHdV3pGA5GRB",
          "timestamp": 1525967513417
        },
        {
          "file_id": "1NklpTikd6rDldr-jSv_OpjPcATonmEEL",
          "timestamp": 1525967273447
        }
      ],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "NbpH46NhWXlE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9a3cb54f-17ba-406b-d7a3-a7aa9872f6c7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525967715175,
          "user_tz": -540,
          "elapsed": 3391,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dynet in /usr/local/lib/python2.7/dist-packages (2.0.3)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from dynet) (1.14.3)\r\n",
            "Requirement already satisfied: cython in /usr/local/lib/python2.7/dist-packages (from dynet) (0.28.2)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0Nh8HTurXHyZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6084a309-65b3-4d5f-efaa-77a1f64077eb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525967717180,
          "user_tz": -540,
          "elapsed": 1978,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4UGDlQCGWxgs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import dynet as dy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4RAMj1moWy9f",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "N = 2 # The length of the n-gram\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "HID_SIZE = 128 # The size of the hidden layer\n",
        "\n",
        "# Functions to read in the corpus\n",
        "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
        "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
        "#       data we would have to do pre-processing and consider how to choose\n",
        "#       unknown words, etc.\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "S = w2i[\"<s>\"]\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      yield [w2i[x] for x in line.strip().split(\" \")]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BS61BVrJW1Nx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Read in the data\n",
        "train = list(read_dataset(\"nn4nlp-code/data/ptb/train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"nn4nlp-code/data/ptb/valid.txt\"))\n",
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZM4zb9bFW4Cy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Start DyNet and define trainer\n",
        "model = dy.Model()\n",
        "\n",
        "# CHANGE 1: Use Adam instead of Simple SGD\n",
        "trainer = dy.AdamTrainer(model, alpha=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJNy19A0W8iS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "W_emb = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word weights at each position\n",
        "W_h_p = model.add_parameters((HID_SIZE, EMB_SIZE * N))    # Weights of the softmax\n",
        "b_h_p = model.add_parameters((HID_SIZE))                  # Weights of the softmax\n",
        "W_sm_p = model.add_parameters((nwords, HID_SIZE))         # Weights of the softmax\n",
        "b_sm_p = model.add_parameters((nwords))                   # Softmax bias\n",
        "\n",
        "# A function to calculate scores for one value\n",
        "def calc_score_of_history(words, dropout=0.0):\n",
        "  # Lookup the embeddings and concatenate them\n",
        "  emb = dy.concatenate([W_emb[x] for x in words])\n",
        "  # Create the hidden layer\n",
        "  W_h = dy.parameter(W_h_p)\n",
        "  b_h = dy.parameter(b_h_p)\n",
        "  h = dy.tanh(dy.affine_transform([b_h, W_h, emb]))\n",
        "  # CHANGE 2: perform dropout\n",
        "  if dropout != 0.0:\n",
        "    h = dy.dropout(h, dropout)\n",
        "  # Calculate the score and return\n",
        "  W_sm = dy.parameter(W_sm_p)\n",
        "  b_sm = dy.parameter(b_sm_p)\n",
        "  return dy.affine_transform([b_sm, W_sm, h])\n",
        "\n",
        "# Calculate the loss value for the entire sentence\n",
        "def calc_sent_loss(sent, dropout=0.0):\n",
        "  # Create a computation graph\n",
        "  dy.renew_cg()\n",
        "  # The initial history is equal to end of sentence symbols\n",
        "  hist = [S] * N\n",
        "  # Step through the sentence, including the end of sentence token\n",
        "  all_losses = []\n",
        "  for next_word in sent + [S]:\n",
        "    s = calc_score_of_history(hist, dropout=dropout)\n",
        "    all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
        "    hist = hist[1:] + [next_word]\n",
        "  return dy.esum(all_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXOnrsHhW_j2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "# Generate a sentence\n",
        "def generate_sent():\n",
        "  dy.renew_cg()\n",
        "  hist = [S] * N\n",
        "  sent = []\n",
        "  while True:\n",
        "    p = dy.softmax(calc_score_of_history(hist)).npvalue()\n",
        "    next_word = np.random.choice(nwords, p=p/p.sum())\n",
        "    if next_word == S or len(sent) == MAX_LEN:\n",
        "      break\n",
        "    sent.append(next_word)\n",
        "    hist = hist[1:] + [next_word]\n",
        "  return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0aljWYXRWa4A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "last_dev = 1e20\n",
        "best_dev = 1e20\n",
        "\n",
        "for ITER in range(100):\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(train):\n",
        "    my_loss = calc_sent_loss(sent, dropout=0.2)\n",
        "    train_loss += my_loss.value()\n",
        "    train_words += len(sent)\n",
        "    my_loss.backward()\n",
        "    trainer.update()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences\" % (sent_id+1))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(dev):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    dev_loss += my_loss.value()\n",
        "    dev_words += len(sent)\n",
        "    trainer.update()\n",
        "  # CHANGE 3: Keep track of the development accuracy and reduce the learning rate if it got worse\n",
        "  if last_dev < dev_loss:\n",
        "    trainer.learning_rate /= 2\n",
        "  last_dev = dev_loss\n",
        "  # CHANGE 4: Keep track of the best development accuracy, and save the model only if it's the best one\n",
        "  if best_dev > dev_loss:\n",
        "    model.save(\"model.txt\")\n",
        "    best_dev = dev_loss\n",
        "  # Save the model\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
        "  # Generate a few sentences\n",
        "  for _ in range(5):\n",
        "    sent = generate_sent()\n",
        "    print(\" \".join([i2w[x] for x in sent]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2DcvvmV5WiRX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}