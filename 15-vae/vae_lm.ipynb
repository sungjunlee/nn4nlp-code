{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae-lm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "trdmr-56xTkz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "262820be-05b7-4512-b08d-cbd9abbd4c89"
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dynet in /usr/local/lib/python2.7/dist-packages (2.0.3)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from dynet) (1.14.5)\r\n",
            "Requirement already satisfied: cython in /usr/local/lib/python2.7/dist-packages (from dynet) (0.28.5)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6aPCLjZQxbeY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import dynet as dy\n",
        "import numpy as np\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cADZ8ETZxoNu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# much of the beginning is the same as the text retrieval\n",
        "# format of files: each line is \"word1 word2 ...\" aligned line-by-line\n",
        "train_src_file = \"nn4nlp-code/data/parallel/train.ja\"\n",
        "train_trg_file = \"nn4nlp-code//data/parallel/train.en\"\n",
        "dev_src_file = \"nn4nlp-code//data/parallel/dev.ja\"\n",
        "dev_trg_file = \"nn4nlp-code//data/parallel/dev.en\"\n",
        "test_src_file = \"nn4nlp-code//data/parallel/test.ja\"\n",
        "test_trg_file = \"nn4nlp-code//data/parallel/test.en\"\n",
        "\n",
        "w2i_src = defaultdict(lambda: len(w2i_src))\n",
        "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
        "\n",
        "\n",
        "def read(fname_src, fname_trg):\n",
        "    \"\"\"\n",
        "    Read parallel files where each line lines up\n",
        "    \"\"\"\n",
        "    with open(fname_src, \"r\") as f_src, open(fname_trg, \"r\") as f_trg:\n",
        "        for line_src, line_trg in zip(f_src, f_trg):\n",
        "            # need to append EOS tags to at least the target sentence\n",
        "            sent_src = [w2i_src[x] for x in line_src.strip().split() + ['</s>']]\n",
        "            sent_trg = [w2i_trg[x] for x in ['<s>'] + line_trg.strip().split() + ['</s>']]\n",
        "            yield (sent_src, sent_trg)\n",
        "\n",
        "\n",
        "# Read the data\n",
        "train = list(read(train_src_file, train_trg_file))\n",
        "unk_src = w2i_src[\"<unk>\"]\n",
        "eos_src = w2i_src['</s>']\n",
        "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
        "unk_trg = w2i_trg[\"<unk>\"]\n",
        "eos_trg = w2i_trg['</s>']\n",
        "sos_trg = w2i_trg['<s>']\n",
        "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
        "i2w_trg = {v: k for k, v in w2i_trg.items()}\n",
        "\n",
        "nwords_src = len(w2i_src)\n",
        "nwords_trg = len(w2i_trg)\n",
        "dev = list(read(dev_src_file, dev_trg_file))\n",
        "test = list(read(test_src_file, test_trg_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Kw7rOGexOWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DyNet Starts\n",
        "model = dy.Model()\n",
        "trainer = dy.AdamTrainer(model)\n",
        "\n",
        "# Model parameters\n",
        "EMBED_SIZE = 64\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Especially in early training, the model can generate basically infinitly without generating an EOS\n",
        "# have a max sent size that you end at\n",
        "MAX_SENT_SIZE = 50\n",
        "\n",
        "# Lookup parameters for word embeddings\n",
        "LOOKUP_SRC = model.add_lookup_parameters((nwords_src, EMBED_SIZE))\n",
        "LOOKUP_TRG = model.add_lookup_parameters((nwords_trg, EMBED_SIZE))\n",
        "\n",
        "# Word-level LSTMs\n",
        "LSTM_SRC_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model)\n",
        "LSTM_TRG_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model)\n",
        "\n",
        "# The MLP parameters to compute mean variance from source output. We use the same hidden size for simplicity.\n",
        "Q_HIDDEN_SIZE = 64\n",
        "W_mean_p = model.add_parameters((Q_HIDDEN_SIZE, HIDDEN_SIZE))\n",
        "V_mean_p = model.add_parameters((HIDDEN_SIZE, Q_HIDDEN_SIZE))\n",
        "b_mean_p = model.add_parameters((Q_HIDDEN_SIZE))\n",
        "\n",
        "W_var_p = model.add_parameters((Q_HIDDEN_SIZE, HIDDEN_SIZE))\n",
        "V_var_p = model.add_parameters((HIDDEN_SIZE, Q_HIDDEN_SIZE))\n",
        "b_var_p = model.add_parameters((Q_HIDDEN_SIZE))\n",
        "\n",
        "# the softmax from the hidden size\n",
        "W_sm_p = model.add_parameters((nwords_trg, HIDDEN_SIZE))  # Weights of the softmax\n",
        "b_sm_p = model.add_parameters((nwords_trg))  # Softmax bias\n",
        "\n",
        "\n",
        "def reparameterize(mu, logvar):\n",
        "    # Get z by reparameterization.\n",
        "    d = mu.dim()[0][0]\n",
        "    eps = dy.random_normal(d)\n",
        "    std = dy.exp(logvar * 0.5)\n",
        "\n",
        "    return mu + dy.cmult(std, eps)\n",
        "\n",
        "\n",
        "def mlp(x, W, V, b):\n",
        "    # A mlp with only one hidden layer.\n",
        "    return V * dy.tanh(W * x + b)\n",
        "\n",
        "\n",
        "def calc_loss(sent):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    src = sent[0]\n",
        "    trg = sent[1]\n",
        "\n",
        "    # initialize the LSTM\n",
        "    init_state_src = LSTM_SRC_BUILDER.initial_state()\n",
        "\n",
        "    # get the output of the first LSTM\n",
        "    src_output = init_state_src.add_inputs([LOOKUP_SRC[x] for x in src])[-1].output()\n",
        "\n",
        "    # Now compute mean and standard deviation of source hidden state.\n",
        "    W_mean = dy.parameter(W_mean_p)\n",
        "    V_mean = dy.parameter(V_mean_p)\n",
        "    b_mean = dy.parameter(b_mean_p)\n",
        "\n",
        "    W_var = dy.parameter(W_var_p)\n",
        "    V_var = dy.parameter(V_var_p)\n",
        "    b_var = dy.parameter(b_var_p)\n",
        "\n",
        "    # The mean vector from the encoder.\n",
        "    mu = mlp(src_output, W_mean, V_mean, b_mean)\n",
        "    # This is the diagonal vector of the log co-variance matrix from the encoder\n",
        "    # (regard this as log variance is easier for furture implementation)\n",
        "    log_var = mlp(src_output, W_var, V_var, b_var)\n",
        "\n",
        "    # Compute KL[N(u(x), sigma(x)) || N(0, I)]\n",
        "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    kl_loss = -0.5 * dy.sum_elems(1 + log_var - dy.pow(mu, dy.inputVector([2])) - dy.exp(log_var))\n",
        "\n",
        "    z = reparameterize(mu, log_var)\n",
        "\n",
        "    # now step through the output sentence\n",
        "    all_losses = []\n",
        "\n",
        "    current_state = LSTM_TRG_BUILDER.initial_state().set_s([z, dy.tanh(z)])\n",
        "    prev_word = trg[0]\n",
        "    W_sm = dy.parameter(W_sm_p)\n",
        "    b_sm = dy.parameter(b_sm_p)\n",
        "\n",
        "    for next_word in trg[1:]:\n",
        "        # feed the current state into the\n",
        "        current_state = current_state.add_input(LOOKUP_TRG[prev_word])\n",
        "        output_embedding = current_state.output()\n",
        "\n",
        "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
        "        all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
        "\n",
        "        prev_word = next_word\n",
        "\n",
        "    softmax_loss = dy.esum(all_losses)\n",
        "\n",
        "    return kl_loss, softmax_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4hH5U9wx2Lv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "fda64c7e-d5c9-4449-8c99-a67719f7e539"
      },
      "cell_type": "code",
      "source": [
        "for ITER in range(100):\n",
        "    # Perform training\n",
        "    random.shuffle(train)\n",
        "    train_words, train_loss, train_kl_loss, train_reconstruct_loss = 0, 0.0, 0.0, 0.0\n",
        "    start = time.time()\n",
        "    for sent_id, sent in enumerate(train):\n",
        "        kl_loss, softmax_loss = calc_loss(sent)\n",
        "        total_loss = dy.esum([kl_loss, softmax_loss])\n",
        "        train_loss += total_loss.value()\n",
        "\n",
        "        # Record the KL loss and reconstruction loss separately help you monitor the training.\n",
        "        train_kl_loss += kl_loss.value()\n",
        "        train_reconstruct_loss += softmax_loss.value()\n",
        "\n",
        "        train_words += len(sent)\n",
        "        total_loss.backward()\n",
        "        trainer.update()\n",
        "        if (sent_id + 1) % 1000 == 0:\n",
        "            print(\"--finished %r sentences\" % (sent_id + 1))\n",
        "\n",
        "    print(\"iter %r: train loss/word=%.4f, kl loss/word=%.4f, reconstruction loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (\n",
        "        ITER, train_loss / train_words, train_kl_loss / train_words, train_reconstruct_loss / train_words,\n",
        "        math.exp(train_loss / train_words), time.time() - start))\n",
        "\n",
        "    # Evaluate on dev set\n",
        "    dev_words, dev_loss, dev_kl_loss, dev_reconstruct_loss = 0, 0.0, 0.0, 0.0\n",
        "    start = time.time()\n",
        "    for sent_id, sent in enumerate(dev):\n",
        "        kl_loss, softmax_loss = calc_loss(sent)\n",
        "\n",
        "        dev_kl_loss += kl_loss.value()\n",
        "        dev_reconstruct_loss += softmax_loss.value()\n",
        "        dev_loss += kl_loss.value() + softmax_loss.value()\n",
        "\n",
        "        dev_words += len(sent)\n",
        "        trainer.update()\n",
        "\n",
        "    print(\"iter %r: dev loss/word=%.4f, kl loss/word=%.4f, reconstruction loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (\n",
        "        ITER, dev_loss / dev_words, dev_kl_loss / dev_words, dev_reconstruct_loss / dev_words,\n",
        "        math.exp(dev_loss / dev_words), time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--finished 1000 sentences\n",
            "--finished 2000 sentences\n",
            "--finished 3000 sentences\n",
            "--finished 4000 sentences\n",
            "--finished 5000 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W0bdd5lLyO6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}