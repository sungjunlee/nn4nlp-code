{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bilstm-tagger-dynet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MqGJCXGs5mYH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "e7cbb0e6-7c83-4b71-bbe2-d099efe43a66",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1531741484444,
          "user_tz": -540,
          "elapsed": 11655,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dynet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/8c/767cc83241b2abe567d705f87589d8ad44cca321f7c78720269c45e0469f/dyNET-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (27.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 27.8MB 1.3MB/s \n",
            "\u001b[?25hCollecting cython (from dynet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/10/ffebdb9faa901c83b69ab7040a1f5f3b2c71899be141752a6d466718c491/Cython-0.28.4-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.4MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dynet) (1.14.5)\n",
            "Installing collected packages: cython, dynet\n",
            "Successfully installed cython-0.28.4 dynet-2.0.3\n",
            "Cloning into 'nn4nlp-code'...\n",
            "remote: Counting objects: 372, done.\u001b[K\n",
            "remote: Total 372 (delta 0), reused 0 (delta 0), pack-reused 372\u001b[K\n",
            "Receiving objects: 100% (372/372), 6.33 MiB | 18.78 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CW_B8G_x5vtX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import dynet as dy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yt-MDLts50dE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# format of files: each line is \"word1|tag1 word2|tag2 ...\"\n",
        "train_file = \"nn4nlp-code/data/tags/train.txt\"\n",
        "dev_file = \"nn4nlp-code/data/tags/dev.txt\"\n",
        "\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "t2i = defaultdict(lambda: len(t2i))\n",
        "\n",
        "\n",
        "def read(fname):\n",
        "    \"\"\"\n",
        "    Read tagged file\n",
        "    \"\"\"\n",
        "    with open(fname, \"r\") as f:\n",
        "        for line in f:\n",
        "            words, tags = [], []\n",
        "            for wt in line.strip().split():\n",
        "                w, t = wt.split('|')\n",
        "                words.append(w2i[w])\n",
        "                tags.append(t2i[t])\n",
        "            yield (words, tags)\n",
        "\n",
        "\n",
        "# Read the data\n",
        "train = list(read(train_file))\n",
        "unk_word = w2i[\"<unk>\"]\n",
        "w2i = defaultdict(lambda: unk_word, w2i)\n",
        "unk_tag = t2i[\"<unk>\"]\n",
        "t2i = defaultdict(lambda: unk_tag, t2i)\n",
        "nwords = len(w2i)\n",
        "ntags = len(t2i)\n",
        "dev = list(read(dev_file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgo1lmyp55r8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DyNet Starts\n",
        "model = dy.Model()\n",
        "trainer = dy.AdamTrainer(model)\n",
        "\n",
        "# Model parameters\n",
        "EMBED_SIZE = 64\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "# Lookup parameters for word embeddings\n",
        "LOOKUP = model.add_lookup_parameters((nwords, EMBED_SIZE))\n",
        "\n",
        "# Word-level BiLSTM\n",
        "LSTM = dy.BiRNNBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model, dy.LSTMBuilder)\n",
        "\n",
        "# Word-level softmax\n",
        "W_sm = model.add_parameters((ntags, HIDDEN_SIZE))\n",
        "b_sm = model.add_parameters(ntags)\n",
        "\n",
        "\n",
        "# Calculate the scores for one example\n",
        "def calc_scores(words):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    word_reps = LSTM.transduce([LOOKUP[x] for x in words])\n",
        "\n",
        "    # Softmax scores\n",
        "    W = dy.parameter(W_sm)\n",
        "    b = dy.parameter(b_sm)\n",
        "    scores = [dy.affine_transform([b, W, x]) for x in word_reps]\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Calculate MLE loss for one example\n",
        "def calc_loss(scores, tags):\n",
        "    losses = [dy.pickneglogsoftmax(score, tag) for score, tag in zip(scores, tags)]\n",
        "    return dy.esum(losses)\n",
        "\n",
        "\n",
        "# Calculate number of tags correct for one example\n",
        "def calc_correct(scores, tags):\n",
        "    correct = [np.argmax(score.npvalue()) == tag for score, tag in zip(scores, tags)]\n",
        "    return sum(correct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aqzFXAhR5hr3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "1f805840-416d-4d51-9ef3-ab478d38830b"
      },
      "cell_type": "code",
      "source": [
        "# Perform training\n",
        "for ITER in range(100):\n",
        "    random.shuffle(train)\n",
        "    start = time.time()\n",
        "    this_sents = this_words = this_loss = this_correct = 0\n",
        "    for sid in range(0, len(train)):\n",
        "        this_sents += 1\n",
        "        if this_sents % int(1000) == 0:\n",
        "            print(\"train loss/word=%.4f, acc=%.2f%%, word/sec=%.4f\" % (\n",
        "                this_loss / this_words, 100 * this_correct / this_words, this_words / (time.time() - start)),\n",
        "                  file=sys.stderr)\n",
        "        # train on the example\n",
        "        words, tags = train[sid]\n",
        "        scores = calc_scores(words)\n",
        "        loss_exp = calc_loss(scores, tags)\n",
        "        this_correct += calc_correct(scores, tags)\n",
        "        this_loss += loss_exp.scalar_value()\n",
        "        this_words += len(words)\n",
        "        loss_exp.backward()\n",
        "        trainer.update()\n",
        "    # Perform evaluation \n",
        "    start = time.time()\n",
        "    this_sents = this_words = this_loss = this_correct = 0\n",
        "    for words, tags in dev:\n",
        "        this_sents += 1\n",
        "        scores = calc_scores(words)\n",
        "        loss_exp = calc_loss(scores, tags)\n",
        "        this_correct += calc_correct(scores, tags)\n",
        "        this_loss += loss_exp.scalar_value()\n",
        "        this_words += len(words)\n",
        "    print(\"dev loss/word=%.4f, acc=%.2f%%, word/sec=%.4f\" % (\n",
        "        this_loss / this_words, 100 * this_correct / this_words, this_words / (time.time() - start)), file=sys.stderr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.5794, acc=86.50%, word/sec=5527.2139\n",
            "train loss/word=0.5046, acc=86.94%, word/sec=5590.0005\n",
            "train loss/word=0.4581, acc=87.59%, word/sec=5526.4661\n",
            "train loss/word=0.4292, acc=88.02%, word/sec=5479.5203\n",
            "train loss/word=0.4030, acc=88.56%, word/sec=5489.2620\n",
            "train loss/word=0.3842, acc=88.93%, word/sec=5496.1088\n",
            "train loss/word=0.3686, acc=89.28%, word/sec=5414.9681\n",
            "train loss/word=0.3557, acc=89.58%, word/sec=5410.3987\n",
            "train loss/word=0.3436, acc=89.89%, word/sec=5405.9590\n",
            "train loss/word=0.3324, acc=90.18%, word/sec=5407.8844\n",
            "dev loss/word=0.3695, acc=87.59%, word/sec=16066.9803\n",
            "train loss/word=0.1871, acc=94.12%, word/sec=5435.4437\n",
            "train loss/word=0.1810, acc=94.35%, word/sec=5375.2543\n",
            "train loss/word=0.1810, acc=94.40%, word/sec=5274.1147\n",
            "train loss/word=0.1796, acc=94.49%, word/sec=5268.6465\n",
            "train loss/word=0.1793, acc=94.52%, word/sec=5239.0151\n",
            "train loss/word=0.1773, acc=94.60%, word/sec=5235.5572\n",
            "train loss/word=0.1769, acc=94.63%, word/sec=5262.8996\n",
            "train loss/word=0.1741, acc=94.72%, word/sec=5268.2157\n",
            "train loss/word=0.1730, acc=94.76%, word/sec=5264.0241\n",
            "train loss/word=0.1711, acc=94.81%, word/sec=5270.4147\n",
            "dev loss/word=0.3864, acc=88.65%, word/sec=15253.3370\n",
            "train loss/word=0.1067, acc=96.75%, word/sec=5507.3743\n",
            "train loss/word=0.1164, acc=96.52%, word/sec=5443.4367\n",
            "train loss/word=0.1188, acc=96.48%, word/sec=5382.7983\n",
            "train loss/word=0.1197, acc=96.48%, word/sec=5357.7230\n",
            "train loss/word=0.1199, acc=96.46%, word/sec=5336.7158\n",
            "train loss/word=0.1186, acc=96.51%, word/sec=5353.8949\n",
            "train loss/word=0.1180, acc=96.52%, word/sec=5349.1238\n",
            "train loss/word=0.1174, acc=96.56%, word/sec=5298.8088\n",
            "train loss/word=0.1166, acc=96.60%, word/sec=5301.7726\n",
            "train loss/word=0.1162, acc=96.61%, word/sec=5315.6161\n",
            "dev loss/word=0.3953, acc=89.57%, word/sec=15098.8475\n",
            "train loss/word=0.0966, acc=97.42%, word/sec=5245.2362\n",
            "train loss/word=0.0909, acc=97.43%, word/sec=5379.5099\n",
            "train loss/word=0.0882, acc=97.50%, word/sec=5396.1635\n",
            "train loss/word=0.0879, acc=97.51%, word/sec=5394.6693\n",
            "train loss/word=0.0866, acc=97.56%, word/sec=5393.5610\n",
            "train loss/word=0.0848, acc=97.61%, word/sec=5401.4505\n",
            "train loss/word=0.0840, acc=97.62%, word/sec=5427.1517\n",
            "train loss/word=0.0829, acc=97.66%, word/sec=5431.0033\n",
            "train loss/word=0.0830, acc=97.67%, word/sec=5437.7526\n",
            "train loss/word=0.0829, acc=97.66%, word/sec=5446.6826\n",
            "dev loss/word=0.4784, acc=89.11%, word/sec=13806.2074\n",
            "train loss/word=0.0600, acc=98.40%, word/sec=5161.2096\n",
            "train loss/word=0.0554, acc=98.47%, word/sec=5235.6880\n",
            "train loss/word=0.0556, acc=98.49%, word/sec=5280.7848\n",
            "train loss/word=0.0558, acc=98.47%, word/sec=5284.0634\n",
            "train loss/word=0.0565, acc=98.45%, word/sec=5288.3896\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}