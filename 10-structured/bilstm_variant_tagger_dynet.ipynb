{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bilstm-variant-tagger-dynet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "HFhZ5CEFA_p3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "a00e2dda-653a-4195-93a5-3f36e2c7d563",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532008252312,
          "user_tz": -540,
          "elapsed": 5171,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dynet in /usr/local/lib/python3.6/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dynet) (1.14.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from dynet) (0.28.4)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xMRvKjI9BL-G",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import dynet as dy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJjH4rZ3BPaG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='BiLSTM variants.')\n",
        "parser.add_argument('--teacher', action='store_true')\n",
        "parser.add_argument('--perceptron', action='store_true')\n",
        "parser.add_argument('--cost', action='store_true')\n",
        "parser.add_argument('--hinge', action='store_true')\n",
        "parser.add_argument('--schedule', action='store_true')\n",
        "\n",
        "opts = ['--teacher']\n",
        "\n",
        "args = parser.parse_args(opts)\n",
        "use_teacher_forcing = args.teacher\n",
        "use_structure_perceptron = args.perceptron\n",
        "use_cost_augmented = args.cost\n",
        "use_hinge = args.hinge\n",
        "use_schedule = args.schedule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UEntyalEBaLx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d250986-d5a3-41b1-b8d9-bab0c5091ff3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532008255516,
          "user_tz": -540,
          "elapsed": 1384,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Training BiLSTM %s teacher forcing (%s schedule), %s structured perceptron loss, %s augmented cost, %s margin.\"\n",
        "      % (\"with\" if use_teacher_forcing else \"without\",\n",
        "         \"with\" if use_schedule else \"without\",\n",
        "         \"with\" if use_structure_perceptron else \"without\",\n",
        "         \"with\" if use_cost_augmented else \"without\",\n",
        "         \"with\" if use_hinge else \"without\"\n",
        "         )\n",
        "      )\n",
        "\n",
        "# format of files: each line is \"word1|tag1 word2|tag2 ...\"\n",
        "train_file = \"nn4nlp-code/data/tags/train.txt\"\n",
        "dev_file = \"nn4nlp-code/data/tags/dev.txt\"\n",
        "\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "t2i = defaultdict(lambda: len(t2i))\n",
        "\n",
        "\n",
        "def read(fname):\n",
        "    \"\"\"\n",
        "    Read tagged file\n",
        "    \"\"\"\n",
        "    with open(fname, \"r\") as f:\n",
        "        for line in f:\n",
        "            words, tags = [], []\n",
        "            for wt in line.strip().split():\n",
        "                w, t = wt.split('|')\n",
        "                words.append(w2i[w])\n",
        "                tags.append(t2i[t])\n",
        "            yield (words, tags)\n",
        "\n",
        "\n",
        "class AlwaysTrueSampler:\n",
        "    \"\"\"\n",
        "    An always true sampler, only sample fromtrue distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    def sample_true(self):\n",
        "        return True\n",
        "\n",
        "    def decay(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class ScheduleSampler:\n",
        "    \"\"\"\n",
        "    A linear schedule sampler.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_rate=1, min_rate=0.2, decay_rate=0.1):\n",
        "        self.min_rate = min_rate\n",
        "        self.iter = 0\n",
        "        self.decay_rate = decay_rate\n",
        "        self.start_rate = start_rate\n",
        "        self.reach_min = False\n",
        "        self.sample_rate = start_rate\n",
        "\n",
        "    def decay_func(self):\n",
        "        if not self.reach_min:\n",
        "            self.sample_rate = self.start_rate - self.iter * self.decay_rate\n",
        "            if self.sample_rate < self.min_rate:\n",
        "                self.reach_min = True\n",
        "                self.sample_rate = self.min_rate\n",
        "\n",
        "    def decay(self):\n",
        "        self.iter += 1\n",
        "        self.decay_func()\n",
        "        print(\"Sample rate is now %.2f\" % self.sample_rate)\n",
        "\n",
        "    def sample_true(self):\n",
        "        return random.random() < self.sample_rate\n",
        "\n",
        "\n",
        "# Read the data\n",
        "train = list(read(train_file))\n",
        "unk_word = w2i[\"<unk>\"]\n",
        "w2i = defaultdict(lambda: unk_word, w2i)\n",
        "unk_tag = t2i[\"<unk>\"]\n",
        "start_tag = t2i[\"<start>\"]\n",
        "t2i = defaultdict(lambda: unk_tag, t2i)\n",
        "nwords = len(w2i)\n",
        "ntags = len(t2i)\n",
        "dev = list(read(dev_file))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training BiLSTM with teacher forcing (without schedule), without structured perceptron loss, without augmented cost, without margin.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hv4CThJhBfm6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DyNet Starts\n",
        "model = dy.Model()\n",
        "trainer = dy.AdamTrainer(model)\n",
        "\n",
        "# Model parameters\n",
        "EMBED_SIZE = 64\n",
        "TAG_EMBED_SIZE = 16\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "assert HIDDEN_SIZE % 2 == 0\n",
        "\n",
        "# Lookup parameters for word embeddings\n",
        "LOOKUP = model.add_lookup_parameters((nwords, EMBED_SIZE))\n",
        "\n",
        "if use_teacher_forcing:\n",
        "    TAG_LOOKUP = model.add_lookup_parameters((ntags, TAG_EMBED_SIZE))\n",
        "\n",
        "if use_schedule:\n",
        "    sampler = ScheduleSampler()\n",
        "else:\n",
        "    sampler = AlwaysTrueSampler()\n",
        "\n",
        "# Word-level BiLSTM is just a composition of two LSTMs.\n",
        "if use_teacher_forcing:\n",
        "    fwdLSTM = dy.SimpleRNNBuilder(1, EMBED_SIZE + TAG_EMBED_SIZE, HIDDEN_SIZE / 2, model)  # Forward LSTM\n",
        "else:\n",
        "    fwdLSTM = dy.SimpleRNNBuilder(1, EMBED_SIZE, HIDDEN_SIZE / 2, model)  # Forward LSTM\n",
        "\n",
        "# We cannot insert previous predicted tag to the backward LSTM anyway.\n",
        "bwdLSTM = dy.SimpleRNNBuilder(1, EMBED_SIZE, HIDDEN_SIZE / 2, model)  # Backward LSTM\n",
        "\n",
        "# Word-level softmax\n",
        "W_sm = model.add_parameters((ntags, HIDDEN_SIZE))\n",
        "b_sm = model.add_parameters(ntags)\n",
        "\n",
        "\n",
        "# Calculate the scores for one example\n",
        "def calc_scores(words):\n",
        "    \"\"\"\n",
        "    Calculate scores using BiLSTM.\n",
        "    :param words:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    dy.renew_cg()\n",
        "\n",
        "    word_embs = [LOOKUP[x] for x in words]\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    fwd_init = fwdLSTM.initial_state()\n",
        "    fwd_word_reps = fwd_init.transduce(word_embs)\n",
        "    bwd_init = bwdLSTM.initial_state()\n",
        "    bwd_word_reps = bwd_init.transduce(reversed(word_embs))\n",
        "\n",
        "    combined_word_reps = [dy.concatenate([f, b]) for f, b in zip(fwd_word_reps, reversed(bwd_word_reps))]\n",
        "\n",
        "    # Softmax scores\n",
        "    W = dy.parameter(W_sm)\n",
        "    b = dy.parameter(b_sm)\n",
        "    scores = [dy.affine_transform([b, W, x]) for x in combined_word_reps]\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def calc_scores_with_previous_tag(words, referent_tags=None):\n",
        "    \"\"\"\n",
        "    Calculate scores using previous tag as input. If the referent tags are provided, then we will sample from previous\n",
        "    referent tag or previous system prediction.\n",
        "    :param words:\n",
        "    :param referent_tags:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    dy.renew_cg()\n",
        "\n",
        "    word_embs = [LOOKUP[x] for x in words]\n",
        "\n",
        "    # Transduce all batch elements for the backward LSTM, using the original word embeddings.\n",
        "    bwd_init = bwdLSTM.initial_state()\n",
        "    bwd_word_reps = bwd_init.transduce(reversed(word_embs))\n",
        "\n",
        "    # Softmax scores\n",
        "    W = dy.parameter(W_sm)\n",
        "    b = dy.parameter(b_sm)\n",
        "\n",
        "    scores = []\n",
        "    # Transduce one by one for the forward LSTM\n",
        "    fwd_init = fwdLSTM.initial_state()\n",
        "    s_fwd = fwd_init\n",
        "\n",
        "    prev_tag = start_tag\n",
        "\n",
        "    index = 0\n",
        "    for word, bwd_word_rep in zip(word_embs, reversed(bwd_word_reps)):\n",
        "        # Concatenate word and tag representation just as training.\n",
        "        fwd_input = dy.concatenate([word, TAG_LOOKUP[prev_tag]])\n",
        "        s_fwd = s_fwd.add_input(fwd_input)\n",
        "        combined_rep = dy.concatenate([s_fwd.output(), bwd_word_rep])\n",
        "        score = dy.affine_transform([b, W, combined_rep])\n",
        "        prediction = np.argmax(score.npvalue())\n",
        "\n",
        "        if referent_tags:\n",
        "            if sampler.sample_true():\n",
        "                prev_tag = referent_tags[index]\n",
        "            else:\n",
        "                prev_tag = prediction\n",
        "            index += 1\n",
        "        else:\n",
        "            prev_tag = prediction\n",
        "\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def mle(scores, tags):\n",
        "    losses = [dy.pickneglogsoftmax(score, tag) for score, tag in zip(scores, tags)]\n",
        "    return dy.esum(losses)\n",
        "\n",
        "\n",
        "def hamming_cost(predictions, reference):\n",
        "    return sum(p != r for p, r in zip(predictions, reference))\n",
        "\n",
        "\n",
        "def calc_sequence_score(scores, tags):\n",
        "    return dy.esum([score[tag] for score, tag in zip(scores, tags)])\n",
        "\n",
        "\n",
        "def hamming_augmented_decode(scores, reference):\n",
        "    \"\"\"\n",
        "    Local decoding with hamming cost.\n",
        "    :param scores: Local decoding scores.\n",
        "    :param reference: Referent tag result.\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    augmented_result = []\n",
        "    for score, referent_tag in zip(scores, reference):\n",
        "        origin_scores = score.npvalue()\n",
        "        cost = np.ones(origin_scores.shape)\n",
        "        cost[referent_tag] = 0\n",
        "        augmented_result.append(np.argmax(np.add(origin_scores, cost)))\n",
        "    return augmented_result\n",
        "\n",
        "\n",
        "def perceptron_loss(scores, reference):\n",
        "    if use_cost_augmented:\n",
        "        predictions = hamming_augmented_decode(scores, reference)\n",
        "    else:\n",
        "        predictions = [np.argmax(score.npvalue()) for score in scores]\n",
        "\n",
        "    margin = dy.scalarInput(-2)\n",
        "\n",
        "    if predictions != reference:\n",
        "        reference_score = calc_sequence_score(scores, reference)\n",
        "        prediction_score = calc_sequence_score(scores, predictions)\n",
        "        if use_cost_augmented:\n",
        "            # One could actually get the hamming augmented value during decoding, but we didn't do it here for\n",
        "            # demonstration purpose.\n",
        "            hamming = dy.scalarInput(hamming_cost(predictions, reference))\n",
        "            loss = prediction_score + hamming - reference_score\n",
        "        else:\n",
        "            loss = prediction_score - reference_score\n",
        "\n",
        "        if use_hinge:\n",
        "            loss = dy.emax([dy.scalarInput(0), loss - margin])\n",
        "\n",
        "        return loss\n",
        "    else:\n",
        "        return dy.scalarInput(0)\n",
        "\n",
        "\n",
        "# Calculate MLE loss for one example\n",
        "def calc_loss(scores, tags):\n",
        "    if use_structure_perceptron:\n",
        "        return perceptron_loss(scores, tags)\n",
        "    else:\n",
        "        return mle(scores, tags)\n",
        "\n",
        "\n",
        "# Calculate number of tags correct for one example\n",
        "def calc_correct(scores, tags):\n",
        "    correct = [np.argmax(score.npvalue()) == tag for score, tag in zip(scores, tags)]\n",
        "    return sum(correct)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OFH4ffLcA91a",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 19817
        },
        "outputId": "a4b06847-751c-425c-f1ff-340c8365258d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1532010934659,
          "user_tz": -540,
          "elapsed": 2677553,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Perform training\n",
        "for ITER in range(100):\n",
        "    random.shuffle(train)\n",
        "    start = time.time()\n",
        "    this_sents = this_words = this_loss = this_correct = 0\n",
        "    for sid in range(0, len(train)):\n",
        "        this_sents += 1\n",
        "        if this_sents % int(1000) == 0:\n",
        "            print(\"train loss/word=%.4f, acc=%.2f%%, word/sec=%.4f\" % (\n",
        "                this_loss / this_words, 100 * this_correct / this_words, this_words / (time.time() - start)),\n",
        "                  file=sys.stderr)\n",
        "        # train on the example\n",
        "        words, tags = train[sid]\n",
        "        # choose whether to use teacher forcing\n",
        "        if use_teacher_forcing:\n",
        "            scores = calc_scores_with_previous_tag(words, tags)\n",
        "        else:\n",
        "            scores = calc_scores(words)\n",
        "        loss_exp = calc_loss(scores, tags)\n",
        "        this_correct += calc_correct(scores, tags)\n",
        "        this_loss += loss_exp.scalar_value()\n",
        "        this_words += len(words)\n",
        "        loss_exp.backward()\n",
        "        trainer.update()\n",
        "    # Decay the schedule sampler if using schedule sampling.\n",
        "    sampler.decay()\n",
        "    # Perform evaluation\n",
        "    start = time.time()\n",
        "    this_sents = this_words = this_loss = this_correct = 0\n",
        "    for words, tags in dev:\n",
        "        this_sents += 1\n",
        "        # choose whether to use teacher forcing\n",
        "        if use_teacher_forcing:\n",
        "            scores = calc_scores_with_previous_tag(words)\n",
        "        else:\n",
        "            scores = calc_scores(words)\n",
        "        loss_exp = calc_loss(scores, tags)\n",
        "        this_correct += calc_correct(scores, tags)\n",
        "        this_loss += loss_exp.scalar_value()\n",
        "        this_words += len(words)\n",
        "    print(\"dev loss/word=%.4f, acc=%.2f%%, word/sec=%.4f\" % (\n",
        "        this_loss / this_words, 100 * this_correct / this_words, this_words / (time.time() - start)), file=sys.stderr)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.4632, acc=87.31%, word/sec=7594.7372\n",
            "train loss/word=0.3942, acc=88.68%, word/sec=7676.3388\n",
            "train loss/word=0.3526, acc=89.60%, word/sec=7695.7130\n",
            "train loss/word=0.3245, acc=90.35%, word/sec=7686.0091\n",
            "train loss/word=0.3060, acc=90.86%, word/sec=7687.8159\n",
            "train loss/word=0.2890, acc=91.30%, word/sec=7707.5892\n",
            "train loss/word=0.2767, acc=91.64%, word/sec=7710.1013\n",
            "train loss/word=0.2661, acc=91.95%, word/sec=7711.4834\n",
            "train loss/word=0.2563, acc=92.24%, word/sec=7725.5252\n",
            "train loss/word=0.2491, acc=92.45%, word/sec=7722.2980\n",
            "dev loss/word=0.5297, acc=87.03%, word/sec=13315.3852\n",
            "train loss/word=0.1470, acc=95.50%, word/sec=7766.8397\n",
            "train loss/word=0.1443, acc=95.62%, word/sec=7736.8131\n",
            "train loss/word=0.1429, acc=95.71%, word/sec=7706.7104\n",
            "train loss/word=0.1408, acc=95.77%, word/sec=7684.9657\n",
            "train loss/word=0.1396, acc=95.83%, word/sec=7670.9107\n",
            "train loss/word=0.1370, acc=95.91%, word/sec=7662.7986\n",
            "train loss/word=0.1361, acc=95.95%, word/sec=7669.5796\n",
            "train loss/word=0.1342, acc=96.02%, word/sec=7648.1502\n",
            "train loss/word=0.1327, acc=96.07%, word/sec=7627.8808\n",
            "train loss/word=0.1311, acc=96.12%, word/sec=7616.0728\n",
            "dev loss/word=0.5609, acc=88.32%, word/sec=12740.5582\n",
            "train loss/word=0.1001, acc=97.04%, word/sec=7390.5282\n",
            "train loss/word=0.0979, acc=97.07%, word/sec=7314.3890\n",
            "train loss/word=0.0973, acc=97.12%, word/sec=7323.3120\n",
            "train loss/word=0.0972, acc=97.16%, word/sec=7316.1516\n",
            "train loss/word=0.0963, acc=97.20%, word/sec=7296.2029\n",
            "train loss/word=0.0955, acc=97.24%, word/sec=7270.5205\n",
            "train loss/word=0.0963, acc=97.22%, word/sec=7290.3283\n",
            "train loss/word=0.0958, acc=97.25%, word/sec=7364.3641\n",
            "train loss/word=0.0943, acc=97.29%, word/sec=7419.6577\n",
            "train loss/word=0.0925, acc=97.34%, word/sec=7466.6243\n",
            "dev loss/word=0.5776, acc=88.89%, word/sec=13708.6916\n",
            "train loss/word=0.0693, acc=98.09%, word/sec=7975.5305\n",
            "train loss/word=0.0694, acc=98.05%, word/sec=7977.3126\n",
            "train loss/word=0.0674, acc=98.11%, word/sec=7964.5174\n",
            "train loss/word=0.0678, acc=98.08%, word/sec=7962.1053\n",
            "train loss/word=0.0690, acc=98.05%, word/sec=7941.3043\n",
            "train loss/word=0.0687, acc=98.07%, word/sec=7938.6901\n",
            "train loss/word=0.0690, acc=98.06%, word/sec=7892.6522\n",
            "train loss/word=0.0691, acc=98.08%, word/sec=7794.0263\n",
            "train loss/word=0.0687, acc=98.09%, word/sec=7738.7370\n",
            "train loss/word=0.0686, acc=98.09%, word/sec=7699.9688\n",
            "dev loss/word=0.6664, acc=89.40%, word/sec=12748.7556\n",
            "train loss/word=0.0499, acc=98.66%, word/sec=7543.9105\n",
            "train loss/word=0.0496, acc=98.67%, word/sec=7597.4496\n",
            "train loss/word=0.0493, acc=98.67%, word/sec=7628.2861\n",
            "train loss/word=0.0495, acc=98.66%, word/sec=7650.4490\n",
            "train loss/word=0.0506, acc=98.63%, word/sec=7683.5859\n",
            "train loss/word=0.0502, acc=98.64%, word/sec=7700.6349\n",
            "train loss/word=0.0511, acc=98.63%, word/sec=7720.4826\n",
            "train loss/word=0.0517, acc=98.61%, word/sec=7726.0488\n",
            "train loss/word=0.0522, acc=98.59%, word/sec=7730.0069\n",
            "train loss/word=0.0521, acc=98.60%, word/sec=7733.6700\n",
            "dev loss/word=0.8342, acc=88.70%, word/sec=13215.9687\n",
            "train loss/word=0.0382, acc=98.94%, word/sec=7761.2158\n",
            "train loss/word=0.0375, acc=98.96%, word/sec=7728.0571\n",
            "train loss/word=0.0365, acc=98.99%, word/sec=7742.6004\n",
            "train loss/word=0.0376, acc=98.98%, word/sec=7716.5533\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0378, acc=98.98%, word/sec=7705.1904\n",
            "train loss/word=0.0382, acc=98.98%, word/sec=7721.7639\n",
            "train loss/word=0.0386, acc=98.97%, word/sec=7732.3654\n",
            "train loss/word=0.0395, acc=98.96%, word/sec=7729.7172\n",
            "train loss/word=0.0402, acc=98.95%, word/sec=7732.5510\n",
            "train loss/word=0.0398, acc=98.96%, word/sec=7727.4324\n",
            "dev loss/word=0.8354, acc=89.48%, word/sec=13053.5478\n",
            "train loss/word=0.0265, acc=99.33%, word/sec=7722.0404\n",
            "train loss/word=0.0283, acc=99.28%, word/sec=7691.0748\n",
            "train loss/word=0.0287, acc=99.26%, word/sec=7676.2293\n",
            "train loss/word=0.0298, acc=99.23%, word/sec=7670.3747\n",
            "train loss/word=0.0306, acc=99.20%, word/sec=7670.5752\n",
            "train loss/word=0.0318, acc=99.17%, word/sec=7655.0285\n",
            "train loss/word=0.0312, acc=99.19%, word/sec=7641.7315\n",
            "train loss/word=0.0302, acc=99.21%, word/sec=7640.9490\n",
            "train loss/word=0.0300, acc=99.22%, word/sec=7624.6894\n",
            "train loss/word=0.0306, acc=99.21%, word/sec=7602.4867\n",
            "dev loss/word=1.0215, acc=89.26%, word/sec=12823.2680\n",
            "train loss/word=0.0201, acc=99.45%, word/sec=7365.2348\n",
            "train loss/word=0.0199, acc=99.49%, word/sec=7380.9559\n",
            "train loss/word=0.0203, acc=99.48%, word/sec=7328.9895\n",
            "train loss/word=0.0207, acc=99.47%, word/sec=7319.5534\n",
            "train loss/word=0.0225, acc=99.45%, word/sec=7307.1730\n",
            "train loss/word=0.0230, acc=99.44%, word/sec=7397.0692\n",
            "train loss/word=0.0236, acc=99.42%, word/sec=7459.3418\n",
            "train loss/word=0.0234, acc=99.42%, word/sec=7509.3592\n",
            "train loss/word=0.0239, acc=99.41%, word/sec=7553.1377\n",
            "train loss/word=0.0243, acc=99.40%, word/sec=7587.9814\n",
            "dev loss/word=1.2049, acc=88.76%, word/sec=13556.2326\n",
            "train loss/word=0.0177, acc=99.60%, word/sec=7955.8126\n",
            "train loss/word=0.0161, acc=99.59%, word/sec=7938.1928\n",
            "train loss/word=0.0172, acc=99.58%, word/sec=7922.8676\n",
            "train loss/word=0.0168, acc=99.57%, word/sec=7902.1749\n",
            "train loss/word=0.0189, acc=99.53%, word/sec=7879.7629\n",
            "train loss/word=0.0189, acc=99.52%, word/sec=7750.4478\n",
            "train loss/word=0.0189, acc=99.52%, word/sec=7700.7263\n",
            "train loss/word=0.0189, acc=99.52%, word/sec=7655.6181\n",
            "train loss/word=0.0188, acc=99.52%, word/sec=7635.2948\n",
            "train loss/word=0.0191, acc=99.52%, word/sec=7636.8761\n",
            "dev loss/word=1.2536, acc=88.88%, word/sec=13043.7652\n",
            "train loss/word=0.0122, acc=99.72%, word/sec=7705.5556\n",
            "train loss/word=0.0119, acc=99.70%, word/sec=7697.9244\n",
            "train loss/word=0.0126, acc=99.67%, word/sec=7694.1762\n",
            "train loss/word=0.0140, acc=99.66%, word/sec=7699.4671\n",
            "train loss/word=0.0150, acc=99.64%, word/sec=7694.0732\n",
            "train loss/word=0.0156, acc=99.62%, word/sec=7706.9818\n",
            "train loss/word=0.0154, acc=99.62%, word/sec=7712.2568\n",
            "train loss/word=0.0156, acc=99.62%, word/sec=7708.6007\n",
            "train loss/word=0.0153, acc=99.62%, word/sec=7729.2284\n",
            "train loss/word=0.0150, acc=99.63%, word/sec=7731.9831\n",
            "dev loss/word=1.3853, acc=89.26%, word/sec=13512.3472\n",
            "train loss/word=0.0125, acc=99.72%, word/sec=7939.8402\n",
            "train loss/word=0.0117, acc=99.73%, word/sec=7821.3474\n",
            "train loss/word=0.0113, acc=99.71%, word/sec=7774.0649\n",
            "train loss/word=0.0119, acc=99.71%, word/sec=7776.4242\n",
            "train loss/word=0.0122, acc=99.70%, word/sec=7761.6918\n",
            "train loss/word=0.0122, acc=99.70%, word/sec=7728.9348\n",
            "train loss/word=0.0126, acc=99.69%, word/sec=7719.1495\n",
            "train loss/word=0.0125, acc=99.70%, word/sec=7697.6563\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0124, acc=99.69%, word/sec=7689.3305\n",
            "train loss/word=0.0128, acc=99.69%, word/sec=7667.4725\n",
            "dev loss/word=1.5099, acc=89.04%, word/sec=12798.1914\n",
            "train loss/word=0.0073, acc=99.80%, word/sec=7465.6491\n",
            "train loss/word=0.0065, acc=99.83%, word/sec=7435.7393\n",
            "train loss/word=0.0078, acc=99.80%, word/sec=7417.6862\n",
            "train loss/word=0.0079, acc=99.79%, word/sec=7411.8514\n",
            "train loss/word=0.0087, acc=99.78%, word/sec=7399.9155\n",
            "train loss/word=0.0087, acc=99.77%, word/sec=7374.7635\n",
            "train loss/word=0.0089, acc=99.77%, word/sec=7349.9399\n",
            "train loss/word=0.0098, acc=99.76%, word/sec=7327.8066\n",
            "train loss/word=0.0104, acc=99.75%, word/sec=7306.3598\n",
            "train loss/word=0.0104, acc=99.75%, word/sec=7267.2731\n",
            "dev loss/word=1.6517, acc=88.67%, word/sec=11871.3550\n",
            "train loss/word=0.0053, acc=99.85%, word/sec=6917.0639\n",
            "train loss/word=0.0059, acc=99.83%, word/sec=6945.9439\n",
            "train loss/word=0.0073, acc=99.80%, word/sec=7110.3456\n",
            "train loss/word=0.0075, acc=99.80%, word/sec=7246.9967\n",
            "train loss/word=0.0079, acc=99.79%, word/sec=7332.5437\n",
            "train loss/word=0.0082, acc=99.79%, word/sec=7377.4027\n",
            "train loss/word=0.0096, acc=99.77%, word/sec=7407.8417\n",
            "train loss/word=0.0091, acc=99.78%, word/sec=7425.8769\n",
            "train loss/word=0.0092, acc=99.78%, word/sec=7435.8333\n",
            "train loss/word=0.0093, acc=99.78%, word/sec=7444.5631\n",
            "dev loss/word=1.5919, acc=89.27%, word/sec=13098.9600\n",
            "train loss/word=0.0047, acc=99.84%, word/sec=7601.0119\n",
            "train loss/word=0.0059, acc=99.83%, word/sec=7439.6137\n",
            "train loss/word=0.0059, acc=99.83%, word/sec=7302.6909\n",
            "train loss/word=0.0072, acc=99.81%, word/sec=7265.3077\n",
            "train loss/word=0.0070, acc=99.81%, word/sec=7236.6149\n",
            "train loss/word=0.0071, acc=99.82%, word/sec=7233.7319\n",
            "train loss/word=0.0077, acc=99.80%, word/sec=7231.8116\n",
            "train loss/word=0.0085, acc=99.79%, word/sec=7238.7131\n",
            "train loss/word=0.0083, acc=99.79%, word/sec=7257.3232\n",
            "train loss/word=0.0086, acc=99.79%, word/sec=7277.7497\n",
            "dev loss/word=1.7447, acc=89.03%, word/sec=12867.0924\n",
            "train loss/word=0.0038, acc=99.92%, word/sec=7605.8742\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=7497.2215\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=7525.1242\n",
            "train loss/word=0.0053, acc=99.88%, word/sec=7508.6553\n",
            "train loss/word=0.0059, acc=99.86%, word/sec=7479.4249\n",
            "train loss/word=0.0062, acc=99.85%, word/sec=7493.4217\n",
            "train loss/word=0.0070, acc=99.84%, word/sec=7499.1569\n",
            "train loss/word=0.0070, acc=99.84%, word/sec=7512.2626\n",
            "train loss/word=0.0071, acc=99.84%, word/sec=7525.3976\n",
            "train loss/word=0.0074, acc=99.83%, word/sec=7520.3530\n",
            "dev loss/word=1.9033, acc=88.74%, word/sec=12895.5678\n",
            "train loss/word=0.0036, acc=99.90%, word/sec=7550.4998\n",
            "train loss/word=0.0036, acc=99.90%, word/sec=7448.7637\n",
            "train loss/word=0.0049, acc=99.86%, word/sec=7457.1772\n",
            "train loss/word=0.0058, acc=99.84%, word/sec=7440.5240\n",
            "train loss/word=0.0062, acc=99.85%, word/sec=7432.6641\n",
            "train loss/word=0.0064, acc=99.85%, word/sec=7423.7574\n",
            "train loss/word=0.0063, acc=99.85%, word/sec=7421.9733\n",
            "train loss/word=0.0065, acc=99.84%, word/sec=7418.1005\n",
            "train loss/word=0.0065, acc=99.84%, word/sec=7414.1054\n",
            "train loss/word=0.0068, acc=99.83%, word/sec=7404.4929\n",
            "dev loss/word=1.9521, acc=88.86%, word/sec=12488.1253\n",
            "train loss/word=0.0063, acc=99.84%, word/sec=7227.5542\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0075, acc=99.83%, word/sec=7270.3820\n",
            "train loss/word=0.0067, acc=99.84%, word/sec=7263.0707\n",
            "train loss/word=0.0061, acc=99.85%, word/sec=7299.5546\n",
            "train loss/word=0.0061, acc=99.85%, word/sec=7284.7109\n",
            "train loss/word=0.0065, acc=99.84%, word/sec=7254.3548\n",
            "train loss/word=0.0065, acc=99.84%, word/sec=7227.7656\n",
            "train loss/word=0.0066, acc=99.84%, word/sec=7201.2996\n",
            "train loss/word=0.0065, acc=99.84%, word/sec=7164.6323\n",
            "train loss/word=0.0067, acc=99.84%, word/sec=7189.7518\n",
            "dev loss/word=1.9690, acc=88.72%, word/sec=13123.6037\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=7597.9591\n",
            "train loss/word=0.0050, acc=99.88%, word/sec=7570.2281\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=7567.0992\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7579.4155\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7598.4475\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7594.4433\n",
            "train loss/word=0.0049, acc=99.88%, word/sec=7592.9215\n",
            "train loss/word=0.0052, acc=99.87%, word/sec=7587.7793\n",
            "train loss/word=0.0057, acc=99.86%, word/sec=7561.6460\n",
            "train loss/word=0.0058, acc=99.86%, word/sec=7504.9997\n",
            "dev loss/word=2.0506, acc=89.18%, word/sec=12066.5537\n",
            "train loss/word=0.0054, acc=99.87%, word/sec=7144.9980\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=7222.3571\n",
            "train loss/word=0.0046, acc=99.89%, word/sec=7251.5544\n",
            "train loss/word=0.0042, acc=99.89%, word/sec=7291.6300\n",
            "train loss/word=0.0039, acc=99.90%, word/sec=7306.3077\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7328.8501\n",
            "train loss/word=0.0045, acc=99.88%, word/sec=7344.8620\n",
            "train loss/word=0.0047, acc=99.87%, word/sec=7355.8786\n",
            "train loss/word=0.0051, acc=99.87%, word/sec=7370.0950\n",
            "train loss/word=0.0056, acc=99.86%, word/sec=7385.4852\n",
            "dev loss/word=2.1994, acc=89.28%, word/sec=13086.9629\n",
            "train loss/word=0.0021, acc=99.95%, word/sec=7624.9515\n",
            "train loss/word=0.0033, acc=99.92%, word/sec=7603.4252\n",
            "train loss/word=0.0033, acc=99.91%, word/sec=7556.6995\n",
            "train loss/word=0.0037, acc=99.90%, word/sec=7578.7745\n",
            "train loss/word=0.0041, acc=99.88%, word/sec=7558.5051\n",
            "train loss/word=0.0046, acc=99.87%, word/sec=7580.0520\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=7573.5571\n",
            "train loss/word=0.0048, acc=99.87%, word/sec=7578.3636\n",
            "train loss/word=0.0050, acc=99.87%, word/sec=7580.5669\n",
            "train loss/word=0.0055, acc=99.86%, word/sec=7568.9037\n",
            "dev loss/word=2.2862, acc=89.19%, word/sec=12934.8359\n",
            "train loss/word=0.0020, acc=99.95%, word/sec=7436.1971\n",
            "train loss/word=0.0039, acc=99.92%, word/sec=7410.5066\n",
            "train loss/word=0.0039, acc=99.90%, word/sec=7398.0445\n",
            "train loss/word=0.0046, acc=99.88%, word/sec=7394.7185\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7375.7406\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7371.1723\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7364.6421\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=7359.0907\n",
            "train loss/word=0.0049, acc=99.88%, word/sec=7342.0808\n",
            "train loss/word=0.0052, acc=99.87%, word/sec=7333.4814\n",
            "dev loss/word=2.3457, acc=89.32%, word/sec=12307.1852\n",
            "train loss/word=0.0050, acc=99.90%, word/sec=7154.6619\n",
            "train loss/word=0.0038, acc=99.91%, word/sec=7126.1151\n",
            "train loss/word=0.0036, acc=99.91%, word/sec=7090.8989\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=7055.5938\n",
            "train loss/word=0.0043, acc=99.89%, word/sec=7052.2035\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0044, acc=99.88%, word/sec=7123.3318\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7192.3911\n",
            "train loss/word=0.0049, acc=99.87%, word/sec=7240.8749\n",
            "train loss/word=0.0053, acc=99.87%, word/sec=7281.6387\n",
            "train loss/word=0.0054, acc=99.87%, word/sec=7311.2099\n",
            "dev loss/word=2.3639, acc=89.27%, word/sec=13178.9161\n",
            "train loss/word=0.0043, acc=99.89%, word/sec=7600.0957\n",
            "train loss/word=0.0030, acc=99.91%, word/sec=7578.2556\n",
            "train loss/word=0.0038, acc=99.90%, word/sec=7564.7113\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=7560.5085\n",
            "train loss/word=0.0049, acc=99.88%, word/sec=7482.2437\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=7406.3525\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7396.2039\n",
            "train loss/word=0.0049, acc=99.88%, word/sec=7385.3635\n",
            "train loss/word=0.0052, acc=99.87%, word/sec=7383.9161\n",
            "train loss/word=0.0053, acc=99.87%, word/sec=7384.1588\n",
            "dev loss/word=2.4927, acc=89.40%, word/sec=12880.0939\n",
            "train loss/word=0.0069, acc=99.88%, word/sec=7609.6448\n",
            "train loss/word=0.0056, acc=99.88%, word/sec=7536.0672\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=7532.9723\n",
            "train loss/word=0.0055, acc=99.87%, word/sec=7503.5118\n",
            "train loss/word=0.0057, acc=99.87%, word/sec=7514.4019\n",
            "train loss/word=0.0058, acc=99.87%, word/sec=7519.1916\n",
            "train loss/word=0.0056, acc=99.87%, word/sec=7517.5355\n",
            "train loss/word=0.0055, acc=99.87%, word/sec=7516.4072\n",
            "train loss/word=0.0054, acc=99.87%, word/sec=7529.9618\n",
            "train loss/word=0.0054, acc=99.87%, word/sec=7536.5331\n",
            "dev loss/word=2.6952, acc=88.82%, word/sec=12915.3412\n",
            "train loss/word=0.0036, acc=99.93%, word/sec=7509.7567\n",
            "train loss/word=0.0039, acc=99.91%, word/sec=7557.1398\n",
            "train loss/word=0.0047, acc=99.90%, word/sec=7542.0967\n",
            "train loss/word=0.0045, acc=99.89%, word/sec=7534.8276\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=7526.5710\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=7530.8060\n",
            "train loss/word=0.0043, acc=99.89%, word/sec=7539.0783\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7530.4516\n",
            "train loss/word=0.0045, acc=99.89%, word/sec=7516.2198\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=7512.7887\n",
            "dev loss/word=2.5243, acc=88.85%, word/sec=12386.5596\n",
            "train loss/word=0.0037, acc=99.90%, word/sec=7290.1196\n",
            "train loss/word=0.0039, acc=99.89%, word/sec=7289.9155\n",
            "train loss/word=0.0042, acc=99.89%, word/sec=7289.8076\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7265.5880\n",
            "train loss/word=0.0041, acc=99.89%, word/sec=7254.2202\n",
            "train loss/word=0.0038, acc=99.90%, word/sec=7248.2226\n",
            "train loss/word=0.0041, acc=99.89%, word/sec=7234.3508\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7209.5525\n",
            "train loss/word=0.0044, acc=99.88%, word/sec=7175.0753\n",
            "train loss/word=0.0044, acc=99.88%, word/sec=7159.3057\n",
            "dev loss/word=2.5368, acc=89.13%, word/sec=11949.0377\n",
            "train loss/word=0.0025, acc=99.96%, word/sec=7088.0595\n",
            "train loss/word=0.0035, acc=99.92%, word/sec=7365.3839\n",
            "train loss/word=0.0037, acc=99.91%, word/sec=7453.1477\n",
            "train loss/word=0.0038, acc=99.90%, word/sec=7495.1208\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=7518.3840\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=7518.7612\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7516.0415\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7527.1865\n",
            "train loss/word=0.0046, acc=99.88%, word/sec=7536.7605\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0045, acc=99.88%, word/sec=7546.1716\n",
            "dev loss/word=2.5615, acc=89.13%, word/sec=13063.4829\n",
            "train loss/word=0.0027, acc=99.92%, word/sec=7072.8859\n",
            "train loss/word=0.0030, acc=99.90%, word/sec=7112.7696\n",
            "train loss/word=0.0031, acc=99.91%, word/sec=7178.2346\n",
            "train loss/word=0.0040, acc=99.90%, word/sec=7227.2347\n",
            "train loss/word=0.0042, acc=99.89%, word/sec=7275.6101\n",
            "train loss/word=0.0043, acc=99.89%, word/sec=7312.9829\n",
            "train loss/word=0.0042, acc=99.89%, word/sec=7335.4512\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7345.9218\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=7365.9875\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7370.9827\n",
            "dev loss/word=2.7159, acc=89.11%, word/sec=12892.5852\n",
            "train loss/word=0.0036, acc=99.91%, word/sec=7501.3458\n",
            "train loss/word=0.0033, acc=99.91%, word/sec=7536.1849\n",
            "train loss/word=0.0040, acc=99.89%, word/sec=7489.4903\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=7458.3739\n",
            "train loss/word=0.0049, acc=99.88%, word/sec=7445.6383\n",
            "train loss/word=0.0051, acc=99.88%, word/sec=7458.2223\n",
            "train loss/word=0.0051, acc=99.88%, word/sec=7454.2048\n",
            "train loss/word=0.0051, acc=99.88%, word/sec=7465.2585\n",
            "train loss/word=0.0049, acc=99.88%, word/sec=7469.9252\n",
            "train loss/word=0.0050, acc=99.88%, word/sec=7470.4933\n",
            "dev loss/word=2.9226, acc=88.88%, word/sec=12949.5468\n",
            "train loss/word=0.0040, acc=99.90%, word/sec=7442.9036\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=7445.6445\n",
            "train loss/word=0.0046, acc=99.89%, word/sec=7452.8689\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7410.6622\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=7408.5172\n",
            "train loss/word=0.0046, acc=99.88%, word/sec=7406.5645\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7426.2845\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7425.2015\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=7404.6817\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=7382.4437\n",
            "dev loss/word=3.0915, acc=88.84%, word/sec=12422.5884\n",
            "train loss/word=0.0026, acc=99.93%, word/sec=7145.8877\n",
            "train loss/word=0.0032, acc=99.91%, word/sec=7188.0996\n",
            "train loss/word=0.0028, acc=99.93%, word/sec=7171.7813\n",
            "train loss/word=0.0035, acc=99.92%, word/sec=7140.1528\n",
            "train loss/word=0.0041, acc=99.91%, word/sec=7121.6983\n",
            "train loss/word=0.0040, acc=99.91%, word/sec=7086.8208\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=7071.1432\n",
            "train loss/word=0.0044, acc=99.90%, word/sec=7094.6844\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7148.8945\n",
            "train loss/word=0.0045, acc=99.89%, word/sec=7197.7544\n",
            "dev loss/word=2.7793, acc=89.35%, word/sec=13231.3852\n",
            "train loss/word=0.0035, acc=99.91%, word/sec=7655.0654\n",
            "train loss/word=0.0038, acc=99.92%, word/sec=7626.1115\n",
            "train loss/word=0.0036, acc=99.91%, word/sec=7608.5282\n",
            "train loss/word=0.0041, acc=99.91%, word/sec=7616.7862\n",
            "train loss/word=0.0041, acc=99.91%, word/sec=7603.1954\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=7582.2621\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=7567.6040\n",
            "train loss/word=0.0044, acc=99.90%, word/sec=7501.7433\n",
            "train loss/word=0.0048, acc=99.90%, word/sec=7465.9095\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7441.7841\n",
            "dev loss/word=2.8019, acc=89.24%, word/sec=12687.0534\n",
            "train loss/word=0.0031, acc=99.90%, word/sec=7498.0009\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=7470.9444\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0041, acc=99.90%, word/sec=7492.8186\n",
            "train loss/word=0.0040, acc=99.90%, word/sec=7482.3269\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=7476.4188\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7492.5410\n",
            "train loss/word=0.0046, acc=99.89%, word/sec=7488.3797\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7497.9890\n",
            "train loss/word=0.0049, acc=99.88%, word/sec=7506.1480\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7512.8848\n",
            "dev loss/word=3.1139, acc=88.68%, word/sec=13192.4632\n",
            "train loss/word=0.0051, acc=99.88%, word/sec=7576.2522\n",
            "train loss/word=0.0040, acc=99.90%, word/sec=7495.1016\n",
            "train loss/word=0.0037, acc=99.91%, word/sec=7479.7192\n",
            "train loss/word=0.0039, acc=99.91%, word/sec=7447.5803\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=7456.2976\n",
            "train loss/word=0.0045, acc=99.89%, word/sec=7470.9440\n",
            "train loss/word=0.0045, acc=99.89%, word/sec=7458.5436\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=7465.2298\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=7468.8643\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=7464.2727\n",
            "dev loss/word=2.9786, acc=88.55%, word/sec=12879.2240\n",
            "train loss/word=0.0042, acc=99.91%, word/sec=7341.8373\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=7378.9530\n",
            "train loss/word=0.0037, acc=99.90%, word/sec=7411.0707\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=7400.3771\n",
            "train loss/word=0.0045, acc=99.89%, word/sec=7403.8850\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=7385.7653\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7372.2591\n",
            "train loss/word=0.0051, acc=99.88%, word/sec=7349.8194\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7322.1329\n",
            "train loss/word=0.0050, acc=99.88%, word/sec=7296.1374\n",
            "dev loss/word=3.1506, acc=88.95%, word/sec=12051.1994\n",
            "train loss/word=0.0040, acc=99.89%, word/sec=7085.5397\n",
            "train loss/word=0.0039, acc=99.90%, word/sec=7084.5429\n",
            "train loss/word=0.0043, acc=99.89%, word/sec=7057.7018\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=7106.1427\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=7194.3343\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=7256.8184\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=7320.9708\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=7365.1660\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=7405.2989\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=7434.3619\n",
            "dev loss/word=3.1420, acc=88.69%, word/sec=13257.4969\n",
            "train loss/word=0.0044, acc=99.89%, word/sec=7679.6561\n",
            "train loss/word=0.0036, acc=99.90%, word/sec=7621.8505\n",
            "train loss/word=0.0035, acc=99.89%, word/sec=7613.7785\n",
            "train loss/word=0.0035, acc=99.89%, word/sec=7438.3064\n",
            "train loss/word=0.0035, acc=99.90%, word/sec=7358.4820\n",
            "train loss/word=0.0037, acc=99.90%, word/sec=7322.9127\n",
            "train loss/word=0.0042, acc=99.89%, word/sec=7299.9121\n",
            "train loss/word=0.0046, acc=99.88%, word/sec=7295.7606\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=7296.7782\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=7307.1236\n",
            "dev loss/word=3.1159, acc=89.15%, word/sec=12753.9593\n",
            "train loss/word=0.0030, acc=99.93%, word/sec=7564.5168\n",
            "train loss/word=0.0032, acc=99.94%, word/sec=7460.0662\n",
            "train loss/word=0.0030, acc=99.93%, word/sec=7480.1204\n",
            "train loss/word=0.0038, acc=99.93%, word/sec=7489.6561\n",
            "train loss/word=0.0037, acc=99.92%, word/sec=7531.1855\n",
            "train loss/word=0.0040, acc=99.91%, word/sec=7563.3344\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0042, acc=99.91%, word/sec=7582.5337\n",
            "train loss/word=0.0044, acc=99.90%, word/sec=7599.9868\n",
            "train loss/word=0.0045, acc=99.90%, word/sec=7602.5288\n",
            "train loss/word=0.0045, acc=99.89%, word/sec=7616.1677\n",
            "dev loss/word=3.0961, acc=88.79%, word/sec=13075.9041\n",
            "train loss/word=0.0034, acc=99.94%, word/sec=7530.4593\n",
            "train loss/word=0.0040, acc=99.91%, word/sec=7509.6847\n",
            "train loss/word=0.0041, acc=99.91%, word/sec=7521.7453\n",
            "train loss/word=0.0046, acc=99.90%, word/sec=7495.7599\n",
            "train loss/word=0.0048, acc=99.90%, word/sec=7490.5659\n",
            "train loss/word=0.0047, acc=99.90%, word/sec=7495.6058\n",
            "train loss/word=0.0046, acc=99.90%, word/sec=7492.3477\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7508.0384\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7494.9854\n",
            "train loss/word=0.0051, acc=99.89%, word/sec=7489.1986\n",
            "dev loss/word=3.1609, acc=88.38%, word/sec=12677.7453\n",
            "train loss/word=0.0027, acc=99.94%, word/sec=7393.1001\n",
            "train loss/word=0.0030, acc=99.93%, word/sec=7350.7006\n",
            "train loss/word=0.0036, acc=99.92%, word/sec=7347.1352\n",
            "train loss/word=0.0040, acc=99.91%, word/sec=7321.6637\n",
            "train loss/word=0.0044, acc=99.90%, word/sec=7313.0130\n",
            "train loss/word=0.0050, acc=99.90%, word/sec=7297.5071\n",
            "train loss/word=0.0050, acc=99.90%, word/sec=7284.3071\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7247.5094\n",
            "train loss/word=0.0051, acc=99.89%, word/sec=7222.0149\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7205.3019\n",
            "dev loss/word=3.2477, acc=88.60%, word/sec=12423.4308\n",
            "train loss/word=0.0029, acc=99.92%, word/sec=7695.5604\n",
            "train loss/word=0.0037, acc=99.91%, word/sec=7695.2020\n",
            "train loss/word=0.0043, acc=99.91%, word/sec=7708.5851\n",
            "train loss/word=0.0043, acc=99.91%, word/sec=7699.2759\n",
            "train loss/word=0.0046, acc=99.90%, word/sec=7697.8858\n",
            "train loss/word=0.0047, acc=99.90%, word/sec=7707.6438\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7718.7873\n",
            "train loss/word=0.0053, acc=99.88%, word/sec=7724.2429\n",
            "train loss/word=0.0055, acc=99.88%, word/sec=7723.4641\n",
            "train loss/word=0.0056, acc=99.88%, word/sec=7729.7262\n",
            "dev loss/word=3.0788, acc=88.95%, word/sec=12143.4082\n",
            "train loss/word=0.0051, acc=99.90%, word/sec=7264.5388\n",
            "train loss/word=0.0041, acc=99.92%, word/sec=7283.8371\n",
            "train loss/word=0.0043, acc=99.91%, word/sec=7307.8015\n",
            "train loss/word=0.0044, acc=99.90%, word/sec=7344.6371\n",
            "train loss/word=0.0045, acc=99.90%, word/sec=7367.7095\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=7387.1041\n",
            "train loss/word=0.0050, acc=99.90%, word/sec=7402.4864\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=7417.0901\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=7430.1634\n",
            "train loss/word=0.0055, acc=99.88%, word/sec=7453.1288\n",
            "dev loss/word=3.1564, acc=88.87%, word/sec=13093.7939\n",
            "train loss/word=0.0030, acc=99.92%, word/sec=7671.0852\n",
            "train loss/word=0.0041, acc=99.89%, word/sec=7618.6162\n",
            "train loss/word=0.0046, acc=99.89%, word/sec=7608.3061\n",
            "train loss/word=0.0046, acc=99.89%, word/sec=7603.5454\n",
            "train loss/word=0.0045, acc=99.89%, word/sec=7551.1328\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=7549.8173\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=7562.8885\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=7577.0960\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=7594.6127\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=7601.3192\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dev loss/word=3.1853, acc=88.98%, word/sec=13179.5839\n",
            "train loss/word=0.0025, acc=99.93%, word/sec=7562.2943\n",
            "train loss/word=0.0029, acc=99.93%, word/sec=7532.7526\n",
            "train loss/word=0.0037, acc=99.91%, word/sec=7524.5099\n",
            "train loss/word=0.0038, acc=99.91%, word/sec=7505.0532\n",
            "train loss/word=0.0039, acc=99.90%, word/sec=7475.6959\n",
            "train loss/word=0.0040, acc=99.90%, word/sec=7479.8612\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=7467.0890\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=7469.6812\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=7452.4601\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=7439.2911\n",
            "dev loss/word=3.0740, acc=89.07%, word/sec=12453.5916\n",
            "train loss/word=0.0028, acc=99.92%, word/sec=7135.6223\n",
            "train loss/word=0.0036, acc=99.90%, word/sec=7163.8177\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=7181.0892\n",
            "train loss/word=0.0045, acc=99.90%, word/sec=7176.1996\n",
            "train loss/word=0.0047, acc=99.90%, word/sec=7164.9055\n",
            "train loss/word=0.0048, acc=99.90%, word/sec=7149.2523\n",
            "train loss/word=0.0049, acc=99.90%, word/sec=7146.7091\n",
            "train loss/word=0.0053, acc=99.89%, word/sec=7192.4066\n",
            "train loss/word=0.0054, acc=99.89%, word/sec=7232.2410\n",
            "train loss/word=0.0056, acc=99.89%, word/sec=7265.5664\n",
            "dev loss/word=3.1088, acc=89.19%, word/sec=13166.3006\n",
            "train loss/word=0.0025, acc=99.92%, word/sec=7585.1387\n",
            "train loss/word=0.0029, acc=99.91%, word/sec=7621.7538\n",
            "train loss/word=0.0034, acc=99.90%, word/sec=7608.8918\n",
            "train loss/word=0.0033, acc=99.91%, word/sec=7635.3206\n",
            "train loss/word=0.0039, acc=99.90%, word/sec=7652.4891\n",
            "train loss/word=0.0044, acc=99.90%, word/sec=7645.2081\n",
            "train loss/word=0.0052, acc=99.89%, word/sec=7839.8063\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=8290.5828\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=8712.3200\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=9074.6039\n",
            "dev loss/word=3.0774, acc=89.04%, word/sec=25423.1606\n",
            "train loss/word=0.0029, acc=99.92%, word/sec=14361.1458\n",
            "train loss/word=0.0037, acc=99.91%, word/sec=14448.7007\n",
            "train loss/word=0.0048, acc=99.90%, word/sec=14422.7055\n",
            "train loss/word=0.0052, acc=99.90%, word/sec=14464.9448\n",
            "train loss/word=0.0049, acc=99.90%, word/sec=14497.7136\n",
            "train loss/word=0.0053, acc=99.89%, word/sec=14531.0628\n",
            "train loss/word=0.0052, acc=99.89%, word/sec=14593.0730\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14600.1367\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14594.2568\n",
            "train loss/word=0.0058, acc=99.87%, word/sec=14609.7865\n",
            "dev loss/word=3.3481, acc=88.97%, word/sec=25748.2667\n",
            "train loss/word=0.0038, acc=99.90%, word/sec=14325.9573\n",
            "train loss/word=0.0036, acc=99.91%, word/sec=14435.6930\n",
            "train loss/word=0.0047, acc=99.90%, word/sec=14407.0701\n",
            "train loss/word=0.0053, acc=99.89%, word/sec=14420.6319\n",
            "train loss/word=0.0057, acc=99.89%, word/sec=14462.5603\n",
            "train loss/word=0.0057, acc=99.88%, word/sec=14496.5604\n",
            "train loss/word=0.0059, acc=99.87%, word/sec=14522.7013\n",
            "train loss/word=0.0063, acc=99.87%, word/sec=14533.9801\n",
            "train loss/word=0.0063, acc=99.87%, word/sec=14568.4004\n",
            "train loss/word=0.0061, acc=99.87%, word/sec=14593.6863\n",
            "dev loss/word=3.2855, acc=88.83%, word/sec=25626.5126\n",
            "train loss/word=0.0034, acc=99.90%, word/sec=14576.3366\n",
            "train loss/word=0.0036, acc=99.91%, word/sec=14625.7952\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=14673.7854\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0044, acc=99.89%, word/sec=14665.1088\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=14680.9559\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=14690.0896\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=14707.9047\n",
            "train loss/word=0.0053, acc=99.88%, word/sec=14727.4850\n",
            "train loss/word=0.0055, acc=99.88%, word/sec=14746.5360\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14723.5432\n",
            "dev loss/word=3.4605, acc=88.44%, word/sec=24912.4928\n",
            "train loss/word=0.0068, acc=99.84%, word/sec=14730.1221\n",
            "train loss/word=0.0057, acc=99.87%, word/sec=14619.2006\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=14684.1350\n",
            "train loss/word=0.0053, acc=99.88%, word/sec=14731.1862\n",
            "train loss/word=0.0053, acc=99.87%, word/sec=14727.0209\n",
            "train loss/word=0.0051, acc=99.88%, word/sec=14739.1136\n",
            "train loss/word=0.0057, acc=99.87%, word/sec=14733.1856\n",
            "train loss/word=0.0056, acc=99.87%, word/sec=14722.3962\n",
            "train loss/word=0.0061, acc=99.86%, word/sec=14725.4973\n",
            "train loss/word=0.0061, acc=99.86%, word/sec=14724.9747\n",
            "dev loss/word=3.5756, acc=88.79%, word/sec=25447.5074\n",
            "train loss/word=0.0019, acc=99.93%, word/sec=14843.5714\n",
            "train loss/word=0.0028, acc=99.93%, word/sec=14798.8131\n",
            "train loss/word=0.0033, acc=99.91%, word/sec=14613.0034\n",
            "train loss/word=0.0035, acc=99.91%, word/sec=14395.9873\n",
            "train loss/word=0.0036, acc=99.91%, word/sec=14409.0688\n",
            "train loss/word=0.0037, acc=99.91%, word/sec=14395.6284\n",
            "train loss/word=0.0039, acc=99.91%, word/sec=14426.2662\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=14433.9062\n",
            "train loss/word=0.0044, acc=99.90%, word/sec=14411.5992\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=14400.3751\n",
            "dev loss/word=3.2936, acc=88.55%, word/sec=25042.5776\n",
            "train loss/word=0.0042, acc=99.88%, word/sec=14232.5801\n",
            "train loss/word=0.0048, acc=99.88%, word/sec=14244.8330\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=14374.8425\n",
            "train loss/word=0.0045, acc=99.90%, word/sec=14413.6462\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=14473.9627\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=14486.0509\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=14463.9801\n",
            "train loss/word=0.0052, acc=99.89%, word/sec=14455.0167\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14451.6987\n",
            "train loss/word=0.0055, acc=99.88%, word/sec=14449.7749\n",
            "dev loss/word=3.3781, acc=89.02%, word/sec=25692.5963\n",
            "train loss/word=0.0040, acc=99.92%, word/sec=14463.9559\n",
            "train loss/word=0.0038, acc=99.92%, word/sec=14506.9542\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=14442.2636\n",
            "train loss/word=0.0046, acc=99.90%, word/sec=14457.4925\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=14484.5814\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=14491.3971\n",
            "train loss/word=0.0056, acc=99.88%, word/sec=14511.2301\n",
            "train loss/word=0.0057, acc=99.88%, word/sec=14530.4463\n",
            "train loss/word=0.0060, acc=99.87%, word/sec=14553.9600\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14544.7379\n",
            "dev loss/word=3.2780, acc=88.72%, word/sec=25639.3276\n",
            "train loss/word=0.0041, acc=99.92%, word/sec=14607.3467\n",
            "train loss/word=0.0038, acc=99.91%, word/sec=14636.2310\n",
            "train loss/word=0.0041, acc=99.90%, word/sec=14529.0086\n",
            "train loss/word=0.0047, acc=99.90%, word/sec=14523.4049\n",
            "train loss/word=0.0051, acc=99.89%, word/sec=14560.2049\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14571.4060\n",
            "train loss/word=0.0057, acc=99.87%, word/sec=14594.7739\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0060, acc=99.87%, word/sec=14590.5699\n",
            "train loss/word=0.0062, acc=99.86%, word/sec=14555.7251\n",
            "train loss/word=0.0063, acc=99.86%, word/sec=14549.6489\n",
            "dev loss/word=3.4927, acc=88.40%, word/sec=25259.2535\n",
            "train loss/word=0.0072, acc=99.84%, word/sec=14452.4640\n",
            "train loss/word=0.0061, acc=99.87%, word/sec=14428.4546\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14449.6797\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14472.2351\n",
            "train loss/word=0.0057, acc=99.88%, word/sec=14490.3318\n",
            "train loss/word=0.0060, acc=99.88%, word/sec=14502.5378\n",
            "train loss/word=0.0060, acc=99.88%, word/sec=14498.8020\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14481.7849\n",
            "train loss/word=0.0063, acc=99.87%, word/sec=14475.9121\n",
            "train loss/word=0.0066, acc=99.87%, word/sec=14474.3579\n",
            "dev loss/word=3.4491, acc=88.94%, word/sec=25206.1919\n",
            "train loss/word=0.0065, acc=99.86%, word/sec=14568.2810\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=14568.3841\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=14577.9252\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=14596.7882\n",
            "train loss/word=0.0053, acc=99.88%, word/sec=14575.4091\n",
            "train loss/word=0.0055, acc=99.88%, word/sec=14494.5784\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14471.8384\n",
            "train loss/word=0.0057, acc=99.87%, word/sec=14496.3160\n",
            "train loss/word=0.0055, acc=99.88%, word/sec=14496.7526\n",
            "train loss/word=0.0056, acc=99.88%, word/sec=14501.6282\n",
            "dev loss/word=3.3352, acc=88.93%, word/sec=25630.9528\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=14577.8105\n",
            "train loss/word=0.0040, acc=99.90%, word/sec=14630.5174\n",
            "train loss/word=0.0044, acc=99.88%, word/sec=14610.8383\n",
            "train loss/word=0.0047, acc=99.88%, word/sec=14606.5096\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=14537.7197\n",
            "train loss/word=0.0053, acc=99.88%, word/sec=14530.6337\n",
            "train loss/word=0.0057, acc=99.87%, word/sec=14521.2936\n",
            "train loss/word=0.0056, acc=99.88%, word/sec=14503.2181\n",
            "train loss/word=0.0058, acc=99.87%, word/sec=14518.5582\n",
            "train loss/word=0.0059, acc=99.88%, word/sec=14509.9532\n",
            "dev loss/word=3.2130, acc=88.89%, word/sec=25539.6761\n",
            "train loss/word=0.0055, acc=99.85%, word/sec=14280.8337\n",
            "train loss/word=0.0054, acc=99.87%, word/sec=14224.8558\n",
            "train loss/word=0.0060, acc=99.87%, word/sec=14338.9225\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14419.7761\n",
            "train loss/word=0.0061, acc=99.88%, word/sec=14378.8989\n",
            "train loss/word=0.0059, acc=99.88%, word/sec=14351.0028\n",
            "train loss/word=0.0059, acc=99.88%, word/sec=14381.5870\n",
            "train loss/word=0.0061, acc=99.88%, word/sec=14403.5955\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14389.1820\n",
            "train loss/word=0.0063, acc=99.88%, word/sec=14387.1127\n",
            "dev loss/word=3.5237, acc=88.73%, word/sec=25279.5603\n",
            "train loss/word=0.0043, acc=99.90%, word/sec=14391.9821\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=14205.2565\n",
            "train loss/word=0.0047, acc=99.89%, word/sec=14236.1820\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=14254.8312\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=14269.0238\n",
            "train loss/word=0.0049, acc=99.88%, word/sec=14254.7008\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=14276.3734\n",
            "train loss/word=0.0056, acc=99.87%, word/sec=14259.0405\n",
            "train loss/word=0.0061, acc=99.87%, word/sec=14232.5795\n",
            "train loss/word=0.0061, acc=99.87%, word/sec=14246.9888\n",
            "dev loss/word=3.3450, acc=89.05%, word/sec=25136.7621\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0067, acc=99.86%, word/sec=14232.0518\n",
            "train loss/word=0.0064, acc=99.86%, word/sec=14238.9554\n",
            "train loss/word=0.0061, acc=99.86%, word/sec=14252.4532\n",
            "train loss/word=0.0061, acc=99.86%, word/sec=14246.4934\n",
            "train loss/word=0.0066, acc=99.85%, word/sec=14244.7933\n",
            "train loss/word=0.0070, acc=99.85%, word/sec=14245.2206\n",
            "train loss/word=0.0070, acc=99.84%, word/sec=14204.0278\n",
            "train loss/word=0.0073, acc=99.84%, word/sec=14172.1441\n",
            "train loss/word=0.0075, acc=99.84%, word/sec=14177.4607\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14109.5385\n",
            "dev loss/word=3.6755, acc=88.20%, word/sec=24684.0574\n",
            "train loss/word=0.0064, acc=99.85%, word/sec=13823.4283\n",
            "train loss/word=0.0050, acc=99.88%, word/sec=13858.3917\n",
            "train loss/word=0.0065, acc=99.87%, word/sec=13947.5606\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14027.7863\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14043.8759\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14110.7625\n",
            "train loss/word=0.0072, acc=99.86%, word/sec=14150.1296\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14134.8991\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14157.0928\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14192.7896\n",
            "dev loss/word=3.4897, acc=88.31%, word/sec=25540.3818\n",
            "train loss/word=0.0057, acc=99.88%, word/sec=14458.1444\n",
            "train loss/word=0.0059, acc=99.87%, word/sec=14420.8663\n",
            "train loss/word=0.0059, acc=99.87%, word/sec=14427.5208\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14408.6772\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14412.9189\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14426.5768\n",
            "train loss/word=0.0065, acc=99.86%, word/sec=14442.5839\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14449.1495\n",
            "train loss/word=0.0071, acc=99.85%, word/sec=14454.5248\n",
            "train loss/word=0.0072, acc=99.85%, word/sec=14424.6346\n",
            "dev loss/word=3.5381, acc=88.37%, word/sec=24945.9046\n",
            "train loss/word=0.0055, acc=99.88%, word/sec=14461.1360\n",
            "train loss/word=0.0061, acc=99.87%, word/sec=14537.5210\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14531.6405\n",
            "train loss/word=0.0063, acc=99.86%, word/sec=14529.1326\n",
            "train loss/word=0.0071, acc=99.85%, word/sec=14497.1840\n",
            "train loss/word=0.0074, acc=99.85%, word/sec=14535.4820\n",
            "train loss/word=0.0073, acc=99.85%, word/sec=14526.1719\n",
            "train loss/word=0.0074, acc=99.85%, word/sec=14512.1894\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14496.5289\n",
            "train loss/word=0.0075, acc=99.85%, word/sec=14516.4556\n",
            "dev loss/word=3.6261, acc=89.14%, word/sec=25270.3571\n",
            "train loss/word=0.0058, acc=99.89%, word/sec=14311.6575\n",
            "train loss/word=0.0059, acc=99.87%, word/sec=14396.4484\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14437.9525\n",
            "train loss/word=0.0061, acc=99.87%, word/sec=14401.7275\n",
            "train loss/word=0.0060, acc=99.87%, word/sec=14360.5292\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14371.8391\n",
            "train loss/word=0.0060, acc=99.87%, word/sec=14365.2210\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14377.6674\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14395.8992\n",
            "train loss/word=0.0070, acc=99.86%, word/sec=14410.2111\n",
            "dev loss/word=3.7729, acc=88.77%, word/sec=25487.9522\n",
            "train loss/word=0.0057, acc=99.86%, word/sec=14481.4788\n",
            "train loss/word=0.0044, acc=99.90%, word/sec=14489.8344\n",
            "train loss/word=0.0046, acc=99.89%, word/sec=14510.1137\n",
            "train loss/word=0.0051, acc=99.89%, word/sec=14514.3053\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0060, acc=99.88%, word/sec=14529.9658\n",
            "train loss/word=0.0062, acc=99.88%, word/sec=14535.8062\n",
            "train loss/word=0.0064, acc=99.87%, word/sec=14541.2361\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14538.4758\n",
            "train loss/word=0.0065, acc=99.87%, word/sec=14509.5073\n",
            "train loss/word=0.0068, acc=99.87%, word/sec=14504.2380\n",
            "dev loss/word=3.5274, acc=88.45%, word/sec=25375.5904\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14374.0473\n",
            "train loss/word=0.0048, acc=99.89%, word/sec=14316.0221\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14318.9891\n",
            "train loss/word=0.0065, acc=99.87%, word/sec=14359.2871\n",
            "train loss/word=0.0064, acc=99.87%, word/sec=14343.3381\n",
            "train loss/word=0.0063, acc=99.87%, word/sec=14362.9708\n",
            "train loss/word=0.0063, acc=99.87%, word/sec=14360.7072\n",
            "train loss/word=0.0069, acc=99.86%, word/sec=14365.8874\n",
            "train loss/word=0.0075, acc=99.85%, word/sec=14361.7111\n",
            "train loss/word=0.0073, acc=99.85%, word/sec=14346.0715\n",
            "dev loss/word=3.6347, acc=88.58%, word/sec=25164.2318\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14382.5103\n",
            "train loss/word=0.0056, acc=99.88%, word/sec=14420.0123\n",
            "train loss/word=0.0064, acc=99.86%, word/sec=14297.2469\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14261.2661\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14248.6467\n",
            "train loss/word=0.0079, acc=99.83%, word/sec=14238.6626\n",
            "train loss/word=0.0081, acc=99.83%, word/sec=14242.8677\n",
            "train loss/word=0.0087, acc=99.82%, word/sec=14258.6187\n",
            "train loss/word=0.0090, acc=99.81%, word/sec=14253.5373\n",
            "train loss/word=0.0094, acc=99.81%, word/sec=14257.5302\n",
            "dev loss/word=3.5943, acc=89.00%, word/sec=25273.4645\n",
            "train loss/word=0.0095, acc=99.84%, word/sec=14148.8479\n",
            "train loss/word=0.0070, acc=99.86%, word/sec=14190.6864\n",
            "train loss/word=0.0064, acc=99.87%, word/sec=14285.0268\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14346.8846\n",
            "train loss/word=0.0072, acc=99.85%, word/sec=14379.3069\n",
            "train loss/word=0.0071, acc=99.84%, word/sec=14410.7432\n",
            "train loss/word=0.0071, acc=99.84%, word/sec=14393.5673\n",
            "train loss/word=0.0073, acc=99.85%, word/sec=14397.0792\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14416.4976\n",
            "train loss/word=0.0081, acc=99.83%, word/sec=14415.0905\n",
            "dev loss/word=3.5533, acc=88.49%, word/sec=25606.3140\n",
            "train loss/word=0.0058, acc=99.86%, word/sec=14472.3631\n",
            "train loss/word=0.0066, acc=99.85%, word/sec=14658.9603\n",
            "train loss/word=0.0059, acc=99.86%, word/sec=14591.0478\n",
            "train loss/word=0.0063, acc=99.86%, word/sec=14613.0084\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14527.6233\n",
            "train loss/word=0.0068, acc=99.85%, word/sec=14442.5044\n",
            "train loss/word=0.0074, acc=99.84%, word/sec=14447.8467\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14407.2472\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14428.5425\n",
            "train loss/word=0.0082, acc=99.84%, word/sec=14423.3403\n",
            "dev loss/word=3.1815, acc=88.31%, word/sec=25172.0914\n",
            "train loss/word=0.0060, acc=99.86%, word/sec=14587.7763\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14458.5985\n",
            "train loss/word=0.0064, acc=99.87%, word/sec=14478.8003\n",
            "train loss/word=0.0070, acc=99.86%, word/sec=14496.7524\n",
            "train loss/word=0.0072, acc=99.86%, word/sec=14485.6333\n",
            "train loss/word=0.0075, acc=99.86%, word/sec=14436.5571\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14427.6958\n",
            "train loss/word=0.0078, acc=99.85%, word/sec=14419.2546\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0080, acc=99.85%, word/sec=14419.8834\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14411.5493\n",
            "dev loss/word=3.5681, acc=88.81%, word/sec=25400.9621\n",
            "train loss/word=0.0086, acc=99.84%, word/sec=14331.9410\n",
            "train loss/word=0.0078, acc=99.84%, word/sec=14466.3715\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14490.7881\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14490.6858\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14508.0567\n",
            "train loss/word=0.0084, acc=99.83%, word/sec=14537.9244\n",
            "train loss/word=0.0082, acc=99.84%, word/sec=14560.0745\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14559.2507\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14553.6222\n",
            "train loss/word=0.0078, acc=99.84%, word/sec=14549.2620\n",
            "dev loss/word=3.4100, acc=89.08%, word/sec=25600.3520\n",
            "train loss/word=0.0055, acc=99.87%, word/sec=14610.5684\n",
            "train loss/word=0.0074, acc=99.85%, word/sec=14520.0189\n",
            "train loss/word=0.0066, acc=99.86%, word/sec=14516.4167\n",
            "train loss/word=0.0065, acc=99.86%, word/sec=14508.0305\n",
            "train loss/word=0.0064, acc=99.86%, word/sec=14529.5086\n",
            "train loss/word=0.0066, acc=99.86%, word/sec=14543.4854\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14524.5027\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14537.0740\n",
            "train loss/word=0.0069, acc=99.86%, word/sec=14501.8993\n",
            "train loss/word=0.0070, acc=99.85%, word/sec=14523.4709\n",
            "dev loss/word=3.4190, acc=88.76%, word/sec=25403.3167\n",
            "train loss/word=0.0042, acc=99.90%, word/sec=13809.3020\n",
            "train loss/word=0.0039, acc=99.90%, word/sec=14144.9160\n",
            "train loss/word=0.0050, acc=99.89%, word/sec=14270.6528\n",
            "train loss/word=0.0050, acc=99.88%, word/sec=14360.3220\n",
            "train loss/word=0.0054, acc=99.88%, word/sec=14398.0600\n",
            "train loss/word=0.0060, acc=99.87%, word/sec=14437.3668\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14454.7920\n",
            "train loss/word=0.0066, acc=99.86%, word/sec=14454.9722\n",
            "train loss/word=0.0070, acc=99.86%, word/sec=14457.0460\n",
            "train loss/word=0.0075, acc=99.85%, word/sec=14429.9774\n",
            "dev loss/word=3.5384, acc=88.89%, word/sec=25442.3945\n",
            "train loss/word=0.0050, acc=99.87%, word/sec=14333.3216\n",
            "train loss/word=0.0050, acc=99.87%, word/sec=14288.4641\n",
            "train loss/word=0.0057, acc=99.86%, word/sec=14298.1339\n",
            "train loss/word=0.0056, acc=99.86%, word/sec=14307.5513\n",
            "train loss/word=0.0060, acc=99.86%, word/sec=14345.1698\n",
            "train loss/word=0.0065, acc=99.85%, word/sec=14415.0506\n",
            "train loss/word=0.0064, acc=99.85%, word/sec=14427.0105\n",
            "train loss/word=0.0068, acc=99.85%, word/sec=14451.3480\n",
            "train loss/word=0.0069, acc=99.85%, word/sec=14471.0053\n",
            "train loss/word=0.0070, acc=99.86%, word/sec=14488.2293\n",
            "dev loss/word=3.5081, acc=88.30%, word/sec=25372.8239\n",
            "train loss/word=0.0039, acc=99.90%, word/sec=14461.5621\n",
            "train loss/word=0.0053, acc=99.89%, word/sec=14613.2682\n",
            "train loss/word=0.0064, acc=99.87%, word/sec=14667.3435\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14642.9570\n",
            "train loss/word=0.0081, acc=99.85%, word/sec=14635.4902\n",
            "train loss/word=0.0080, acc=99.85%, word/sec=14596.6179\n",
            "train loss/word=0.0084, acc=99.84%, word/sec=14610.5765\n",
            "train loss/word=0.0087, acc=99.84%, word/sec=14612.0204\n",
            "train loss/word=0.0089, acc=99.84%, word/sec=14601.6030\n",
            "train loss/word=0.0092, acc=99.84%, word/sec=14579.4477\n",
            "dev loss/word=3.7908, acc=88.20%, word/sec=25500.0686\n",
            "train loss/word=0.0068, acc=99.88%, word/sec=14198.6547\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0078, acc=99.84%, word/sec=14366.3647\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14433.0275\n",
            "train loss/word=0.0080, acc=99.84%, word/sec=14387.3358\n",
            "train loss/word=0.0082, acc=99.84%, word/sec=14347.8089\n",
            "train loss/word=0.0082, acc=99.84%, word/sec=14336.5893\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14350.7419\n",
            "train loss/word=0.0086, acc=99.84%, word/sec=14355.8948\n",
            "train loss/word=0.0090, acc=99.83%, word/sec=14358.5909\n",
            "train loss/word=0.0089, acc=99.83%, word/sec=14344.3789\n",
            "dev loss/word=3.7647, acc=88.95%, word/sec=25128.8320\n",
            "train loss/word=0.0056, acc=99.87%, word/sec=14377.7934\n",
            "train loss/word=0.0078, acc=99.84%, word/sec=14422.7566\n",
            "train loss/word=0.0070, acc=99.85%, word/sec=14425.5126\n",
            "train loss/word=0.0083, acc=99.83%, word/sec=14434.3164\n",
            "train loss/word=0.0084, acc=99.83%, word/sec=14407.4550\n",
            "train loss/word=0.0085, acc=99.84%, word/sec=14421.9891\n",
            "train loss/word=0.0089, acc=99.83%, word/sec=14448.5797\n",
            "train loss/word=0.0092, acc=99.83%, word/sec=14467.9202\n",
            "train loss/word=0.0094, acc=99.82%, word/sec=14467.9376\n",
            "train loss/word=0.0094, acc=99.83%, word/sec=14456.3087\n",
            "dev loss/word=3.6965, acc=88.86%, word/sec=25381.8105\n",
            "train loss/word=0.0069, acc=99.85%, word/sec=14461.2271\n",
            "train loss/word=0.0080, acc=99.82%, word/sec=14484.7957\n",
            "train loss/word=0.0088, acc=99.82%, word/sec=14498.3564\n",
            "train loss/word=0.0088, acc=99.82%, word/sec=14439.9538\n",
            "train loss/word=0.0096, acc=99.82%, word/sec=14459.6796\n",
            "train loss/word=0.0092, acc=99.83%, word/sec=14495.8798\n",
            "train loss/word=0.0093, acc=99.83%, word/sec=14534.4103\n",
            "train loss/word=0.0093, acc=99.82%, word/sec=14524.7258\n",
            "train loss/word=0.0092, acc=99.83%, word/sec=14534.4572\n",
            "train loss/word=0.0094, acc=99.83%, word/sec=14552.7986\n",
            "dev loss/word=3.5710, acc=88.28%, word/sec=25680.1270\n",
            "train loss/word=0.0076, acc=99.86%, word/sec=14311.7714\n",
            "train loss/word=0.0059, acc=99.86%, word/sec=14216.9687\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14174.1608\n",
            "train loss/word=0.0065, acc=99.86%, word/sec=14210.6998\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14268.3180\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14276.6443\n",
            "train loss/word=0.0078, acc=99.84%, word/sec=14331.9191\n",
            "train loss/word=0.0077, acc=99.84%, word/sec=14347.6401\n",
            "train loss/word=0.0080, acc=99.84%, word/sec=14364.6799\n",
            "train loss/word=0.0082, acc=99.83%, word/sec=14396.0206\n",
            "dev loss/word=3.5468, acc=88.61%, word/sec=25588.3843\n",
            "train loss/word=0.0074, acc=99.85%, word/sec=14614.6294\n",
            "train loss/word=0.0066, acc=99.86%, word/sec=14386.8326\n",
            "train loss/word=0.0065, acc=99.86%, word/sec=14382.9986\n",
            "train loss/word=0.0070, acc=99.85%, word/sec=14332.2766\n",
            "train loss/word=0.0065, acc=99.86%, word/sec=14347.6762\n",
            "train loss/word=0.0068, acc=99.85%, word/sec=14329.8258\n",
            "train loss/word=0.0081, acc=99.83%, word/sec=14298.7581\n",
            "train loss/word=0.0082, acc=99.83%, word/sec=14285.8252\n",
            "train loss/word=0.0086, acc=99.82%, word/sec=14291.2114\n",
            "train loss/word=0.0085, acc=99.82%, word/sec=14298.7439\n",
            "dev loss/word=3.6268, acc=88.97%, word/sec=25738.4155\n",
            "train loss/word=0.0062, acc=99.86%, word/sec=14489.3026\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14503.0022\n",
            "train loss/word=0.0063, acc=99.86%, word/sec=14555.3761\n",
            "train loss/word=0.0073, acc=99.84%, word/sec=14581.9414\n",
            "train loss/word=0.0075, acc=99.84%, word/sec=14492.4368\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0078, acc=99.84%, word/sec=14500.8083\n",
            "train loss/word=0.0077, acc=99.84%, word/sec=14498.3103\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14514.3354\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14509.5369\n",
            "train loss/word=0.0083, acc=99.83%, word/sec=14491.8680\n",
            "dev loss/word=3.8522, acc=88.90%, word/sec=25289.8999\n",
            "train loss/word=0.0070, acc=99.84%, word/sec=14577.0836\n",
            "train loss/word=0.0055, acc=99.87%, word/sec=14493.8660\n",
            "train loss/word=0.0061, acc=99.87%, word/sec=14472.3612\n",
            "train loss/word=0.0069, acc=99.86%, word/sec=14466.6630\n",
            "train loss/word=0.0072, acc=99.86%, word/sec=14422.5406\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14348.3024\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14374.3185\n",
            "train loss/word=0.0088, acc=99.84%, word/sec=14379.2946\n",
            "train loss/word=0.0089, acc=99.84%, word/sec=14356.9162\n",
            "train loss/word=0.0088, acc=99.83%, word/sec=14366.0718\n",
            "dev loss/word=3.7721, acc=88.51%, word/sec=25471.1751\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=14448.0332\n",
            "train loss/word=0.0078, acc=99.83%, word/sec=14469.6594\n",
            "train loss/word=0.0078, acc=99.83%, word/sec=14508.7723\n",
            "train loss/word=0.0078, acc=99.83%, word/sec=14347.4403\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14349.0226\n",
            "train loss/word=0.0079, acc=99.83%, word/sec=14356.1131\n",
            "train loss/word=0.0080, acc=99.84%, word/sec=14378.3266\n",
            "train loss/word=0.0080, acc=99.84%, word/sec=14379.6672\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14389.1160\n",
            "train loss/word=0.0085, acc=99.83%, word/sec=14383.6903\n",
            "dev loss/word=3.5522, acc=88.75%, word/sec=25235.9077\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14301.6093\n",
            "train loss/word=0.0057, acc=99.88%, word/sec=14351.3211\n",
            "train loss/word=0.0071, acc=99.86%, word/sec=14400.8417\n",
            "train loss/word=0.0072, acc=99.86%, word/sec=14419.5545\n",
            "train loss/word=0.0072, acc=99.86%, word/sec=14446.5341\n",
            "train loss/word=0.0074, acc=99.86%, word/sec=14448.7294\n",
            "train loss/word=0.0072, acc=99.86%, word/sec=14448.9006\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14432.5621\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14442.6518\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14455.4583\n",
            "dev loss/word=3.5875, acc=88.63%, word/sec=25502.7278\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14523.6040\n",
            "train loss/word=0.0059, acc=99.88%, word/sec=14467.2731\n",
            "train loss/word=0.0071, acc=99.87%, word/sec=14462.0695\n",
            "train loss/word=0.0081, acc=99.85%, word/sec=14496.9739\n",
            "train loss/word=0.0084, acc=99.84%, word/sec=14505.2348\n",
            "train loss/word=0.0087, acc=99.83%, word/sec=14512.7125\n",
            "train loss/word=0.0090, acc=99.83%, word/sec=14515.4902\n",
            "train loss/word=0.0088, acc=99.84%, word/sec=14476.7310\n",
            "train loss/word=0.0089, acc=99.83%, word/sec=14414.1218\n",
            "train loss/word=0.0089, acc=99.83%, word/sec=14404.9263\n",
            "dev loss/word=3.6593, acc=88.89%, word/sec=24749.6764\n",
            "train loss/word=0.0076, acc=99.86%, word/sec=14553.4354\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14439.8537\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14480.9462\n",
            "train loss/word=0.0065, acc=99.86%, word/sec=14435.0278\n",
            "train loss/word=0.0069, acc=99.86%, word/sec=14428.8397\n",
            "train loss/word=0.0074, acc=99.85%, word/sec=14340.3669\n",
            "train loss/word=0.0081, acc=99.84%, word/sec=14318.7120\n",
            "train loss/word=0.0086, acc=99.83%, word/sec=14308.5246\n",
            "train loss/word=0.0088, acc=99.83%, word/sec=14302.4719\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0090, acc=99.83%, word/sec=14289.8483\n",
            "dev loss/word=3.5685, acc=88.70%, word/sec=24936.1445\n",
            "train loss/word=0.0076, acc=99.86%, word/sec=14113.1354\n",
            "train loss/word=0.0084, acc=99.85%, word/sec=14157.9884\n",
            "train loss/word=0.0077, acc=99.85%, word/sec=14090.8469\n",
            "train loss/word=0.0082, acc=99.83%, word/sec=14158.1339\n",
            "train loss/word=0.0081, acc=99.83%, word/sec=14212.5621\n",
            "train loss/word=0.0081, acc=99.84%, word/sec=14246.3358\n",
            "train loss/word=0.0086, acc=99.83%, word/sec=14277.8755\n",
            "train loss/word=0.0090, acc=99.83%, word/sec=14300.0359\n",
            "train loss/word=0.0088, acc=99.83%, word/sec=14320.2231\n",
            "train loss/word=0.0089, acc=99.83%, word/sec=14340.7172\n",
            "dev loss/word=3.7219, acc=88.83%, word/sec=25031.3821\n",
            "train loss/word=0.0034, acc=99.89%, word/sec=14452.6791\n",
            "train loss/word=0.0052, acc=99.87%, word/sec=14545.6868\n",
            "train loss/word=0.0059, acc=99.88%, word/sec=14519.0110\n",
            "train loss/word=0.0063, acc=99.87%, word/sec=14513.5993\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14551.3558\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14556.4563\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14537.5009\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14523.1068\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14535.6414\n",
            "train loss/word=0.0082, acc=99.84%, word/sec=14514.1604\n",
            "dev loss/word=3.5917, acc=88.62%, word/sec=25203.1089\n",
            "train loss/word=0.0077, acc=99.83%, word/sec=14405.3423\n",
            "train loss/word=0.0087, acc=99.83%, word/sec=14561.0152\n",
            "train loss/word=0.0077, acc=99.84%, word/sec=14603.1634\n",
            "train loss/word=0.0078, acc=99.84%, word/sec=14617.6185\n",
            "train loss/word=0.0080, acc=99.83%, word/sec=14614.5479\n",
            "train loss/word=0.0079, acc=99.83%, word/sec=14663.1937\n",
            "train loss/word=0.0085, acc=99.82%, word/sec=14704.4781\n",
            "train loss/word=0.0090, acc=99.82%, word/sec=14719.2919\n",
            "train loss/word=0.0091, acc=99.82%, word/sec=14702.9215\n",
            "train loss/word=0.0092, acc=99.82%, word/sec=14735.2452\n",
            "dev loss/word=3.6918, acc=88.94%, word/sec=25956.3594\n",
            "train loss/word=0.0054, acc=99.87%, word/sec=15186.2582\n",
            "train loss/word=0.0064, acc=99.88%, word/sec=15169.6203\n",
            "train loss/word=0.0061, acc=99.87%, word/sec=15200.7979\n",
            "train loss/word=0.0060, acc=99.87%, word/sec=15182.9510\n",
            "train loss/word=0.0066, acc=99.86%, word/sec=15206.6217\n",
            "train loss/word=0.0070, acc=99.86%, word/sec=15229.4399\n",
            "train loss/word=0.0071, acc=99.86%, word/sec=15245.7329\n",
            "train loss/word=0.0069, acc=99.86%, word/sec=15248.1943\n",
            "train loss/word=0.0074, acc=99.86%, word/sec=15247.7388\n",
            "train loss/word=0.0080, acc=99.85%, word/sec=15240.3736\n",
            "dev loss/word=3.7428, acc=88.63%, word/sec=26140.3824\n",
            "train loss/word=0.0061, acc=99.88%, word/sec=14992.3743\n",
            "train loss/word=0.0060, acc=99.88%, word/sec=14931.7319\n",
            "train loss/word=0.0058, acc=99.88%, word/sec=14991.3750\n",
            "train loss/word=0.0057, acc=99.88%, word/sec=14990.3678\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14977.0872\n",
            "train loss/word=0.0066, acc=99.86%, word/sec=14891.9729\n",
            "train loss/word=0.0073, acc=99.85%, word/sec=14780.8338\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14738.3399\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14681.0765\n",
            "train loss/word=0.0081, acc=99.83%, word/sec=14631.5387\n",
            "dev loss/word=4.0115, acc=88.58%, word/sec=25429.3131\n",
            "train loss/word=0.0089, acc=99.84%, word/sec=14259.7181\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14155.3435\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0072, acc=99.86%, word/sec=14008.0582\n",
            "train loss/word=0.0070, acc=99.86%, word/sec=14001.7855\n",
            "train loss/word=0.0069, acc=99.86%, word/sec=14015.1151\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14045.0392\n",
            "train loss/word=0.0070, acc=99.86%, word/sec=14047.5661\n",
            "train loss/word=0.0072, acc=99.86%, word/sec=14049.4039\n",
            "train loss/word=0.0072, acc=99.86%, word/sec=14083.6039\n",
            "train loss/word=0.0075, acc=99.85%, word/sec=14106.7749\n",
            "dev loss/word=3.8634, acc=88.86%, word/sec=24698.9557\n",
            "train loss/word=0.0077, acc=99.85%, word/sec=14049.6143\n",
            "train loss/word=0.0070, acc=99.85%, word/sec=13983.4751\n",
            "train loss/word=0.0069, acc=99.85%, word/sec=14047.9696\n",
            "train loss/word=0.0082, acc=99.84%, word/sec=14082.0218\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14060.5662\n",
            "train loss/word=0.0085, acc=99.83%, word/sec=14086.4954\n",
            "train loss/word=0.0087, acc=99.83%, word/sec=14023.6762\n",
            "train loss/word=0.0085, acc=99.83%, word/sec=14035.6192\n",
            "train loss/word=0.0083, acc=99.83%, word/sec=14061.5904\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14072.9811\n",
            "dev loss/word=3.6195, acc=88.84%, word/sec=24953.7463\n",
            "train loss/word=0.0079, acc=99.86%, word/sec=14292.7057\n",
            "train loss/word=0.0083, acc=99.86%, word/sec=14317.5794\n",
            "train loss/word=0.0075, acc=99.87%, word/sec=14291.1949\n",
            "train loss/word=0.0079, acc=99.86%, word/sec=14241.2595\n",
            "train loss/word=0.0082, acc=99.85%, word/sec=14243.9767\n",
            "train loss/word=0.0088, acc=99.84%, word/sec=14259.3715\n",
            "train loss/word=0.0090, acc=99.84%, word/sec=14273.2354\n",
            "train loss/word=0.0090, acc=99.84%, word/sec=14299.4388\n",
            "train loss/word=0.0091, acc=99.84%, word/sec=14306.8015\n",
            "train loss/word=0.0091, acc=99.84%, word/sec=14318.7577\n",
            "dev loss/word=3.5937, acc=88.84%, word/sec=24918.0362\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=14357.5549\n",
            "train loss/word=0.0067, acc=99.88%, word/sec=14333.8818\n",
            "train loss/word=0.0075, acc=99.86%, word/sec=14355.6404\n",
            "train loss/word=0.0073, acc=99.85%, word/sec=14377.4068\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14336.0645\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14303.7171\n",
            "train loss/word=0.0080, acc=99.85%, word/sec=14293.1070\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14292.9737\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14303.7508\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14300.1042\n",
            "dev loss/word=3.6069, acc=88.50%, word/sec=25449.5618\n",
            "train loss/word=0.0059, acc=99.88%, word/sec=14111.1887\n",
            "train loss/word=0.0057, acc=99.88%, word/sec=13831.6768\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=13941.0108\n",
            "train loss/word=0.0082, acc=99.85%, word/sec=13963.2001\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=13994.9534\n",
            "train loss/word=0.0081, acc=99.84%, word/sec=14009.2219\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14022.6443\n",
            "train loss/word=0.0080, acc=99.85%, word/sec=14036.7897\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14064.5098\n",
            "train loss/word=0.0084, acc=99.84%, word/sec=14078.9846\n",
            "dev loss/word=3.6196, acc=88.71%, word/sec=25159.0465\n",
            "train loss/word=0.0066, acc=99.86%, word/sec=14347.5072\n",
            "train loss/word=0.0060, acc=99.86%, word/sec=14366.6743\n",
            "train loss/word=0.0075, acc=99.85%, word/sec=14328.3237\n",
            "train loss/word=0.0079, acc=99.85%, word/sec=14296.1047\n",
            "train loss/word=0.0082, acc=99.84%, word/sec=14297.2966\n",
            "train loss/word=0.0086, acc=99.83%, word/sec=14251.0182\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss/word=0.0082, acc=99.84%, word/sec=14266.2422\n",
            "train loss/word=0.0084, acc=99.84%, word/sec=14265.4277\n",
            "train loss/word=0.0087, acc=99.83%, word/sec=14264.1806\n",
            "train loss/word=0.0090, acc=99.83%, word/sec=14280.3914\n",
            "dev loss/word=3.6400, acc=88.94%, word/sec=25045.6253\n",
            "train loss/word=0.0072, acc=99.88%, word/sec=14274.6850\n",
            "train loss/word=0.0069, acc=99.87%, word/sec=14283.8791\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14320.7218\n",
            "train loss/word=0.0066, acc=99.86%, word/sec=14317.7244\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14313.9062\n",
            "train loss/word=0.0071, acc=99.86%, word/sec=14314.5926\n",
            "train loss/word=0.0075, acc=99.85%, word/sec=14278.8566\n",
            "train loss/word=0.0078, acc=99.84%, word/sec=14260.9116\n",
            "train loss/word=0.0080, acc=99.84%, word/sec=14225.6420\n",
            "train loss/word=0.0081, acc=99.83%, word/sec=14233.2507\n",
            "dev loss/word=3.5966, acc=88.79%, word/sec=24648.8546\n",
            "train loss/word=0.0052, acc=99.88%, word/sec=14219.2323\n",
            "train loss/word=0.0049, acc=99.89%, word/sec=14318.4616\n",
            "train loss/word=0.0051, acc=99.88%, word/sec=14379.6886\n",
            "train loss/word=0.0059, acc=99.87%, word/sec=14293.4362\n",
            "train loss/word=0.0062, acc=99.87%, word/sec=14301.0034\n",
            "train loss/word=0.0068, acc=99.86%, word/sec=14310.1530\n",
            "train loss/word=0.0071, acc=99.86%, word/sec=14316.9472\n",
            "train loss/word=0.0073, acc=99.85%, word/sec=14326.1224\n",
            "train loss/word=0.0076, acc=99.85%, word/sec=14334.2501\n",
            "train loss/word=0.0073, acc=99.85%, word/sec=14331.6108\n",
            "dev loss/word=3.6244, acc=88.46%, word/sec=25330.2018\n",
            "train loss/word=0.0049, acc=99.91%, word/sec=14437.2277\n",
            "train loss/word=0.0067, acc=99.86%, word/sec=14485.1342\n",
            "train loss/word=0.0071, acc=99.85%, word/sec=14512.1853\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14500.3162\n",
            "train loss/word=0.0076, acc=99.84%, word/sec=14498.8927\n",
            "train loss/word=0.0079, acc=99.84%, word/sec=14503.4955\n",
            "train loss/word=0.0077, acc=99.84%, word/sec=14480.5316\n",
            "train loss/word=0.0084, acc=99.83%, word/sec=14446.8423\n",
            "train loss/word=0.0085, acc=99.83%, word/sec=14456.3197\n",
            "train loss/word=0.0083, acc=99.84%, word/sec=14464.2056\n",
            "dev loss/word=3.5102, acc=88.59%, word/sec=25367.6938\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}