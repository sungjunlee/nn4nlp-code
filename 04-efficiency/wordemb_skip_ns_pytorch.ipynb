{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordemb-skip-ns-pytorch.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kHAp1LT8G-sf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "aa9152d1-f0df-40de-ff82-c54cc208736c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529029076354,
          "user_tz": -540,
          "elapsed": 3846,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.2.3)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\r\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.23.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.4.16)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ofzd7MdnG_u-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZS__n2CHDPb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "K=3 #number of negative samples\n",
        "N=2 #length of window on each side (so N=2 gives a total window size of 5, as in t-2 t-1 t t+1 t+2)\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "\n",
        "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
        "labels_location = \"labels.txt\" #the file to write the labels to\n",
        "\n",
        "# We reuse the data reading from the language modeling class\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "\n",
        "#word counts for negative sampling\n",
        "word_counts = defaultdict(int)\n",
        "\n",
        "S = w2i[\"<s>\"]\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      line = line.strip().split(\" \")\n",
        "      for word in line:\n",
        "        word_counts[w2i[word]] += 1\n",
        "      yield [w2i[x] for x in line]\n",
        "\n",
        "\n",
        "# Read in the data\n",
        "train = list(read_dataset(\"nn4nlp-code/data/ptb/train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"nn4nlp-code/data/ptb/valid.txt\"))\n",
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)\n",
        "\n",
        "\n",
        "# take the word counts to the 3/4, normalize\n",
        "counts =  np.array([list(x) for x in word_counts.items()])[:,1]**.75\n",
        "normalizing_constant = sum(counts)\n",
        "word_probabilities = np.zeros(nwords)\n",
        "for word_id in word_counts:\n",
        "  word_probabilities[word_id] = word_counts[word_id]**.75/normalizing_constant\n",
        "\n",
        "with open(labels_location, 'w') as labels_file:\n",
        "  for i in range(nwords):\n",
        "    labels_file.write(i2w[i] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wMjjTJS-DNN_",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, padding_idx=0):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    \n",
        "    self.vocab_size = vocab_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.embeddings_i = nn.Embedding(vocab_size, embed_dim, padding_idx)\n",
        "    self.embeddings_o = nn.Embedding(vocab_size, embed_dim, padding_idx)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.forward_i(x)\n",
        "  \n",
        "  def forward_i(self, x):\n",
        "    return self.embeddings_i(x)\n",
        "  \n",
        "  def forward_o(self, x):\n",
        "    return self.embeddings_o(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pzehg6Y1HQBv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class SGNS(nn.Module):\n",
        "  def __init__(self, word2vec, n_neg_samp=10, word_probs=None):\n",
        "    super(SGNS, self).__init__()\n",
        "    self.word2vec = word2vec\n",
        "    self.vocab_size = self.word2vec.vocab_size\n",
        "    self.n_neg_samp = n_neg_samp\n",
        "    if word_probs is not None:\n",
        "      wp = np.power(word_probs, 0.75)\n",
        "      wp = wp / wp.sum()\n",
        "      self.word_probs = torch.FloatTensor(wp)\n",
        "    else:\n",
        "      self.word_probs = None\n",
        "    \n",
        "  def forward(self, iword, owords):\n",
        "    batch_size = owords.shape[0]\n",
        "    context_size = 1\n",
        "    if self.word_probs is not None:\n",
        "      neg_words = torch.multinomial(self.word_probs, \n",
        "                                    batch_size * context_size *self.n_neg_samp, \n",
        "                                    replacement=True).view(batch_size, -1).to(device)\n",
        "    else:\n",
        "      neg_words = torch.FloatTensor(batch_size, context_size *self.n_neg_samp).uniform_(0, self.vocab_size - 1).long().to(device)\n",
        "    ivectors = self.word2vec.forward_i(iword) # bs x emb_dim\n",
        "    ovectors = self.word2vec.forward_o(owords) # bs x emb_dim\n",
        "    nvectors = self.word2vec.forward_o(neg_words) # bs x neg_samp x emb_dim\n",
        "    pos_score = F.logsigmoid(torch.bmm(ovectors.unsqueeze(1), ivectors.unsqueeze(2))).squeeze()\n",
        "    neg_score = F.logsigmoid(-torch.bmm(nvectors, ivectors.unsqueeze(2))).sum(1).squeeze()\n",
        "#     if -(pos_score + neg_score).mean() > 1000.0:\n",
        "#       print(ivectors.shape, ovectors.shape, nvectors.shape)\n",
        "#       print(pos_score.shape, neg_score.shape)\n",
        "#       print(pos_score.mean(), neg_score.mean())\n",
        "    return -(pos_score + neg_score).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A6poyOQuxxlZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def get_next(data, bs=4):\n",
        "  iwords = []\n",
        "  owords = []\n",
        "  for sent_id, sent in enumerate(data):\n",
        "    for word_id, word in enumerate(sent):\n",
        "      ow_ids = list(range(word_id)) + list(range(word_id+1, len(sent)))\n",
        "      for oword_id in ow_ids:\n",
        "        iwords.append(word)\n",
        "        owords.append(sent[oword_id])\n",
        "        if len(iwords) >= bs:\n",
        "          yield torch.LongTensor(iwords).to(device), torch.LongTensor(owords).to(device)\n",
        "          iwords, owords = [], []\n",
        "  if len(iwords) > 0:\n",
        "    yield torch.LongTensor(iwords).to(device), torch.LongTensor(owords).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RZ-oLIKPujEe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWSjkXFzWS2A",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ur7hi_2ZvZwD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "fb4c55ae-e3f9-46bb-f499-36947aa85a5b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529029082784,
          "user_tz": -540,
          "elapsed": 646,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "word2vec = Word2Vec(nwords, EMB_SIZE).to(device)\n",
        "model = SGNS(word2vec, K, word_probabilities).to(device)\n",
        "trainer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "model"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGNS(\n",
              "  (word2vec): Word2Vec(\n",
              "    (embeddings_i): Embedding(10000, 128, padding_idx=0)\n",
              "    (embeddings_o): Embedding(10000, 128, padding_idx=0)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "metadata": {
        "id": "x7O1NwF_Gu-F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# for batch_id, (iword, owords) in enumerate(get_next(train, batch_size)):\n",
        "#   my_loss = model(iword, owords)\n",
        "#   if my_loss> 100.0:\n",
        "#     print(\"WWW %f\" % my_loss)\n",
        "#     break\n",
        "#   if batch_id % 10000 == 0:\n",
        "#     print(my_loss)\n",
        "#   if batch_id >= 1000000:\n",
        "#     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8yv3ZPRIEaW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "bdce48c4-fb86-4ab2-b34c-9b429bb3421a",
        "executionInfo": {
          "status": "error",
          "timestamp": 1529029093006,
          "user_tz": -540,
          "elapsed": 9336,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "batch_size = 64\n",
        "\n",
        "for ITER in range(100):\n",
        "  print(\"started iter %r\" % ITER)\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for batch_id, (iword, owords) in enumerate(get_next(train, batch_size)):\n",
        "    my_loss = model(iword, owords)\n",
        "    train_loss += my_loss.item()\n",
        "    train_words += len(iword)\n",
        "    my_loss.backward()\n",
        "    trainer.step()\n",
        "    if (batch_id+1) % 10000 == 0:\n",
        "      print(train_loss, train_words)\n",
        "      print(\"--finished %r words. loss/word=%.4f\" % (batch_id*batch_size+1, train_loss/train_words))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for batch_id, (iword, owords) in enumerate(get_next(dev, batch_size)):\n",
        "    my_loss = model(iword, owords)\n",
        "    dev_loss += my_loss.item()\n",
        "    dev_words += len(iword)\n",
        "    trainer.step()\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
        "\n",
        "  print(\"saving embedding files\")\n",
        "  with open(embeddings_location, 'w') as embeddings_file:\n",
        "    W_w_np = word2vec.embeddings_i.weight.data.numpy()\n",
        "    for i in range(nwords):\n",
        "      ith_embedding = '\\t'.join(map(str, W_w_np[i]))\n",
        "      embeddings_file.write(ith_embedding + '\\n')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started iter 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-b1beb80e19cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "RzAF-_-PW8Qg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}