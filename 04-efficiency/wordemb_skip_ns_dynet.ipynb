{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordemb-skip-ns-dynet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "v9GEO95-F3IP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "402d7cd5-40d1-4a5b-8746-3ad30adf78c9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528724801173,
          "user_tz": -540,
          "elapsed": 3542,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dynet in /usr/local/lib/python2.7/dist-packages (2.0.3)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from dynet) (1.14.3)\r\n",
            "Requirement already satisfied: cython in /usr/local/lib/python2.7/dist-packages (from dynet) (0.28.3)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C4M6jfqaF8eU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import dynet as dy\n",
        "import numpy as np\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oXQesHe2F96t",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "K=3 #number of negative samples\n",
        "N=2 #length of window on each side (so N=2 gives a total window size of 5, as in t-2 t-1 t t+1 t+2)\n",
        "EMB_SIZE = 128 # The size of the embedding\n",
        "\n",
        "embeddings_location = \"embeddings.txt\" #the file to write the word embeddings to\n",
        "labels_location = \"labels.txt\" #the file to write the labels to\n",
        "\n",
        "# We reuse the data reading from the language modeling class\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "\n",
        "#word counts for negative sampling\n",
        "word_counts = defaultdict(int)\n",
        "\n",
        "S = w2i[\"<s>\"]\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "    for line in f:\n",
        "      line = line.strip().split(\" \")\n",
        "      for word in line:\n",
        "        word_counts[w2i[word]] += 1\n",
        "      yield [w2i[x] for x in line]\n",
        "\n",
        "\n",
        "# Read in the data\n",
        "\n",
        "train = list(read_dataset(\"nn4nlp-code/data/ptb/train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"nn4nlp-code/data/ptb/valid.txt\"))\n",
        "i2w = {v: k for k, v in w2i.items()}\n",
        "nwords = len(w2i)\n",
        "\n",
        "\n",
        "# take the word counts to the 3/4, normalize\n",
        "counts =  np.array([list(x) for x in word_counts.items()])[:,1]**.75\n",
        "normalizing_constant = sum(counts)\n",
        "word_probabilities = np.zeros(nwords)\n",
        "for word_id in word_counts:\n",
        "  word_probabilities[word_id] = word_counts[word_id]**.75/normalizing_constant\n",
        "\n",
        "with open(labels_location, 'w') as labels_file:\n",
        "  for i in range(nwords):\n",
        "    labels_file.write(i2w[i] + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-RJkHblBF0MT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 4427
        },
        "outputId": "1c5bdda7-c404-41f2-f1d7-f276445e3150"
      },
      "cell_type": "code",
      "source": [
        "# Start DyNet and define trainer\n",
        "model = dy.Model()\n",
        "trainer = dy.SimpleSGDTrainer(model, learning_rate=0.1)\n",
        "\n",
        "# Define the model\n",
        "W_c_p = model.add_lookup_parameters((nwords, EMB_SIZE)) # Context word weights\n",
        "W_w_p = model.add_lookup_parameters((nwords, EMB_SIZE)) # Word weights\n",
        "\n",
        "# Calculate the loss value for the entire sentence\n",
        "def calc_sent_loss(sent):\n",
        "  # Create a computation graph\n",
        "  dy.renew_cg()\n",
        "  \n",
        "  # Get embeddings for the sentence\n",
        "  emb = [W_w_p[x] for x in sent]\n",
        "\n",
        "  # Sample K negative words for each predicted word at each position\n",
        "  all_neg_words = np.random.choice(nwords, size=2*N*K*len(emb), replace=True, p=word_probabilities)\n",
        "\n",
        "  # W_w = dy.parameter(W_w_p)\n",
        "  # Step through the sentence and calculate the negative and positive losses\n",
        "  all_losses = [] \n",
        "  for i, my_emb in enumerate(emb):\n",
        "    neg_words = all_neg_words[i*K*2*N:(i+1)*K*2*N]\n",
        "    pos_words = ([sent[x] if x >= 0 else S for x in range(i-N,i)] +\n",
        "                 [sent[x] if x < len(sent) else S for x in range(i+1,i+N+1)])\n",
        "    neg_loss = -dy.log(dy.logistic(-dy.dot_product(my_emb, dy.lookup_batch(W_c_p, neg_words))))\n",
        "    pos_loss = -dy.log(dy.logistic(dy.dot_product(my_emb, dy.lookup_batch(W_c_p, pos_words))))\n",
        "    all_losses.append(dy.sum_batches(neg_loss) + dy.sum_batches(pos_loss))\n",
        "  return dy.esum(all_losses)\n",
        "\n",
        "MAX_LEN = 100\n",
        "\n",
        "for ITER in range(100):\n",
        "  print(\"started iter %r\" % ITER)\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(train):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    train_loss += my_loss.value()\n",
        "    train_words += len(sent)\n",
        "    my_loss.backward()\n",
        "    trainer.update()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences\" % (sent_id+1))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(dev):\n",
        "    my_loss = calc_sent_loss(sent)\n",
        "    dev_loss += my_loss.value()\n",
        "    dev_words += len(sent)\n",
        "    trainer.update()\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
        "\n",
        "  print(\"saving embedding files\")\n",
        "  with open(embeddings_location, 'w') as embeddings_file:\n",
        "    W_w_np = W_w_p.as_array()\n",
        "    for i in range(nwords):\n",
        "      ith_embedding = '\\t'.join(map(str, W_w_np[i]))\n",
        "      embeddings_file.write(ith_embedding + '\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started iter 0\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 0: train loss/word=8.1177, ppl=3353.4324, time=269.00s\n",
            "iter 0: dev loss/word=7.5208, ppl=1846.1113, time=8.21s\n",
            "saving embedding files\n",
            "started iter 1\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 1: train loss/word=7.2376, ppl=1390.8127, time=249.21s\n",
            "iter 1: dev loss/word=7.2743, ppl=1442.7288, time=7.19s\n",
            "saving embedding files\n",
            "started iter 2\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 2: train loss/word=6.9978, ppl=1094.1726, time=245.99s\n",
            "iter 2: dev loss/word=7.2185, ppl=1364.4174, time=7.20s\n",
            "saving embedding files\n",
            "started iter 3\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 3: train loss/word=6.8445, ppl=938.6595, time=246.80s\n",
            "iter 3: dev loss/word=7.1591, ppl=1285.7387, time=7.24s\n",
            "saving embedding files\n",
            "started iter 4\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 4: train loss/word=6.7372, ppl=843.1771, time=246.26s\n",
            "iter 4: dev loss/word=7.1512, ppl=1275.6148, time=7.21s\n",
            "saving embedding files\n",
            "started iter 5\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 5: train loss/word=6.6510, ppl=773.5312, time=246.26s\n",
            "iter 5: dev loss/word=7.1434, ppl=1265.7453, time=7.22s\n",
            "saving embedding files\n",
            "started iter 6\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 6: train loss/word=6.5809, ppl=721.1697, time=246.23s\n",
            "iter 6: dev loss/word=7.1500, ppl=1274.1251, time=7.17s\n",
            "saving embedding files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "started iter 7\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 7: train loss/word=6.5169, ppl=676.4564, time=246.00s\n",
            "iter 7: dev loss/word=7.1678, ppl=1297.0433, time=7.19s\n",
            "saving embedding files\n",
            "started iter 8\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 8: train loss/word=6.4650, ppl=642.2640, time=246.78s\n",
            "iter 8: dev loss/word=7.1821, ppl=1315.6682, time=7.25s\n",
            "saving embedding files\n",
            "started iter 9\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 9: train loss/word=6.4202, ppl=614.1212, time=247.26s\n",
            "iter 9: dev loss/word=7.1939, ppl=1331.2535, time=7.19s\n",
            "saving embedding files\n",
            "started iter 10\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 10: train loss/word=6.3779, ppl=588.7025, time=246.69s\n",
            "iter 10: dev loss/word=7.2171, ppl=1362.5189, time=7.18s\n",
            "saving embedding files\n",
            "started iter 11\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 11: train loss/word=6.3441, ppl=569.1253, time=246.08s\n",
            "iter 11: dev loss/word=7.2368, ppl=1389.6513, time=7.29s\n",
            "saving embedding files\n",
            "started iter 12\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 12: train loss/word=6.3104, ppl=550.2580, time=246.79s\n",
            "iter 12: dev loss/word=7.2534, ppl=1412.8316, time=7.23s\n",
            "saving embedding files\n",
            "started iter 13\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 13: train loss/word=6.2799, ppl=533.7236, time=246.32s\n",
            "iter 13: dev loss/word=7.2654, ppl=1429.9899, time=7.19s\n",
            "saving embedding files\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "started iter 14\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 14: train loss/word=6.2528, ppl=519.4526, time=246.64s\n",
            "iter 14: dev loss/word=7.2974, ppl=1476.4037, time=7.23s\n",
            "saving embedding files\n",
            "started iter 15\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 15: train loss/word=6.2295, ppl=507.5243, time=247.27s\n",
            "iter 15: dev loss/word=7.3010, ppl=1481.8051, time=7.21s\n",
            "saving embedding files\n",
            "started iter 16\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 16: train loss/word=6.2061, ppl=495.7798, time=246.57s\n",
            "iter 16: dev loss/word=7.3127, ppl=1499.1486, time=7.28s\n",
            "saving embedding files\n",
            "started iter 17\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 17: train loss/word=6.1844, ppl=485.0988, time=246.25s\n",
            "iter 17: dev loss/word=7.3293, ppl=1524.3332, time=7.24s\n",
            "saving embedding files\n",
            "started iter 18\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 18: train loss/word=6.1663, ppl=476.4232, time=247.03s\n",
            "iter 18: dev loss/word=7.3632, ppl=1576.8817, time=7.22s\n",
            "saving embedding files\n",
            "started iter 19\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n",
            "--finished 25000 sentences\n",
            "--finished 30000 sentences\n",
            "--finished 35000 sentences\n",
            "--finished 40000 sentences\n",
            "iter 19: train loss/word=6.1507, ppl=469.0298, time=245.46s\n",
            "iter 19: dev loss/word=7.3940, ppl=1626.2239, time=7.20s\n",
            "saving embedding files\n",
            "started iter 20\n",
            "--finished 5000 sentences\n",
            "--finished 10000 sentences\n",
            "--finished 15000 sentences\n",
            "--finished 20000 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}