{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "batched_enc_dec_dynet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "aGvhAGoUEDmb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c92b0d04-25b2-432e-da3a-be35e9f6dda8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529680611801,
          "user_tz": -540,
          "elapsed": 5185,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dynet in /usr/local/lib/python3.6/dist-packages (2.0.3)\r\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from dynet) (0.28.3)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dynet) (1.14.5)\n",
            "fatal: destination path 'nn4nlp-code' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cg9wRzBvEEOl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3617
        },
        "outputId": "c4a058ea-0241-474b-d8d1-6935c4742e16",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529683459651,
          "user_tz": -540,
          "elapsed": 2847828,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import dynet as dy\n",
        "import numpy as np\n",
        "import pdb\n",
        "\n",
        "\n",
        "#some of this code borrowed from Qinlan Shen's attention from the MT class last year\n",
        "#much of the beginning is the same as the text retrieval\n",
        "# format of files: each line is \"word1 word2 ...\" aligned line-by-line\n",
        "train_src_file = \"nn4nlp-code/data/parallel/train.ja\"\n",
        "train_trg_file = \"nn4nlp-code/data/parallel/train.en\"\n",
        "dev_src_file = \"nn4nlp-code/data/parallel/dev.ja\"\n",
        "dev_trg_file = \"nn4nlp-code/data/parallel/dev.en\"\n",
        "\n",
        "w2i_src = defaultdict(lambda: len(w2i_src))\n",
        "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
        "\n",
        "\n",
        "# Creates batches where all source sentences are the same length\n",
        "def create_batches(sorted_dataset, max_batch_size):\n",
        "    source = [x[0] for x in sorted_dataset]\n",
        "    src_lengths = [len(x) for x in source]\n",
        "    batches = []\n",
        "    prev = src_lengths[0]\n",
        "    prev_start = 0\n",
        "    batch_size = 1\n",
        "    for i in range(1, len(src_lengths)):\n",
        "        if src_lengths[i] != prev or batch_size == max_batch_size:\n",
        "            batches.append((prev_start, batch_size))\n",
        "            prev = src_lengths[i]\n",
        "            prev_start = i\n",
        "            batch_size = 1\n",
        "        else:\n",
        "            batch_size += 1\n",
        "    return batches\n",
        "\n",
        "\n",
        "def read(fname_src, fname_trg):\n",
        "    \"\"\"\n",
        "    Read parallel files where each line lines up\n",
        "    \"\"\"\n",
        "    with open(fname_src, \"r\") as f_src, open(fname_trg, \"r\") as f_trg:\n",
        "        for line_src, line_trg in zip(f_src, f_trg):\n",
        "            #need to append EOS tags to at least the target sentence\n",
        "            sent_src = [w2i_src[x] for x in line_src.strip().split() + ['</s>']] \n",
        "            sent_trg = [w2i_trg[x] for x in ['<s>'] + line_trg.strip().split() + ['</s>']] \n",
        "            yield (sent_src, sent_trg)\n",
        "\n",
        "# Read the data\n",
        "train = list(read(train_src_file, train_trg_file))\n",
        "unk_src = w2i_src[\"<unk>\"]\n",
        "eos_src = w2i_src['</s>']\n",
        "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
        "unk_trg = w2i_trg[\"<unk>\"]\n",
        "eos_trg = w2i_trg['</s>']\n",
        "sos_trg = w2i_trg['<s>']\n",
        "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
        "i2w_trg = {v: k for k, v in w2i_trg.items()}\n",
        "\n",
        "nwords_src = len(w2i_src)\n",
        "nwords_trg = len(w2i_trg)\n",
        "dev = list(read(dev_src_file, dev_trg_file))\n",
        "\n",
        "# DyNet Starts\n",
        "model = dy.Model()\n",
        "trainer = dy.AdamTrainer(model)\n",
        "\n",
        "# Model parameters\n",
        "EMBED_SIZE = 64\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "#Especially in early training, the model can generate basically infinitly without generating an EOS\n",
        "#have a max sent size that you end at\n",
        "MAX_SENT_SIZE = 50\n",
        "\n",
        "# Lookup parameters for word embeddings\n",
        "LOOKUP_SRC = model.add_lookup_parameters((nwords_src, EMBED_SIZE))\n",
        "LOOKUP_TRG = model.add_lookup_parameters((nwords_trg, EMBED_SIZE))\n",
        "\n",
        "# Word-level LSTMs\n",
        "LSTM_SRC_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model)\n",
        "LSTM_TRG_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model)\n",
        "\n",
        "#the softmax from the hidden size \n",
        "W_sm_p = model.add_parameters((nwords_trg, HIDDEN_SIZE))         # Weights of the softmax\n",
        "b_sm_p = model.add_parameters((nwords_trg))                   # Softmax bias\n",
        "\n",
        "\n",
        "\n",
        "def calc_loss(sents):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    src_sents = [x[0] for x in sents]\n",
        "    tgt_sents = [x[1] for x in sents]\n",
        "    src_cws = []\n",
        "\n",
        "    src_len = [len(sent) for sent in src_sents]        \n",
        "    max_src_len = np.max(src_len)\n",
        "    num_words = 0\n",
        "\n",
        "    for i in range(max_src_len):\n",
        "        src_cws.append([sent[i] for sent in src_sents])\n",
        "\n",
        "\n",
        "    #initialize the LSTM\n",
        "    init_state_src = LSTM_SRC_BUILDER.initial_state()\n",
        "\n",
        "    #get the output of the first LSTM\n",
        "    src_output = init_state_src.add_inputs([dy.lookup_batch(LOOKUP_SRC, cws) for cws in src_cws])[-1].output()\n",
        "    #now decode\n",
        "    all_losses = []\n",
        "\n",
        "    # Decoder\n",
        "    #need to mask padding at end of sentence\n",
        "    tgt_cws = []\n",
        "    tgt_len = [len(sent) for sent in sents]\n",
        "    max_tgt_len = np.max(tgt_len)\n",
        "    masks = []\n",
        "\n",
        "    for i in range(max_tgt_len):\n",
        "        tgt_cws.append([sent[i] if len(sent) > i else eos_trg for sent in tgt_sents])\n",
        "        mask = [(1 if len(sent) > i else 0) for sent in tgt_sents]\n",
        "        masks.append(mask)\n",
        "        num_words += sum(mask)\n",
        "\n",
        "\n",
        "\n",
        "    current_state = LSTM_TRG_BUILDER.initial_state().set_s([src_output, dy.tanh(src_output)])\n",
        "    prev_words = tgt_cws[0]\n",
        "    W_sm = dy.parameter(W_sm_p)\n",
        "    b_sm = dy.parameter(b_sm_p)\n",
        "\n",
        "    for next_words, mask in zip(tgt_cws[1:], masks):\n",
        "        #feed the current state into the \n",
        "        current_state = current_state.add_input(dy.lookup_batch(LOOKUP_TRG, prev_words))\n",
        "        output_embedding = current_state.output()\n",
        "\n",
        "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
        "        loss = (dy.pickneglogsoftmax_batch(s, next_words))\n",
        "        mask_expr = dy.inputVector(mask)\n",
        "        mask_expr = dy.reshape(mask_expr, (1,),len(sents))\n",
        "        mask_loss = loss * mask_expr\n",
        "        all_losses.append(mask_loss)\n",
        "        prev_words = next_words\n",
        "    return dy.sum_batches(dy.esum(all_losses)), num_words\n",
        "\n",
        "def generate(sent):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    sent_reps = [LSTM_SRC.transduce([LOOKUP_SRC[x] for x in src])[-1] for src, trg in sents]\n",
        "\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    src = sent[0]\n",
        "    trg = sent[1]\n",
        "\n",
        "\n",
        "    #initialize the LSTM\n",
        "    init_state_src = LSTM_SRC_BUILDER.initial_state()\n",
        "\n",
        "    #get the output of the first LSTM\n",
        "    src_output = init_state_src.add_inputs([LOOKUP_SRC[x] for x in src])[-1].output()\n",
        "\n",
        "    #generate until a eos tag or max is reached\n",
        "    current_state = LSTM_TRG_BUILDER.initial_state().set_s([src_output, dy.tanh(src_output)])\n",
        "\n",
        "    prev_word = sos_trg\n",
        "    trg_sent = []\n",
        "    W_sm = dy.parameter(W_sm_p)\n",
        "    b_sm = dy.parameter(b_sm_p)\n",
        "\n",
        "    for i in range(MAX_SENT_SIZE):\n",
        "        #feed the previous word into the lstm, calculate the most likely word, add it to the sentence\n",
        "        current_state = current_state.add_input(LOOKUP_TRG[prev_word])\n",
        "        output_embedding = hidden_state.output()\n",
        "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
        "        probs = -dy.log_softmax(s).value()\n",
        "        next_word = np.argmax(probs)\n",
        "\n",
        "        if next_word == eos_trg:\n",
        "            break\n",
        "        prev_word = next_word\n",
        "        trg_sent.append(i2w_trg[next_word])\n",
        "    return trg_sent\n",
        "\n",
        "for ITER in range(100):\n",
        "  # Perform training\n",
        "  train.sort(key=lambda t: len(t[0]), reverse=True)\n",
        "  dev.sort(key=lambda t: len(t[0]), reverse=True)\n",
        "  train_order = create_batches(train, BATCH_SIZE) \n",
        "  dev_order = create_batches(dev, BATCH_SIZE)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, (start, length) in enumerate(train_order):\n",
        "    train_batch = train[start:start+length]\n",
        "    my_loss, num_words = calc_loss(train_batch)\n",
        "    train_loss += my_loss.value()\n",
        "    train_words += num_words\n",
        "    my_loss.backward()\n",
        "    trainer.update()\n",
        "    if (sent_id+1) % 5000 == 0:\n",
        "      print(\"--finished %r sentences\" % (sent_id+1))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, (start, length) in enumerate(dev_order):\n",
        "    dev_batch = dev[start:start+length]\n",
        "    my_loss, num_words = calc_loss(dev_batch)\n",
        "    dev_loss += my_loss.value()\n",
        "    dev_words += num_words\n",
        "    trainer.update()\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0: train loss/word=2.2086, ppl=9.1031, time=1529670659.63s\n",
            "iter 0: dev loss/word=2.2088, ppl=9.1048, time=1529680153.47s\n",
            "iter 1: train loss/word=1.8026, ppl=6.0652, time=1529670689.02s\n",
            "iter 1: dev loss/word=2.4381, ppl=11.4517, time=1529680182.86s\n",
            "iter 2: train loss/word=1.6373, ppl=5.1410, time=1529670718.44s\n",
            "iter 2: dev loss/word=2.2970, ppl=9.9448, time=1529680212.28s\n",
            "iter 3: train loss/word=1.4634, ppl=4.3208, time=1529670747.85s\n",
            "iter 3: dev loss/word=2.1379, ppl=8.4815, time=1529680241.70s\n",
            "iter 4: train loss/word=1.3368, ppl=3.8067, time=1529670777.31s\n",
            "iter 4: dev loss/word=1.9778, ppl=7.2265, time=1529680271.16s\n",
            "iter 5: train loss/word=1.2323, ppl=3.4292, time=1529670806.64s\n",
            "iter 5: dev loss/word=1.9710, ppl=7.1780, time=1529680300.47s\n",
            "iter 6: train loss/word=1.1411, ppl=3.1302, time=1529670835.87s\n",
            "iter 6: dev loss/word=2.0018, ppl=7.4024, time=1529680329.72s\n",
            "iter 7: train loss/word=1.0633, ppl=2.8959, time=1529670865.17s\n",
            "iter 7: dev loss/word=2.0171, ppl=7.5162, time=1529680359.01s\n",
            "iter 8: train loss/word=0.9892, ppl=2.6890, time=1529670894.48s\n",
            "iter 8: dev loss/word=2.0057, ppl=7.4313, time=1529680388.32s\n",
            "iter 9: train loss/word=0.9128, ppl=2.4912, time=1529670923.84s\n",
            "iter 9: dev loss/word=2.0545, ppl=7.8031, time=1529680417.69s\n",
            "iter 10: train loss/word=0.8399, ppl=2.3162, time=1529670953.20s\n",
            "iter 10: dev loss/word=2.1006, ppl=8.1713, time=1529680447.05s\n",
            "iter 11: train loss/word=0.7704, ppl=2.1606, time=1529670982.12s\n",
            "iter 11: dev loss/word=2.1883, ppl=8.9197, time=1529680475.87s\n",
            "iter 12: train loss/word=0.7041, ppl=2.0220, time=1529671010.34s\n",
            "iter 12: dev loss/word=2.2702, ppl=9.6809, time=1529680504.10s\n",
            "iter 13: train loss/word=0.6480, ppl=1.9117, time=1529671038.58s\n",
            "iter 13: dev loss/word=2.2757, ppl=9.7350, time=1529680532.34s\n",
            "iter 14: train loss/word=0.5859, ppl=1.7965, time=1529671066.81s\n",
            "iter 14: dev loss/word=2.3482, ppl=10.4663, time=1529680560.57s\n",
            "iter 15: train loss/word=0.5302, ppl=1.6993, time=1529671095.10s\n",
            "iter 15: dev loss/word=2.3767, ppl=10.7692, time=1529680588.86s\n",
            "iter 16: train loss/word=0.4805, ppl=1.6169, time=1529671123.39s\n",
            "iter 16: dev loss/word=2.4457, ppl=11.5383, time=1529680617.16s\n",
            "iter 17: train loss/word=0.4331, ppl=1.5420, time=1529671151.72s\n",
            "iter 17: dev loss/word=2.4896, ppl=12.0562, time=1529680645.50s\n",
            "iter 18: train loss/word=0.3934, ppl=1.4821, time=1529671180.04s\n",
            "iter 18: dev loss/word=2.5663, ppl=13.0180, time=1529680673.79s\n",
            "iter 19: train loss/word=0.3518, ppl=1.4216, time=1529671208.35s\n",
            "iter 19: dev loss/word=2.6284, ppl=13.8517, time=1529680702.09s\n",
            "iter 20: train loss/word=0.3108, ppl=1.3646, time=1529671236.60s\n",
            "iter 20: dev loss/word=2.6932, ppl=14.7783, time=1529680730.34s\n",
            "iter 21: train loss/word=0.2780, ppl=1.3205, time=1529671264.82s\n",
            "iter 21: dev loss/word=2.7505, ppl=15.6503, time=1529680758.57s\n",
            "iter 22: train loss/word=0.2477, ppl=1.2811, time=1529671293.12s\n",
            "iter 22: dev loss/word=2.7346, ppl=15.4037, time=1529680786.88s\n",
            "iter 23: train loss/word=0.2206, ppl=1.2468, time=1529671321.34s\n",
            "iter 23: dev loss/word=2.8015, ppl=16.4693, time=1529680815.09s\n",
            "iter 24: train loss/word=0.1947, ppl=1.2150, time=1529671349.58s\n",
            "iter 24: dev loss/word=2.8492, ppl=17.2737, time=1529680843.32s\n",
            "iter 25: train loss/word=0.1707, ppl=1.1861, time=1529671377.76s\n",
            "iter 25: dev loss/word=2.9032, ppl=18.2318, time=1529680871.51s\n",
            "iter 26: train loss/word=0.1462, ppl=1.1575, time=1529671405.93s\n",
            "iter 26: dev loss/word=2.9739, ppl=19.5685, time=1529680899.68s\n",
            "iter 27: train loss/word=0.1333, ppl=1.1426, time=1529671434.10s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 27: dev loss/word=3.0381, ppl=20.8658, time=1529680927.84s\n",
            "iter 28: train loss/word=0.1206, ppl=1.1281, time=1529671462.46s\n",
            "iter 28: dev loss/word=3.1096, ppl=22.4112, time=1529680956.22s\n",
            "iter 29: train loss/word=0.1045, ppl=1.1102, time=1529671490.64s\n",
            "iter 29: dev loss/word=3.1077, ppl=22.3691, time=1529680984.39s\n",
            "iter 30: train loss/word=0.0921, ppl=1.0965, time=1529671518.88s\n",
            "iter 30: dev loss/word=3.1497, ppl=23.3287, time=1529681012.64s\n",
            "iter 31: train loss/word=0.0769, ppl=1.0800, time=1529671547.13s\n",
            "iter 31: dev loss/word=3.2458, ppl=25.6833, time=1529681040.88s\n",
            "iter 32: train loss/word=0.0748, ppl=1.0777, time=1529671575.43s\n",
            "iter 32: dev loss/word=3.2386, ppl=25.4974, time=1529681069.18s\n",
            "iter 33: train loss/word=0.0654, ppl=1.0676, time=1529671603.71s\n",
            "iter 33: dev loss/word=3.2893, ppl=26.8246, time=1529681097.47s\n",
            "iter 34: train loss/word=0.0578, ppl=1.0595, time=1529671632.53s\n",
            "iter 34: dev loss/word=3.3192, ppl=27.6390, time=1529681126.33s\n",
            "iter 35: train loss/word=0.0520, ppl=1.0534, time=1529671661.11s\n",
            "iter 35: dev loss/word=3.3972, ppl=29.8815, time=1529681154.90s\n",
            "iter 36: train loss/word=0.0452, ppl=1.0463, time=1529671689.59s\n",
            "iter 36: dev loss/word=3.3976, ppl=29.8934, time=1529681183.39s\n",
            "iter 37: train loss/word=0.0416, ppl=1.0424, time=1529671718.26s\n",
            "iter 37: dev loss/word=3.4350, ppl=31.0316, time=1529681212.04s\n",
            "iter 38: train loss/word=0.0375, ppl=1.0382, time=1529671746.62s\n",
            "iter 38: dev loss/word=3.4024, ppl=30.0348, time=1529681240.38s\n",
            "iter 39: train loss/word=0.0383, ppl=1.0391, time=1529671774.87s\n",
            "iter 39: dev loss/word=3.4642, ppl=31.9495, time=1529681268.63s\n",
            "iter 40: train loss/word=0.0320, ppl=1.0325, time=1529671803.09s\n",
            "iter 40: dev loss/word=3.4554, ppl=31.6717, time=1529681296.83s\n",
            "iter 41: train loss/word=0.0283, ppl=1.0287, time=1529671831.30s\n",
            "iter 41: dev loss/word=3.5663, ppl=35.3856, time=1529681325.07s\n",
            "iter 42: train loss/word=0.0261, ppl=1.0265, time=1529671859.63s\n",
            "iter 42: dev loss/word=3.6170, ppl=37.2255, time=1529681353.38s\n",
            "iter 43: train loss/word=0.0242, ppl=1.0245, time=1529671887.85s\n",
            "iter 43: dev loss/word=3.6998, ppl=40.4372, time=1529681381.60s\n",
            "iter 44: train loss/word=0.0256, ppl=1.0259, time=1529671916.12s\n",
            "iter 44: dev loss/word=3.6820, ppl=39.7274, time=1529681409.87s\n",
            "iter 45: train loss/word=0.0190, ppl=1.0192, time=1529671944.37s\n",
            "iter 45: dev loss/word=3.6851, ppl=39.8477, time=1529681438.12s\n",
            "iter 46: train loss/word=0.0194, ppl=1.0196, time=1529671972.57s\n",
            "iter 46: dev loss/word=3.7241, ppl=41.4326, time=1529681466.31s\n",
            "iter 47: train loss/word=0.0148, ppl=1.0149, time=1529672000.78s\n",
            "iter 47: dev loss/word=3.6725, ppl=39.3517, time=1529681494.52s\n",
            "iter 48: train loss/word=0.0163, ppl=1.0164, time=1529672028.91s\n",
            "iter 48: dev loss/word=3.6882, ppl=39.9725, time=1529681522.65s\n",
            "iter 49: train loss/word=0.0130, ppl=1.0131, time=1529672057.12s\n",
            "iter 49: dev loss/word=3.6776, ppl=39.5533, time=1529681550.89s\n",
            "iter 50: train loss/word=0.0138, ppl=1.0139, time=1529672085.32s\n",
            "iter 50: dev loss/word=3.7501, ppl=42.5272, time=1529681579.08s\n",
            "iter 51: train loss/word=0.0134, ppl=1.0135, time=1529672113.56s\n",
            "iter 51: dev loss/word=3.7852, ppl=44.0444, time=1529681607.30s\n",
            "iter 52: train loss/word=0.0091, ppl=1.0091, time=1529672141.73s\n",
            "iter 52: dev loss/word=3.8088, ppl=45.0949, time=1529681635.47s\n",
            "iter 53: train loss/word=0.0126, ppl=1.0127, time=1529672169.94s\n",
            "iter 53: dev loss/word=3.8142, ppl=45.3399, time=1529681663.68s\n",
            "iter 54: train loss/word=0.0099, ppl=1.0099, time=1529672198.17s\n",
            "iter 54: dev loss/word=3.9565, ppl=52.2756, time=1529681691.94s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 55: train loss/word=0.0087, ppl=1.0087, time=1529672226.35s\n",
            "iter 55: dev loss/word=4.0445, ppl=57.0842, time=1529681720.12s\n",
            "iter 56: train loss/word=0.0083, ppl=1.0084, time=1529672254.59s\n",
            "iter 56: dev loss/word=3.9705, ppl=53.0090, time=1529681748.35s\n",
            "iter 57: train loss/word=0.0077, ppl=1.0078, time=1529672282.87s\n",
            "iter 57: dev loss/word=3.9548, ppl=52.1850, time=1529681776.63s\n",
            "iter 58: train loss/word=0.0099, ppl=1.0100, time=1529672311.22s\n",
            "iter 58: dev loss/word=4.0250, ppl=55.9782, time=1529681804.96s\n",
            "iter 59: train loss/word=0.0088, ppl=1.0089, time=1529672339.45s\n",
            "iter 59: dev loss/word=4.0005, ppl=54.6264, time=1529681833.20s\n",
            "iter 60: train loss/word=0.0075, ppl=1.0075, time=1529672367.65s\n",
            "iter 60: dev loss/word=3.8663, ppl=47.7655, time=1529681861.41s\n",
            "iter 61: train loss/word=0.0085, ppl=1.0085, time=1529672395.90s\n",
            "iter 61: dev loss/word=3.9915, ppl=54.1383, time=1529681889.64s\n",
            "iter 62: train loss/word=0.0065, ppl=1.0065, time=1529672424.03s\n",
            "iter 62: dev loss/word=4.0016, ppl=54.6854, time=1529681917.78s\n",
            "iter 63: train loss/word=0.0064, ppl=1.0064, time=1529672452.29s\n",
            "iter 63: dev loss/word=4.0232, ppl=55.8809, time=1529681946.06s\n",
            "iter 64: train loss/word=0.0050, ppl=1.0050, time=1529672480.57s\n",
            "iter 64: dev loss/word=4.0860, ppl=59.4993, time=1529681974.32s\n",
            "iter 65: train loss/word=0.0088, ppl=1.0089, time=1529672508.87s\n",
            "iter 65: dev loss/word=4.1248, ppl=61.8559, time=1529682002.62s\n",
            "iter 66: train loss/word=0.0085, ppl=1.0085, time=1529672537.29s\n",
            "iter 66: dev loss/word=4.2121, ppl=67.4967, time=1529682031.04s\n",
            "iter 67: train loss/word=0.0095, ppl=1.0095, time=1529672565.87s\n",
            "iter 67: dev loss/word=4.2324, ppl=68.8839, time=1529682059.64s\n",
            "iter 68: train loss/word=0.0095, ppl=1.0095, time=1529672594.22s\n",
            "iter 68: dev loss/word=4.2706, ppl=71.5675, time=1529682087.98s\n",
            "iter 69: train loss/word=0.0046, ppl=1.0046, time=1529672622.48s\n",
            "iter 69: dev loss/word=4.2836, ppl=72.4986, time=1529682116.23s\n",
            "iter 70: train loss/word=0.0066, ppl=1.0066, time=1529672650.68s\n",
            "iter 70: dev loss/word=4.2131, ppl=67.5662, time=1529682144.43s\n",
            "iter 71: train loss/word=0.0098, ppl=1.0099, time=1529672678.89s\n",
            "iter 71: dev loss/word=4.2590, ppl=70.7364, time=1529682172.63s\n",
            "iter 72: train loss/word=0.0070, ppl=1.0071, time=1529672707.10s\n",
            "iter 72: dev loss/word=4.2193, ppl=67.9877, time=1529682200.84s\n",
            "iter 73: train loss/word=0.0070, ppl=1.0070, time=1529672735.49s\n",
            "iter 73: dev loss/word=4.3182, ppl=75.0501, time=1529682229.25s\n",
            "iter 74: train loss/word=0.0064, ppl=1.0065, time=1529672763.87s\n",
            "iter 74: dev loss/word=4.3030, ppl=73.9237, time=1529682257.64s\n",
            "iter 75: train loss/word=0.0043, ppl=1.0043, time=1529672792.18s\n",
            "iter 75: dev loss/word=4.2755, ppl=71.9128, time=1529682285.94s\n",
            "iter 76: train loss/word=0.0042, ppl=1.0042, time=1529672820.55s\n",
            "iter 76: dev loss/word=4.3237, ppl=75.4655, time=1529682314.31s\n",
            "iter 77: train loss/word=0.0065, ppl=1.0065, time=1529672848.94s\n",
            "iter 77: dev loss/word=4.4108, ppl=82.3315, time=1529682342.70s\n",
            "iter 78: train loss/word=0.0045, ppl=1.0046, time=1529672877.32s\n",
            "iter 78: dev loss/word=4.3013, ppl=73.7990, time=1529682371.08s\n",
            "iter 79: train loss/word=0.0036, ppl=1.0036, time=1529672905.63s\n",
            "iter 79: dev loss/word=4.2901, ppl=72.9747, time=1529682399.37s\n",
            "iter 80: train loss/word=0.0064, ppl=1.0064, time=1529672933.89s\n",
            "iter 80: dev loss/word=4.4421, ppl=84.9511, time=1529682427.66s\n",
            "iter 81: train loss/word=0.0052, ppl=1.0052, time=1529672962.30s\n",
            "iter 81: dev loss/word=4.3967, ppl=81.1792, time=1529682456.04s\n",
            "iter 82: train loss/word=0.0073, ppl=1.0073, time=1529672990.74s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "iter 82: dev loss/word=4.4591, ppl=86.4120, time=1529682484.49s\n",
            "iter 83: train loss/word=0.0043, ppl=1.0043, time=1529673019.13s\n",
            "iter 83: dev loss/word=4.4117, ppl=82.4130, time=1529682512.89s\n",
            "iter 84: train loss/word=0.0053, ppl=1.0053, time=1529673047.47s\n",
            "iter 84: dev loss/word=4.4746, ppl=87.7586, time=1529682541.23s\n",
            "iter 85: train loss/word=0.0039, ppl=1.0039, time=1529673075.97s\n",
            "iter 85: dev loss/word=4.4763, ppl=87.9097, time=1529682569.73s\n",
            "iter 86: train loss/word=0.0043, ppl=1.0043, time=1529673104.45s\n",
            "iter 86: dev loss/word=4.5566, ppl=95.2595, time=1529682598.20s\n",
            "iter 87: train loss/word=0.0039, ppl=1.0039, time=1529673132.85s\n",
            "iter 87: dev loss/word=4.6017, ppl=99.6544, time=1529682626.60s\n",
            "iter 88: train loss/word=0.0083, ppl=1.0083, time=1529673161.22s\n",
            "iter 88: dev loss/word=4.3905, ppl=80.6821, time=1529682654.98s\n",
            "iter 89: train loss/word=0.0042, ppl=1.0042, time=1529673189.57s\n",
            "iter 89: dev loss/word=4.5781, ppl=97.3312, time=1529682683.32s\n",
            "iter 90: train loss/word=0.0071, ppl=1.0071, time=1529673217.91s\n",
            "iter 90: dev loss/word=4.6533, ppl=104.9350, time=1529682711.67s\n",
            "iter 91: train loss/word=0.0079, ppl=1.0079, time=1529673246.44s\n",
            "iter 91: dev loss/word=4.6057, ppl=100.0505, time=1529682740.22s\n",
            "iter 92: train loss/word=0.0052, ppl=1.0052, time=1529673275.09s\n",
            "iter 92: dev loss/word=4.5247, ppl=92.2716, time=1529682768.88s\n",
            "iter 93: train loss/word=0.0045, ppl=1.0045, time=1529673303.78s\n",
            "iter 93: dev loss/word=4.6239, ppl=101.8861, time=1529682797.54s\n",
            "iter 94: train loss/word=0.0032, ppl=1.0032, time=1529673332.44s\n",
            "iter 94: dev loss/word=4.6217, ppl=101.6715, time=1529682826.20s\n",
            "iter 95: train loss/word=0.0042, ppl=1.0042, time=1529673361.01s\n",
            "iter 95: dev loss/word=4.5714, ppl=96.6823, time=1529682854.79s\n",
            "iter 96: train loss/word=0.0068, ppl=1.0068, time=1529673389.65s\n",
            "iter 96: dev loss/word=4.6190, ppl=101.3942, time=1529682883.43s\n",
            "iter 97: train loss/word=0.0053, ppl=1.0053, time=1529673418.36s\n",
            "iter 97: dev loss/word=4.6188, ppl=101.3716, time=1529682912.15s\n",
            "iter 98: train loss/word=0.0073, ppl=1.0073, time=1529673446.97s\n",
            "iter 98: dev loss/word=4.5714, ppl=96.6776, time=1529682940.75s\n",
            "iter 99: train loss/word=0.0030, ppl=1.0030, time=1529673475.73s\n",
            "iter 99: dev loss/word=4.6846, ppl=108.2679, time=1529682969.51s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}