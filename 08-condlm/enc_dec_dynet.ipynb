{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "enc_dec_dynet.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "LgFRrs3j73Ny",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d49e3450-30f3-4cf0-ef81-25f0caf65979",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529678490224,
          "user_tz": -540,
          "elapsed": 12421,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dynet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/8c/767cc83241b2abe567d705f87589d8ad44cca321f7c78720269c45e0469f/dyNET-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (27.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 27.8MB 1.6MB/s \n",
            "\u001b[?25hCollecting cython (from dynet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/79/d8e2cd00bea8156a995fb284ce7b6677c49eccd2d318f73e201a9ce560dc/Cython-0.28.3-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.4MB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from dynet) (1.14.5)\n",
            "Installing collected packages: cython, dynet\n",
            "Successfully installed cython-0.28.3 dynet-2.0.3\n",
            "Cloning into 'nn4nlp-code'...\n",
            "remote: Counting objects: 372, done.\u001b[K\n",
            "remote: Total 372 (delta 0), reused 0 (delta 0), pack-reused 372\u001b[K\n",
            "Receiving objects: 100% (372/372), 6.33 MiB | 19.75 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G1vswYZo71pa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "5df12ddf-4c4f-476e-ddae-69822055f638",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529678492188,
          "user_tz": -540,
          "elapsed": 1934,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import time\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import dynet as dy\n",
        "import numpy as np\n",
        "import pdb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dbxRpoZy8HY4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "f1f04a0b-ce90-42aa-f840-e5fabb7661a0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529678492864,
          "user_tz": -540,
          "elapsed": 548,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#much of the beginning is the same as the text retrieval\n",
        "# format of files: each line is \"word1 word2 ...\" aligned line-by-line\n",
        "train_src_file = \"nn4nlp-code/data/parallel/train.ja\"\n",
        "train_trg_file = \"nn4nlp-code/data/parallel/train.en\"\n",
        "dev_src_file = \"nn4nlp-code/data/parallel/dev.ja\"\n",
        "dev_trg_file = \"nn4nlp-code/data/parallel/dev.en\"\n",
        "test_src_file = \"nn4nlp-code/data/parallel/test.ja\"\n",
        "test_trg_file = \"nn4nlp-code/data/parallel/test.en\"\n",
        "\n",
        "\n",
        "w2i_src = defaultdict(lambda: len(w2i_src))\n",
        "w2i_trg = defaultdict(lambda: len(w2i_trg))\n",
        "\n",
        "def read(fname_src, fname_trg):\n",
        "    \"\"\"\n",
        "    Read parallel files where each line lines up\n",
        "    \"\"\"\n",
        "    with open(fname_src, \"r\") as f_src, open(fname_trg, \"r\") as f_trg:\n",
        "        for line_src, line_trg in zip(f_src, f_trg):\n",
        "            #need to append EOS tags to at least the target sentence\n",
        "            sent_src = [w2i_src[x] for x in line_src.strip().split() + ['</s>']] \n",
        "            sent_trg = [w2i_trg[x] for x in ['<s>'] + line_trg.strip().split() + ['</s>']] \n",
        "            yield (sent_src, sent_trg)\n",
        "\n",
        "# Read the data\n",
        "train = list(read(train_src_file, train_trg_file))\n",
        "unk_src = w2i_src[\"<unk>\"]\n",
        "eos_src = w2i_src['</s>']\n",
        "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
        "unk_trg = w2i_trg[\"<unk>\"]\n",
        "eos_trg = w2i_trg['</s>']\n",
        "sos_trg = w2i_trg['<s>']\n",
        "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
        "i2w_trg = {v: k for k, v in w2i_trg.items()}\n",
        "\n",
        "nwords_src = len(w2i_src)\n",
        "nwords_trg = len(w2i_trg)\n",
        "dev = list(read(dev_src_file, dev_trg_file))\n",
        "test = list(read(test_src_file, test_trg_file))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-2yedcz8Ljc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "07143932-bd15-42b3-c413-698704eeee88",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1529678493510,
          "user_tz": -540,
          "elapsed": 538,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# DyNet Starts\n",
        "model = dy.Model()\n",
        "trainer = dy.AdamTrainer(model)\n",
        "\n",
        "# Model parameters\n",
        "EMBED_SIZE = 64\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "#Especially in early training, the model can generate basically infinitly without generating an EOS\n",
        "#have a max sent size that you end at\n",
        "MAX_SENT_SIZE = 50\n",
        "\n",
        "# Lookup parameters for word embeddings\n",
        "LOOKUP_SRC = model.add_lookup_parameters((nwords_src, EMBED_SIZE))\n",
        "LOOKUP_TRG = model.add_lookup_parameters((nwords_trg, EMBED_SIZE))\n",
        "\n",
        "# Word-level LSTMs\n",
        "LSTM_SRC_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model)\n",
        "LSTM_TRG_BUILDER = dy.LSTMBuilder(1, EMBED_SIZE, HIDDEN_SIZE, model)\n",
        "\n",
        "#the softmax from the hidden size \n",
        "W_sm_p = model.add_parameters((nwords_trg, HIDDEN_SIZE))         # Weights of the softmax\n",
        "b_sm_p = model.add_parameters((nwords_trg))                   # Softmax bias\n",
        "\n",
        "def calc_loss(sent):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    src = sent[0]\n",
        "    trg = sent[1]\n",
        "\n",
        "\n",
        "    #initialize the LSTM\n",
        "    init_state_src = LSTM_SRC_BUILDER.initial_state()\n",
        "\n",
        "    #get the output of the first LSTM\n",
        "    src_output = init_state_src.add_inputs([LOOKUP_SRC[x] for x in src])[-1].output()\n",
        "    #now step through the output sentence\n",
        "    all_losses = []\n",
        "\n",
        "    current_state = LSTM_TRG_BUILDER.initial_state().set_s([src_output, dy.tanh(src_output)])\n",
        "    prev_word = trg[0]\n",
        "    W_sm = dy.parameter(W_sm_p)\n",
        "    b_sm = dy.parameter(b_sm_p)\n",
        "\n",
        "    for next_word in trg[1:]:\n",
        "        #feed the current state into the \n",
        "        current_state = current_state.add_input(LOOKUP_TRG[prev_word])\n",
        "        output_embedding = current_state.output()\n",
        "\n",
        "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
        "        all_losses.append(dy.pickneglogsoftmax(s, next_word))\n",
        "\n",
        "        prev_word = next_word\n",
        "    return dy.esum(all_losses)\n",
        "\n",
        "def generate(sent):\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    sent_reps = [LSTM_SRC.transduce([LOOKUP_SRC[x] for x in src])[-1] for src in sents]\n",
        "\n",
        "    dy.renew_cg()\n",
        "\n",
        "    # Transduce all batch elements with an LSTM\n",
        "    src = sent\n",
        "\n",
        "\n",
        "    #initialize the LSTM\n",
        "    init_state_src = LSTM_SRC_BUILDER.initial_state()\n",
        "\n",
        "    #get the output of the first LSTM\n",
        "    src_output = init_state_src.add_inputs([LOOKUP_SRC[x] for x in src])[-1].output()\n",
        "\n",
        "    #generate until a eos tag or max is reached\n",
        "    current_state = LSTM_TRG_BUILDER.initial_state().set_s([src_output, dy.tanh(src_output)])\n",
        "\n",
        "    prev_word = sos_trg\n",
        "    trg_sent = []\n",
        "    W_sm = dy.parameter(W_sm_p)\n",
        "    b_sm = dy.parameter(b_sm_p)\n",
        "\n",
        "    for i in range(MAX_SENT_SIZE):\n",
        "        #feed the previous word into the lstm, calculate the most likely word, add it to the sentence\n",
        "        current_state = current_state.add_input(LOOKUP_TRG[prev_word])\n",
        "        output_embedding = hidden_state.output()\n",
        "        s = dy.affine_transform([b_sm, W_sm, output_embedding])\n",
        "        probs = -dy.log_softmax(s).value()\n",
        "        next_word = np.argmax(probs)\n",
        "\n",
        "        if next_word == eos_trg:\n",
        "            break\n",
        "        prev_word = next_word\n",
        "        trg_sent.append(i2w_trg[next_word])\n",
        "    return trg_sent"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xwz7hu5n7wzK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 6083
        },
        "outputId": "8abc7b28-c6c5-41e7-b90a-bc1e8eddbdcb"
      },
      "cell_type": "code",
      "source": [
        "for ITER in range(100):\n",
        "  # Perform training\n",
        "  random.shuffle(train)\n",
        "  train_words, train_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(train):\n",
        "    my_loss = calc_loss(sent)\n",
        "    train_loss += my_loss.value()\n",
        "    train_words += len(sent)\n",
        "    my_loss.backward()\n",
        "    trainer.update()\n",
        "    if (sent_id+1) % 1000 == 0:\n",
        "      print(\"--finished %r sentences\" % (sent_id+1))\n",
        "  print(\"iter %r: train loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, train_loss/train_words, math.exp(train_loss/train_words), time.time()-start))\n",
        "  # Evaluate on dev set\n",
        "  dev_words, dev_loss = 0, 0.0\n",
        "  start = time.time()\n",
        "  for sent_id, sent in enumerate(dev):\n",
        "    my_loss = calc_loss(sent)\n",
        "    dev_loss += my_loss.value()\n",
        "    dev_words += len(sent)\n",
        "    trainer.update()\n",
        "  print(\"iter %r: dev loss/word=%.4f, ppl=%.4f, time=%.2fs\" % (ITER, dev_loss/dev_words, math.exp(dev_loss/dev_words), time.time()-start))\n",
        "\n",
        "#this is how you generate, can replace with desired sentenced to generate  \n",
        "sentences = []\n",
        "for sent_id, sent in enumerate(test):\n",
        "    translated_sent = generate(sent[0])\n",
        "    sentences.append(translated_sent)\n",
        "for sent in sentences:\n",
        "    print(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}