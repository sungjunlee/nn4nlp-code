{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn-class-pytorch.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1qO-sfwF0huHPQvOt7xAEYh2qr8qmQXmO",
          "timestamp": 1527177385962
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "atRf8NdTq3lY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "a9526c25-0cf6-4b50-d99c-cab65f405a74",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527177600665,
          "user_tz": -540,
          "elapsed": 87353,
          "user": {
            "displayName": "Sungjun Lee",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "107995332831641667384"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchtext\n",
        "!git clone https://github.com/neubig/nn4nlp-code.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 24kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5b048000 @  0x7f4c4fd9d1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 16.0MB/s \n",
            "\u001b[?25hCollecting tqdm (from torchtext)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2018.4.16)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torch, tqdm, torchtext\n",
            "Successfully installed torch-0.4.0 torchtext-0.2.3 tqdm-4.23.4\n",
            "Cloning into 'nn4nlp-code'...\n",
            "remote: Counting objects: 372, done.\u001b[K\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "remote: Total 372 (delta 0), reused 0 (delta 0), pack-reused 372\u001b[K\n",
            "Receiving objects: 100% (372/372), 6.33 MiB | 23.05 MiB/s, done.\n",
            "Resolving deltas: 100% (131/131), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RFd0_jW2rAF2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w9XRE5l-rC-k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Functions to read in the corpus\n",
        "w2i = defaultdict(lambda: len(w2i))\n",
        "t2i = defaultdict(lambda: len(t2i))\n",
        "UNK = w2i[\"<unk>\"]\n",
        "def read_dataset(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        for line in f:\n",
        "            tag, words = line.lower().strip().split(\" ||| \")\n",
        "            yield ([w2i[x] for x in words.split(\" \")], t2i[tag])\n",
        "\n",
        "# Read in the data\n",
        "train = list(read_dataset(\"nn4nlp-code/data/classes/train.txt\"))\n",
        "w2i = defaultdict(lambda: UNK, w2i)\n",
        "dev = list(read_dataset(\"nn4nlp-code/data/classes/test.txt\"))\n",
        "nwords = len(w2i)\n",
        "ntags = len(t2i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zQsP601o3qhi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_size, win_size, filter_size):\n",
        "    super(CNNModel, self).__init__()\n",
        "    self.filter_size = filter_size\n",
        "    self.embeddings = nn.Embedding(vocab_size, emb_size)\n",
        "    self.conv1 = nn.Conv1d(emb_size, filter_size, win_size)\n",
        "    self.maxpool1 = nn.AdaptiveAvgPool1d(1)\n",
        "    self.fcl = nn.Linear(filter_size, ntags)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.embeddings(x)\n",
        "    x = self.conv1(x.transpose(1,2))\n",
        "    x = self.maxpool1(x).view(-1, self.filter_size)\n",
        "    x = self.fcl(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhIk0rmgrFD8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "EMB_SIZE = 64\n",
        "# W_emb = model.add_lookup_parameters((nwords, 1, 1, EMB_SIZE)) # Word embeddings\n",
        "WIN_SIZE = 3\n",
        "FILTER_SIZE = 64\n",
        "# W_cnn = model.add_parameters((1, WIN_SIZE, EMB_SIZE, FILTER_SIZE)) # cnn weights\n",
        "# b_cnn = model.add_parameters((FILTER_SIZE)) # cnn bias\n",
        "\n",
        "# W_sm = model.add_parameters((ntags, FILTER_SIZE))          # Softmax weights\n",
        "# b_sm = model.add_parameters((ntags))                      # Softmax bias\n",
        "\n",
        "# Start DyNet and define trainer\n",
        "model = CNNModel(nwords, EMB_SIZE, WIN_SIZE, FILTER_SIZE)\n",
        "trainer = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ckfiOOeurH2e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def calc_scores(words):\n",
        "    if len(words) < WIN_SIZE:\n",
        "      words += [0] * (WIN_SIZE-len(words))\n",
        "\n",
        "    logit = model(torch.tensor(words).view(1, -1))\n",
        "    return logit\n",
        "#     cnn_in = dy.concatenate([dy.lookup(W_emb, x) for x in words], d=1)\n",
        "#     cnn_out = dy.conv2d_bias(cnn_in, W_cnn_express, b_cnn_express, stride=(1, 1), is_valid=False)\n",
        "#     pool_out = dy.max_dim(cnn_out, d=1)\n",
        "#     pool_out = dy.reshape(pool_out, (FILTER_SIZE,))\n",
        "#     pool_out = dy.rectify(pool_out)\n",
        "#     return W_sm_express * pool_out + b_sm_express"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aMqQeXCRrJ9c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "a8bfd109-7214-4cc3-c22a-9fdca606bff7"
      },
      "cell_type": "code",
      "source": [
        "for ITER in range(100):\n",
        "    # Perform training\n",
        "    random.shuffle(train)\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0.0\n",
        "    start = time.time()\n",
        "    for words, tag in train:\n",
        "        scores = calc_scores(words)\n",
        "        predict = np.argmax(scores.detach().numpy())\n",
        "        if predict == tag:\n",
        "            train_correct += 1\n",
        "        \n",
        "#         print(scores, tag)\n",
        "        my_loss = F.cross_entropy(scores, torch.LongTensor([tag]))\n",
        "#         my_loss = dy.pickneglogsoftmax(scores, tag)\n",
        "        train_loss += my_loss.detach().numpy()\n",
        "        my_loss.backward()\n",
        "        trainer.step()\n",
        "    print(\"iter %r: train loss/sent=%.4f, acc=%.4f, time=%.2fs\" % (ITER, train_loss/len(train), train_correct/len(train), time.time()-start))\n",
        "    # Perform testing\n",
        "    test_correct = 0.0\n",
        "    for words, tag in dev:\n",
        "        scores = calc_scores(words)\n",
        "        predict = np.argmax(scores.detach().numpy())\n",
        "        if predict == tag:\n",
        "            test_correct += 1\n",
        "    print(\"iter %r: test acc=%.4f\" % (ITER, test_correct/len(dev)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0: train loss/sent=1301.1692, acc=0.3641, time=64.08s\n",
            "iter 0: test acc=0.3050\n",
            "iter 1: train loss/sent=2634.9131, acc=0.5372, time=58.52s\n",
            "iter 1: test acc=0.3367\n",
            "iter 2: train loss/sent=4796.3728, acc=0.6945, time=57.26s\n",
            "iter 2: test acc=0.3344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2PmSPKZ7rLue",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}